{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About","text":""},{"location":"#welcome-to-my-personal-site","title":"\ud83d\udc4b Welcome to My Personal Site","text":"<p>This is a page created by Michael Qiu.</p> C++PythonJava <pre><code>#include &lt;iostream&gt;\nusing namespace std;\n\nint main() {\n    ios::sync_with_stdio(false);\n    cin.tie(0);\n    cout.tie(0);\n\n    cout &lt;&lt; \"Hello world!\" &lt;&lt; endl;\n    return 0;\n}\n</code></pre> <pre><code>times = 11.4\n\nwhile times &gt; 0:\n    print(\"Nice to see ya!\")\n    times -= 1\n</code></pre> <pre><code>public class HelloWorld {\n    public static void main(String[] args) {\n        System.out.println(\"Hello, Visitor!\");\n    }\n}\n</code></pre>"},{"location":"#about-me","title":"About Me","text":"<p>Currently an OIer.</p>"},{"location":"#find-me","title":"Find Me","text":"<p>This is my GitHub page: MichaelQiu</p> <p>This is my Luogu page: MikePP</p>"},{"location":"some-websites/","title":"Some Websites","text":""},{"location":"some-websites/#algorithms","title":"Algorithms","text":"<ul> <li>OI-Wiki (Chinese)</li> <li>Buggg\u7684\u5c0f\u5c4b (Chinese)</li> <li>Runoob (Chinese)</li> <li>VisuAlgo</li> <li>Bilibili - \u82f1\u96c4\u54ea\u91cc\u51fa\u6765 (Chinese)</li> </ul>"},{"location":"some-websites/#online-judges","title":"Online Judges","text":"<ul> <li>Luogu (Chinese)</li> <li>LeetCode</li> <li>CodeForces</li> <li>HDU OJ</li> </ul>"},{"location":"some-websites/#c","title":"C++","text":"<ul> <li>CPPReference</li> <li>ISOCPP</li> <li>Microsoft C++ &amp; C</li> <li>CPlusPlus</li> </ul>"},{"location":"some-websites/#ides","title":"IDEs","text":"<ul> <li>Visual Studio Code</li> <li>Visual Studio</li> <li>Xcode (Mac Only)</li> <li>PyCharm</li> <li>CLion</li> </ul>"},{"location":"algorithms/gcd-lcm/","title":"GCD & LCM","text":""},{"location":"algorithms/gcd-lcm/#stl","title":"STL","text":"<p>cppreference</p> Head FilesC++ Versionstd::gcdstd::lcm <pre><code>#include &lt;numeric&gt;\n</code></pre> <pre><code>-std=c++17\n</code></pre> <pre><code>template&lt; class M, class N &gt;\nconstexpr std::common_type_t&lt;M, N&gt; gcd( M m, N n );\n</code></pre> <pre><code>template&lt; class M, class N &gt;\nconstexpr std::common_type_t&lt;M, N&gt; lcm( M m, N n );\n</code></pre>"},{"location":"algorithms/gcd-lcm/#gcd","title":"GCD <sup>1</sup>","text":"Conditional OperatorBitwise Operation <pre><code>int gcd(int x, int y) {\n    return y&gt;0 ? gcd(y, x%y) : x;\n}\n</code></pre> <pre><code>int gcd(int x, int y) {\n    while (y^=x^=y^=x%=y);\n    return x;\n}\n</code></pre>"},{"location":"algorithms/gcd-lcm/#lcm","title":"LCM <sup>2</sup>","text":"<pre><code>int lcm(int x, int y) {\n    return a / gcd(x, y) * y;\n}\n</code></pre> <p>Tips</p> <p>Performing division before multiplication can help avoid overflow.</p> <ol> <li> <p>Greatest Common Divisor\u00a0\u21a9</p> </li> <li> <p>Least Common Multiple\u00a0\u21a9</p> </li> </ol>"},{"location":"algorithms/quick-pow/","title":"Quick Pow","text":"O(log n)<pre><code>long long qpow(long long a, const long long &amp;n, const long long &amp;m=1) {\n    if (n == 0) return 1;\n    else if (n == 1) return a;\n\n    long long tmp = qpow(a, n&gt;&gt;1, m);\n    if (n &amp; 1) return tmp * tmp * a % m;\n    else return tmp * tmp % m;\n}\n</code></pre>"},{"location":"algorithms/prime/euler-sieve/","title":"Euler Sieve","text":""},{"location":"algorithms/prime/euler-sieve/#complexity","title":"Complexity","text":"<p>Time Complexity: $O(n)$</p>"},{"location":"algorithms/prime/euler-sieve/#template","title":"Template","text":"<pre><code>vector&lt;int&gt; pri;\nbool not_prime[maxn];\n\nvoid get_prime(const int &amp;n) {\n    for (int i=2; i&lt;=n; ++i) {\n        if (!not_prime[i]) pri.push_back(i);\n        for (const int &amp;pri_j : pri) {\n            if (i * pri_j &gt; n) break;\n            not_prime[i * pri_j] = true;\n            if (i % pri_j == 0) break;\n        }\n    }\n}\n</code></pre>"},{"location":"algorithms/search/a-star/","title":"A*","text":""},{"location":"algorithms/search/a-star/#complexity","title":"Complexity","text":"<p>Time Complexity: $O((n+m) \\log m)$</p>"},{"location":"algorithms/search/a-star/#template","title":"Template","text":"Head FilesC++ Version <pre><code>#include &lt;vector&gt;\n#include &lt;queue&gt;\n#include &lt;utility&gt;\n#include &lt;algorithm&gt;\n</code></pre> <pre><code>-std=c++11\n</code></pre> Constants<pre><code>const int inf = 0x3f;\nconst int maxn = 1e4 + 5;\n</code></pre> struct<pre><code>struct edge {\n    int v, w;\n};\n\nstruct node {\n    int u, dis;\n\n    bool operator&gt;(const node &amp;x) const {\n        return dis &gt; x.dis;\n    }\n};\n\nvector&lt;edge&gt; e[maxn];\npair&lt;int, int&gt; coord[maxn];\n</code></pre> Manhattan distanceEuclidean Distance <pre><code>int heuristic(const pair&lt;int, int&gt; &amp;u, const pair&lt;int, int&gt; &amp;tg) {\n    return abs(u.first - tg.first) + abs(u.second - tg.second);\n}\n</code></pre> <pre><code>int heuristic(const pair&lt;int, int&gt; &amp;u, const pair&lt;int, int&gt; &amp;tg) {\n    return sqrt(pow(u.first - tg.first, 2) + pow(u.second - tg.second, 2)); // Euclidean distance\n}\n</code></pre> A*<pre><code>vector&lt;int&gt; a_star(const int &amp;sz, const int &amp;st, const int &amp;tg) {\n    vector&lt;int&gt; dis(sz+1, inf);\n    vector&lt;int&gt; prev(sz+1, -1);\n    priority_queue&lt; node, vector&lt;node&gt;, greater&lt;node&gt; &gt; pq;\n\n    dis[st] = 0;\n    pq.emplace((node){st, 0});\n\n    while (!pq.empty()) {\n        int u = pq.top().u; pq.pop();\n\n        if (u == tg) break;\n\n        for (const auto &amp;ed : e[u]) {\n            int v = ed.v, w = ed.w;\n            if (dis[v] &gt; dis[u] + w) {\n                dis[v] = dis[u] + w;\n                pq.emplace((node){v, dis[v]+heuristic(coord[u], coord[tg])});\n            }\n        }\n    }\n\n    vector&lt;int&gt; path;\n    if (dis[tg] != inf) {\n        for (int i = tg; i != -1; i = prev[i]) {\n            path.emplace_back(i);\n        }\n        reverse(path.begin(), path.end());\n    }\n    return path;\n}\n</code></pre>"},{"location":"algorithms/shortest-path/bellman-ford/","title":"Bellman Ford","text":""},{"location":"algorithms/shortest-path/bellman-ford/#video-recommendation","title":"Video Recommendation","text":"<ul> <li>4.4 Bellman Ford Algorithm - Single Source Shortest Path - Dynamic Programming - Adbul Bari</li> </ul>"},{"location":"algorithms/shortest-path/bellman-ford/#template","title":"Template","text":"Head FilesC++ Version <pre><code>#include &lt;vector&gt;\n</code></pre> <pre><code>-std=c++11\n</code></pre> Constants<pre><code>const int inf = 0x3f;\nconst int maxn = 1e4 + 5;\n</code></pre> struct<pre><code>struct edge {\n    int v, w;\n};\n\nstruct node {\n    int u, dis;\n\n    bool operator&gt;(const node &amp;x) const {\n        return dis &gt; x.dis;\n    }\n};\n\nvector&lt;edge&gt; e[maxn];\n</code></pre> Bellman Ford<pre><code>void bellman_ford(const int &amp;sz, const int &amp;st) {\n    vector&lt;int&gt; dis(sz+1, inf);\n    dis[st] = 0;\n\n    bool flag;\n    for (int i=1; i&lt;=sz; ++i) {\n        flag = 0;\n        for (int u=1; u&lt;=sz; ++u) {\n            for (auto &amp;e : e[u]) {\n                int v = e.v, w = e.w;\n                if (dis[v] &gt; dis[u] + w) {\n                    dis[v] = dis[u] + w;\n                    flag = 1;\n                }\n            }\n        }\n\n        if (!flag) return;\n    }\n}\n</code></pre>"},{"location":"algorithms/shortest-path/dijkstra/","title":"Dijkstra (prioirty_queue)","text":""},{"location":"algorithms/shortest-path/dijkstra/#video-recommendation","title":"Video Recommendation","text":"<ul> <li>3.6 Dijkstra Algorithm - Single Source Shortest Path - Greedy Method - Abdul Bari</li> <li>\u3010C++\u3011\u5355\u6e90\u6700\u77ed\u8defDijkstra-\u8fea\u6770\u65af\u7279\u62c9\u7b97\u6cd5</li> </ul>"},{"location":"algorithms/shortest-path/dijkstra/#complexity","title":"Complexity","text":"<p>Time Complexity: $O(m \\log m)$</p>"},{"location":"algorithms/shortest-path/dijkstra/#template","title":"Template","text":"Head FilesC++ Version <pre><code>#include &lt;vector&gt;\n#include &lt;queue&gt;\n</code></pre> <pre><code>-std=c++11\n</code></pre> Constants<pre><code>const int inf = 0x3f;\nconst int maxn = 1e4 + 5;\n</code></pre> struct<pre><code>struct edge {\n    int v, w;\n};\n\nstruct node {\n    int u, dis;\n\n    bool operator&gt;(const node &amp;x) const {\n        return dis &gt; x.dis;\n    }\n};\n\nvector&lt;edge&gt; e[maxn];\n</code></pre> priority_queue Template<pre><code>vector&lt;int&gt; dijkstra(const int &amp;sz, const int &amp;st) {\n    vector&lt;int&gt; dis(sz+1, inf);\n    vector&lt;bool&gt; vis(sz+1, 0);\n    priority_queue&lt;node, vector&lt;node&gt;, greater&lt;node&gt; &gt; pq;\n\n    dis[st] = 0;\n    pq.emplace((node){st, 0});\n\n    while (!pq.empty()) {\n        int u = pq.top().u; pq.pop();\n        if (vis[u]) continue;\n        vis[u] = 1;\n\n        for (const auto &amp;ed : e[u]) {\n            int v = ed.v, w = ed.w;\n            if (dis[v] &gt; dis[u] + w) {\n                dis[v] = dis[u] + w;\n                pq.emplace((node){v, dis[v]});\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"algorithms/shortest-path/dijkstra/#problems","title":"Problems","text":"<p>Luogu</p> Solution <pre><code>#include &lt;bits/stdc++.h&gt;\nusing namespace std;\nconst int maxn = 1e5 + 2;\nconst int inf = 0x3f3f3f3f;\n\nstruct edge {\n    int v, w;\n}; vector&lt;edge&gt; e[maxn];\n\nstruct node {\n    int u, dis;\n\n    bool operator&gt;(const node &amp;x) const {\n        return dis &gt; x.dis;\n    } \n};\n\nint n, m, s;\n\nvoid dijkstra(const int &amp;sz, const int &amp;st) {\n    vector&lt;int&gt; dis(sz+1, inf);\n    vector&lt;bool&gt; vis(sz+1, 0);\n    priority_queue&lt;node, vector&lt;node&gt;, greater&lt;node&gt; &gt; pq;\n\n    dis[st] = 0;\n    pq.push((node){st, 0});\n    while (!pq.empty()) {\n        int u = pq.top().u; pq.pop();\n        if (vis[u]) continue;\n        vis[u] = 1;\n\n        for (const auto &amp;ed : e[u]) {\n            int v = ed.v, w = ed.w;\n            if (dis[v] &gt; dis[u] + w) {\n                dis[v] = dis[u] + w;\n                pq.push((node){v, dis[v]});\n            }\n        }\n    }\n\n    for (int i=1; i&lt;=n; ++i) {\n        cout &lt;&lt; dis[i] &lt;&lt; (i == n ? \"\\n\" : \" \");\n    }\n}\n\nsigned main() {\n    ios::sync_with_stdio(0);\n    cin.tie(0);\n    cout.tie(0);\n\n    cin &gt;&gt; n &gt;&gt; m &gt;&gt; s;\n    for (int i=1; i&lt;=m; ++i) {\n        int u, v, w;\n        cin &gt;&gt; u &gt;&gt; v &gt;&gt; w;\n\n        e[u].push_back((edge){v, w});\n    }\n\n    dijkstra(n, s);\n}\n</code></pre>"},{"location":"algorithms/shortest-path/floyd/","title":"Floyd","text":""},{"location":"algorithms/shortest-path/floyd/#complexity","title":"Complexity","text":"<p>Time Complexity: $O(n^3)$</p> <p>Space Complexity: $O(n^2)$</p> Floyd<pre><code>void floyd(int dis[][], int sz) {\n    for (int k=1; i&lt;=sz; ++i) {\n        for (int x=1; x&lt;=sz; ++x) {\n            for (int y=1; y&lt;=sz; ++y) {\n                dis[x][y] = min(dis[x][y], dis[x][k] + dis[k][y]);\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"algorithms/shortest-path/floyd/#problems","title":"Problems","text":""},{"location":"algorithms/shortest-path/floyd/#luogu-p119","title":"Luogu P119 \u707e\u540e\u91cd\u5efa","text":"Solution <pre><code>#include &lt;bits/stdc++.h&gt;\n#define endl \"\\n\"\nusing namespace std;\nconst int maxn = 202;\nconst int maxm = 20000;\nconst int inf = 0x3f3f3f3f;\n\nint n, m;\nvector&lt; vector&lt;int&gt; &gt; adj(maxn, vector&lt;int&gt;(maxn, inf));\nvector&lt;int&gt; t(maxn, 0);\n\nsigned main() {\n    ios::sync_with_stdio(0);\n    cin.tie(0);\n    cout.tie(0);\n\n    cin &gt;&gt; n &gt;&gt; m;\n\n    for (int i=0; i&lt;n; ++i) {\n        adj[i][i] = 0;\n    }\n\n    for (int i=0; i&lt;n; ++i) {\n        cin &gt;&gt; t[i];\n    } for (int i=0; i&lt;m; ++i) {\n        int x, y, z;\n        cin &gt;&gt; x &gt;&gt; y &gt;&gt; z;\n        adj[x][y] = adj[y][x] = z;\n    }\n\n    int Q; cin &gt;&gt; Q;\n    int last = 0; // last accessed time\n    while (Q--) {\n        int x, y, z;\n        cin &gt;&gt; x &gt;&gt; y &gt;&gt; z;\n\n        if (t[x] &gt; z || t[y] &gt; z) {cout &lt;&lt; -1 &lt;&lt; endl; continue;}\n\n        while (last &lt; n &amp;&amp; t[last] &lt;= z) {\n            // Floyd\n            for (int i=0; i&lt;n; ++i) {\n                for (int j=0; j&lt;n; ++j) {\n                    adj[i][j] = min(adj[i][j], adj[i][last] + adj[last][j]);\n                }\n            }\n            ++last;\n        }\n\n        cout &lt;&lt; (adj[x][y] == inf ? -1 : adj[x][y]) &lt;&lt; endl;\n    }\n} \n</code></pre>"},{"location":"algorithms/shortest-path/spfa/","title":"SPFA","text":""},{"location":"algorithms/shortest-path/spfa/#complexity","title":"Complexity","text":"<p>Time Complexity: $O(nm)$</p>"},{"location":"algorithms/shortest-path/spfa/#template","title":"Template","text":"Head FilesC++ Version <pre><code>#include &lt;vector&gt;\n#include &lt;queue&gt;\n</code></pre> <pre><code>-std=c++11\n</code></pre> Constants<pre><code>const int inf = 0x3f;\nconst int maxn = 1e4 + 5;\n</code></pre> struct<pre><code>struct edge {\n    int v, w;\n};\nvector&lt;edge&gt; e[maxn];\n</code></pre> Template<pre><code>vector&lt;int&gt; spfa(const int &amp;sz, const int &amp;st) {\n    vector&lt;int&gt; dis(maxn, inf);\n    vector&lt;bool&gt; vis(maxn, 0);\n    queue&lt;int&gt; q;\n\n    dis[st] = 0;\n    vis[st] = 1;\n    q.push(st);\n    while (!q.empty()) {\n        int u = q.front(); q.pop();\n        vis[u] = 0;\n\n        for (const auto &amp;ed : e[u]) {\n            int v = ed.v, w = ed.w;\n            if (dis[v] &gt; dis[u] + w) {\n                dis[v] = dis[u] + w;\n\n                if (!vis[v]) {\n                    q.push(v);\n                    vis[v] = 1;\n                }\n            }\n        }\n    }\n\n    return dis;\n}\n</code></pre>"},{"location":"algorithms/shortest-path/spfa/#problems","title":"Problems","text":"<p>Luogu P3371</p> Solution <pre><code>#include &lt;bits/stdc++.h&gt;\n#define int unsigned long long\nusing namespace std;\nconst int maxn = 2e5 + 2;\nconst int inf = (1ull&lt;&lt;31) - 1;\n\nstruct edge {\n    int v, w;\n};\nvector&lt;edge&gt; e[maxn];\n\nvoid spfa(const int &amp;sz, const int &amp;st) {\n    vector&lt;int&gt; dis(maxn, inf);\n    vector&lt;bool&gt; vis(maxn, 0);\n    queue&lt;int&gt; q;\n\n    dis[st] = 0;\n    vis[st] = 1;\n    q.push(st);\n    while (!q.empty()) {\n        int u = q.front(); q.pop();\n        vis[u] = 0;\n\n        for (const auto &amp;ed : e[u]) {\n            int v = ed.v, w = ed.w;\n            if (dis[v] &gt; dis[u] + w) {\n                dis[v] = dis[u] + w;\n\n                if (!vis[v]) {\n                    q.push(v);\n                    vis[v] = 1;\n                }\n            }\n        }\n    }\n\n    for (int i=1; i&lt;=sz; ++i) {\n        cout &lt;&lt; dis[i] &lt;&lt; \" \";\n    }\n}\n\nint n, m, s;\n\nsigned main() {\n    ios::sync_with_stdio(0);\n    cin.tie(0);\n    cout.tie(0);\n\n    cin &gt;&gt; n &gt;&gt; m &gt;&gt; s;\n    for (int i=1; i&lt;=m; ++i) {\n        int u, v, w;\n        cin &gt;&gt; u &gt;&gt; v &gt;&gt; w;\n        e[u].push_back((edge){v, w}); \n    }\n\n    spfa(n, s);\n}\n</code></pre>"},{"location":"algorithms/sort/bubble-sort/","title":"Bubble sort","text":"<pre><code>    template &lt;typename Type&gt;\n    void Bubble(Type arr[], int n) {\n    assert(sizeof(arr) / sizeof(Type) != n);\n\n    int loop = 0;\n    for (int i=1; i&lt;n-1; ++i) {\n        bool sorted = 1;\n        for (int j=1; j&lt;n; ++j) {\n            if (arr[j] &lt; arr[j - 1]) {\n                swap(arr[j], arr[j - 1]);\n                sorted = 0;\n            }\n\n            ++loop;\n        }\n\n        if (sorted) {\n            break;\n        }\n    }\n}\n</code></pre>"},{"location":"algorithms/sort/radix-sort/","title":"Radix sort","text":""},{"location":"algorithms/sort/radix-sort/#video-recommendation","title":"Video Recommendation","text":"<ul> <li>\u3010\u6392\u5e8f\u7b97\u6cd5\u3011\u57fa\u6570\u6392\u5e8f\u3011</li> </ul>"},{"location":"algorithms/sort/radix-sort/#complexity","title":"Complexity","text":"<p>$O(n)$ or $O(n \\lg n)$</p>"},{"location":"algorithms/sort/radix-sort/#template","title":"Template","text":""},{"location":"algorithms/sort/sort-overview/","title":"Sort Overview","text":""},{"location":"algorithms/sort/sort-overview/#sort-algorithms","title":"Sort Algorithms","text":"Algorithms Time Complexity Bubble Sort $O(n^2)$ Insertion Sort $O(n^2)$ Selection Sort $O(n^2)$ Quick Sort $O(n \\log n) \\rightarrow O(n^2)$ Merge Sort $O(n \\log n)$ Heap Sort $O(n \\log n)$ Bucket Sort $O(n + n^2/k + k) \\rightarrow O(n^2)$ Radix Sort $O(n)$ <code>std::sort()</code> $O(n \\log n)$"},{"location":"blog/","title":"Index","text":""},{"location":"blog/#blog","title":"Blog","text":""},{"location":"blog/2024/09/14/deploy-ai-models-locally/","title":"Deploy AI Models Locally","text":""},{"location":"blog/2024/09/14/deploy-ai-models-locally/#ollama","title":"Ollama","text":""},{"location":"blog/2024/09/14/deploy-ai-models-locally/#what-is-ollama","title":"What Is Ollama?","text":"<p>Ollama is an open-source platform designed to make large language models (LLMs) more accessible and usable.</p>"},{"location":"blog/2024/09/14/deploy-ai-models-locally/#how-to-deploy","title":"How to Deploy","text":"<ol> <li>Visit Ollama and download.</li> <li>Choose a model you like</li> <li>Run <code>ollama run YourModel:ParameterSize</code> in the terminal or Windows CMD (Windows: Win+R $\\rightarrow$ cmd)</li> <li>Your model will start downloading. If the model has already been downloaded, you will go to the chat page (starts with <code>&gt;&gt;&gt;</code>);</li> </ol>"},{"location":"blog/2024/09/14/deploy-ai-models-locally/#how-to-choose-an-appropriate-model","title":"How to Choose An Appropriate Model","text":""},{"location":"blog/2024/09/14/deploy-ai-models-locally/#choose-model","title":"Choose Model","text":"<p>Visit Model Library</p> <p>Here is the Aritificial Analysis's LLM Leaderboard</p> <p>Recommendations:</p> <ul> <li>Google Gemma 2</li> <li>Meta Llama 3.1</li> <li>Alibaba Qwen2</li> </ul>"},{"location":"blog/2024/09/14/deploy-ai-models-locally/#choose-parameter-size","title":"Choose Parameter Size","text":"<p>The model size should be smaller than your memory size.</p> <p>For instance, the <code>gemma2:9b</code> model is 5.4GB which is runnable on devices with 16GB memory.</p> <p>However, the <code>gemma2:27b</code> model is 16GB and it is not runnable on the same device (there must be some other programs that eat up your memory).</p>"},{"location":"blog/2024/09/14/deploy-ai-models-locally/#drawbacks-of-running-in-terminal","title":"Drawbacks of Running in Terminal","text":"<p>Your chat history cannot be saved. If you want to store your history chat, deploy one of the following apps (the titles link to their GitHub Repos):</p> <ul> <li>Enchanted (macOS Only)</li> <li>Open WebUI</li> </ul>"},{"location":"blog/2024/09/14/deploy-ai-models-locally/#enchanted","title":"Enchanted","text":"<p>Download in AppStore</p>"},{"location":"blog/2024/09/14/deploy-ai-models-locally/#open-webui","title":"Open WebUI","text":"<ol> <li>Download Docker</li> <li>Follow the Instructions</li> </ol>"},{"location":"blog/2024/09/14/deploy-ai-models-locally/#moa","title":"MoA","text":"<p>Together MoA Repo</p> <p>Mixture of Agents (MoA) is a novel approach that leverages the collective strengths of multiple LLMs to enhance performance, achieving state-of-the-art results.</p> <p></p> <p>The original MoA does not support Ollama. Here is my edited Ollama version of MoA forked from severian42's MoA-Ollama-Chat. I edited the GUI to make it look better.</p> <p>Follow the Instructions here.</p>"},{"location":"blog/2024/10/01/musicgpt/","title":"MusicGPT","text":""},{"location":"blog/2024/10/01/musicgpt/#musicgpt","title":"MusicGPT","text":"<p>gabotechs's MusciGPT Repo</p> <p>You can generate music using natural language prompts locally on MusicGPT.</p>"},{"location":"blog/2024/10/01/musicgpt/#install","title":"Install","text":"<p>Official Instruction</p>"},{"location":"blog/2024/10/01/musicgpt/#usage","title":"Usage","text":"<p>Execute the following command in terminal:</p> <pre><code>musicgpt\n</code></pre>"},{"location":"blog/2024/10/01/musicgpt/#samples","title":"Samples","text":"<p>Create a prog rock song with space rock elements</p> <p></p> <p>Create a relaxing jazz song</p> <p></p> <p>Create epic, atmospheric music with soaring strings, synths, and organ. Evoking transcendence, build intensity gradually.</p> <p></p> <p>Note</p> <p>When running the default <code>musicgpt</code> model (small), the maximum duration of audio is 29 seconds.</p> <p>Also, it is NOT able to generate vocal sounds.</p>"},{"location":"data-structures/bignum/","title":"Bignum","text":""},{"location":"data-structures/binary-heap/","title":"Binary Heap","text":"<p>Time Complexity:</p> <ul> <li>Emplace: $O(\\log n)$</li> <li>Pop: $O(\\log n)$</li> </ul> Head FilesC++ Version <pre><code>#include &lt;cstddef&gt;\n#include &lt;algorithm&gt;\n</code></pre> <pre><code>-std=c++11\n</code></pre> Template<pre><code>template &lt;typename Type, size_t Size&gt;\nclass heap {\nprivate:\n    int cnt;\n    Type val[Size];\n\npublic:\n    heap() {cnt=0;}\n\n    void emplace(int x) {\n        val[++cnt] = x;\n        int now = cnt;\n\n        while (now){\n            int fa = now &gt;&gt; 1;\n\n            if (val[fa] &gt; val[now]) {\n                swap(val[fa], val[now]);\n            } else {\n                break;\n            }\n\n            now = fa;\n        }\n    }\n\n    void pop() {\n        swap(val[cnt--], val[1]);\n\n        int now = 1;\n        while ((now &lt;&lt; 1) &lt;= cnt) { \n            int ls = now &lt;&lt; 1;\n\n            if (ls+1 &lt;= cnt &amp;&amp; val[ls+1] &lt; val[ls]) {\n                ++ls;\n            }\n            if (val[ls] &lt; val[now]) {\n                swap(val[now], val[ls]);\n            } else {\n                break;\n            }\n\n            now = ls;\n        }\n    }\n\n    int size() {\n        return cnt; \n    }\n\n    bool empty() {\n        return cnt == 0;\n    }\n\n    Type top() {\n        return val[1];\n    }\n};\n</code></pre>"},{"location":"data-structures/disjoint-set-union/","title":"Disjoint Set Union","text":"Head Files <pre><code>#include &lt;vector&gt;\n</code></pre> Template<pre><code>class dsu {\nprivate:\n    vector&lt;int&gt; fa;\n\npublic:\n    dsu(const int &amp;sz) {\n        fa.resize(sz+1, -1);\n\n        for (int i=1; i&lt;=sz; ++i) {\n            fa[i] = i;\n        }\n    }\n\n    int find(const int &amp;x) {\n        return fa[x] == x ? x : fa[x] = find(fa[x]);\n    }\n\n    void merge(const int &amp;x, const int &amp;y) {\n        fa[find(x)] = find(y);\n    }\n};\n</code></pre>"},{"location":"data-structures/segment-tree/","title":"Segment Tree","text":""},{"location":"data-structures/segment-tree/#time-complexity","title":"Time Complexity:","text":"<ul> <li>Build: $O(n)$</li> <li>Query: $O(\\log n)$</li> <li>Update: $O(\\log n)$</li> <li>Modify: $O(\\log n)$</li> </ul>"},{"location":"data-structures/segment-tree/#template","title":"Template","text":"Head FilesC++ Version <pre><code>#include &lt;vector&gt;\n#include &lt;functional&gt;\n</code></pre> <pre><code>-std=c++11\n</code></pre>"},{"location":"data-structures/segment-tree/#integrated-version","title":"Integrated Version","text":"Template<pre><code>template&lt;typename Type&gt;\nclass segment_tree {\nusing func_type = function&lt;Type(const Type &amp;, const Type &amp;)&gt;;\nprivate:\n    vector&lt;Type&gt; val;\n    vector&lt;Type&gt; add_tag;\n    vector&lt;Type&gt; set_tag;\n    vector&lt;bool&gt; is_set;\n    int len;\n\n    inline int ls(const int &amp;x) { return x &lt;&lt; 1; }\n    inline int rs(const int &amp;x) { return x &lt;&lt; 1 | 1; }\n\n    func_type opr;\n    static Type default_func_type(const Type &amp;x, const Type &amp;y) { return x + y; }\n\n    void push_up(const int &amp;x) {\n        val[x] = opr(val[ls(x)], val[rs(x)]);\n    }\n\n    void add_tag_func(const int &amp;x, const int &amp;l, const int &amp;r, const Type &amp;d) {\n        add_tag[x] += d;\n        val[x] += d * (r - l + 1);\n    }\n\n    void set_tag_func(const int &amp;x, const int &amp;l, const int &amp;r, const Type &amp;d) {\n        set_tag[x] = d;\n        val[x] = d * (r - l + 1);\n        add_tag[x] = 0;\n        is_set[x] = true;\n    }\n\n    void push_down(const int &amp;x, const int &amp;l, const int &amp;r) {\n        int mid = (l + r) &gt;&gt; 1;\n        if (is_set[x]) {\n            set_tag_func(ls(x), l, mid, set_tag[x]);\n            set_tag_func(rs(x), mid + 1, r, set_tag[x]);\n            is_set[x] = false;\n        }\n        if (add_tag[x] != 0) {\n            add_tag_func(ls(x), l, mid, add_tag[x]);\n            add_tag_func(rs(x), mid + 1, r, add_tag[x]);\n            add_tag[x] = 0;\n        }\n    }\n\n    void build(const vector&lt;Type&gt; &amp;s, const int &amp;x, const int &amp;l, const int &amp;r) {\n        add_tag[x] = 0;\n        set_tag[x] = 0;\n        is_set[x] = false;\n        if (l == r) {\n            val[x] = s[l];\n            return;\n        }\n\n        int mid = (l + r) &gt;&gt; 1;\n        build(s, ls(x), l, mid);\n        build(s, rs(x), mid + 1, r);\n        push_up(x);\n    }\n\n    Type query(const int &amp;L, const int &amp;R, const int x, const int &amp;l, const int &amp;r) {\n        if (L &lt;= l &amp;&amp; r &lt;= R) return val[x];\n        push_down(x, l, r);\n\n        Type res = Type();\n        int mid = (l + r) &gt;&gt; 1;\n\n        if (L &lt;= mid) res = opr(res, query(L, R, ls(x), l, mid));\n        if (R &gt; mid) res = opr(res, query(L, R, rs(x), mid + 1, r));\n\n        return res;\n    }\n\n    void update(const int &amp;L, const int &amp;R, const int &amp;x, const int &amp;l, const int &amp;r, const Type &amp;d) {\n        if (L &lt;= l &amp;&amp; r &lt;= R) {\n            add_tag_func(x, l, r, d);\n            return;\n        }\n\n        push_down(x, l, r);\n        int mid = (l + r) &gt;&gt; 1;\n        if (L &lt;= mid) update(L, R, ls(x), l, mid, d);\n        if (R &gt; mid) update(L, R, rs(x), mid + 1, r, d);\n\n        push_up(x);\n    }\n\n    void modify(const int &amp;L, const int &amp;R, const int &amp;x, const int &amp;l, const int &amp;r, const Type &amp;d) {\n        if (L &lt;= l &amp;&amp; r &lt;= R) {\n            set_tag_func(x, l, r, d);\n            return;\n        }\n\n        push_down(x, l, r);\n        int mid = (l + r) &gt;&gt; 1;\n        if (L &lt;= mid) modify(L, R, ls(x), l, mid, d);\n        if (R &gt; mid) modify(L, R, rs(x), mid + 1, r, d);\n\n        push_up(x);\n    }\n\npublic:\n    segment_tree(vector&lt;Type&gt; v, func_type _opr = default_func_type) {\n        val.resize(v.size() &lt;&lt; 2, Type());\n        add_tag.resize(v.size() &lt;&lt; 2, Type());\n        set_tag.resize(v.size() &lt;&lt; 2, Type());\n        is_set.resize(v.size() &lt;&lt; 2, false);\n        len = v.size();\n        opr = _opr;\n\n        v.insert(v.begin(), Type());\n        build(v, 1, 1, len);\n    }\n\n    int size() const {\n        return len;\n    }\n\n    Type query(const int &amp;L, const int &amp;R) {\n        return query(L, R, 1, 1, len);\n    }\n\n    void update(const int &amp;L, const int &amp;R, const Type &amp;d) {\n        update(L, R, 1, 1, len, d);\n    }\n\n    void modify(const int &amp;L, const int &amp;R, const Type &amp;d) {\n        modify(L, R, 1, 1, len, d);\n    }\n};\n</code></pre> Usage<pre><code>signed main() {\n    vector&lt;int&gt; a;\n    int b[]={5, 4, 3, 2, 1};\n\n    for (int i=0; i&lt;5; ++i) {\n        a.emplace_back(b[i]);\n    }\n    segment_tree&lt;int&gt; sg(a);\n\n    cout &lt;&lt; sg.query(1, 4) &lt;&lt; endl;\n    sg.update(1, 2, 2);\n    cout &lt;&lt; sg.query(1, 4) &lt;&lt; endl;\n    sg.modify(1, 2, 2);\n    cout &lt;&lt; sg.query(1, 4) &lt;&lt; endl;\n}\n</code></pre>"},{"location":"data-structures/segment-tree/#add-version","title":"Add Version","text":"<pre><code>class segment_tree {\nusing VI = vector&lt;int&gt;;\nusing CI = const int;\n\nprivate:\n    VI val, tag;\n    int len;\n\n    inline int ls(CI &amp;x) {\n        return x &lt;&lt; 1;\n    }\n    inline int rs(CI &amp;x) {\n        return x &lt;&lt; 1 | 1;\n    }\n\n    void push_up(CI &amp;x) {\n        val[x] = val[ls(x)] + val[rs(x)];\n    }\n\n    void add_tag(CI &amp;x, CI &amp;l, CI &amp;r, CI &amp;d) {\n        tag[x] += d;\n        val[x] += d * (l - r + 1);\n    }\n\n    void push_down(CI &amp;x, CI &amp;l, CI &amp;r) {\n        if (tag[x]) {\n            int mid = (l + r) &gt;&gt; 1;\n            add_tag(ls(x), l, mid, tag[x]);\n            add_tag(rs(x), mid+1, r, tag[x]);\n            tag[x] = 0;\n        }\n    }\n\n    void build(const VI &amp;s, CI &amp;x, CI &amp;l, CI &amp;r) {\n        tag[x] = 0;\n        if (l == r) {\n            val[x] = s[x];\n            return;\n        }\n\n        int mid = (l + r) &gt;&gt; 1;\n        build(s, ls(x), l, mid);\n        build(s, rs(x), mid+1, r);\n        push_up(x);\n    }\n\n    int query(CI &amp;L, CI &amp;R, CI &amp;x, CI &amp;l, CI &amp;r) {\n        if (L &lt;= l &amp;&amp; r &lt;= R) {\n            return val[x];\n        }\n        push_down(x, l, r);\n\n        int ret = 0;\n        int mid = (l+r) &gt;&gt; 1;\n\n        if (L &lt;= mid) ret += query(L, R, ls(x), l, mid);\n        if (R &gt; mid) ret += query(L, R, rs(x), mid+1, r);\n\n        return ret;\n    }\n\n    void update(CI &amp;L, CI &amp;R, CI &amp;x, CI &amp;l, CI &amp;r, CI &amp;d) {\n        if (L &lt;= l &amp;&amp; r &lt;= R) {\n            add_tag(x, l, r, d);\n            return;\n        }\n\n        push_down(x, l, r);\n        int mid = (l+r) &gt;&gt; 1;\n\n        if (L &lt;= mid) update(L, R, ls(x), l, mid, d);\n        if (R &gt; mid) update(L, R, rs(x), mid+1, r, d);\n\n        push_up(x);\n    }\n\npublic:\n    segment_tree(VI &amp;s) {\n        val.resize(s.size() &lt;&lt; 2, 0);\n        tag.resize(s.size() &lt;&lt; 2, 0);\n        len = s.size();\n\n        s.emplace(s.begin(), 0);\n        build(s, 1, 1, len);\n    }\n};\n</code></pre>"},{"location":"data-structures/sparse-table/","title":"Sparse Table","text":"<p>Time Complexity:</p> <ul> <li>Build: $O(n \\log n)$</li> <li>Query: $O(1)$</li> </ul> Head FilesC++ Version <pre><code>#include &lt;vector&gt;\n#include &lt;algorithm&gt;\n#include &lt;functional&gt;\n#include &lt;cmath&gt;\n</code></pre> <pre><code>-std=c++11\n</code></pre> build log2 table<pre><code>vector&lt;int&gt; _log2;\nvoid build_log2(int x) {\n    if (x &lt; _log2.size()) return;\n\n    _log2.resize(x+1);\n    _log2[1] = 0;\n    for (int i=2; i&lt;=x; ++i) {\n        _log2[i] = _log2[i &gt;&gt; 1] + 1;\n    }\n}\n</code></pre> sparse_table<pre><code>template&lt;typename Type&gt;\nclass sparse_table {\nusing func_type = function&lt;Type(const Type &amp;, const Type &amp;)&gt;;\nprivate:\n    vector&lt; vector&lt;Type&gt; &gt; val;\n    static Type default_func_type(const Type &amp;x, const Type &amp;y) { return max(x, y); }\n    func_type opr;\n\npublic:\n    sparse_table(const vector&lt;Type&gt; &amp;v, func_type _opr = default_func_type) {\n        opr = _opr;\n        int row = v.size();\n\n        if (_log2.empty()) { build_log2(row); }\n\n        int col = ceil(_log2[row]) + 1;\n\n        val.assign(row, vector&lt;Type&gt;(col, 0));\n        for (int i=0; i&lt;row; ++i) {\n            val[i][0] = v[i];\n        }\n        for (int j=1; j&lt;col; ++j) {\n            int _j = (1 &lt;&lt; (j-1));\n            for (int i=0; i + _j &lt; row; ++i) {\n                val[i][j] = opr(val[i][j-1], val[i + (1 &lt;&lt; (j-1))][j-1]);\n            }\n        }\n    }\n\n    Type query(int l, int r) {\n        --l; --r;\n        int len = r-l+1;\n        int q = floor(_log2[len]);\n        return opr(val[l][q], val[r - (1 &lt;&lt; q) + 1][q]);\n    }\n};\n</code></pre>"},{"location":"data-structures/stack/","title":"Stack","text":"<p>Time Complexity:</p> <ul> <li>Emplace $O(1)$</li> <li>Pop: $O(1)$</li> </ul> mystack<pre><code>template &lt;typename Type, size_t Size&gt;\nclass mystack {\nprivate:\n    int cnt;\n    Type val[Size];\n\npublic:\n    mystack() {cnt=0;}\n\n    void emplace(int x) {\n        val[++cnt] = x;\n    }\n\n    void pop() {\n        --cnt;\n    }\n\n    Type top() {\n        return val[cnt];\n    }\n\n    bool empty() {\n        return cnt == 0;\n    }\n\n    void clear() {\n        cnt = 0;\n    }\n}\n</code></pre>"},{"location":"data-structures/tree/binary-tree-traversals/","title":"Binary Tree Traversals","text":"node<pre><code>struct node {\n    int val;\n    node *ls, *rs;\n}\n</code></pre>"},{"location":"data-structures/tree/binary-tree-traversals/#preorder-traversal","title":"Preorder Traversal","text":"<pre><code>void preorder(node *rt) {\n    cout &lt;&lt; rt -&gt; val &lt;&lt; endl;\n    preorder(rt -&gt; ls);\n    preorder(rt -&gt; rs);\n}\n</code></pre>"},{"location":"data-structures/tree/binary-tree-traversals/#inorder-traversal","title":"Inorder Traversal","text":"<pre><code>void inorder(node *rt) {\n    inorder(rt -&gt; ls);\n    cout &lt;&lt; rt -&gt; val &lt;&lt; endl;\n    inorder(rt -&gt; rs);\n}\n</code></pre>"},{"location":"data-structures/tree/binary-tree-traversals/#postorder-traversal","title":"Postorder Traversal","text":"<pre><code>void postorder(node *rt) {\n    postorder(rt -&gt; ls);\n    postorder(rt -&gt; rs);\n    cout &lt;&lt; rt -&gt; val &lt;&lt; endl;\n}\n</code></pre>"},{"location":"data-structures/tree/binary-tree-traversals/#get-preorder-traversal","title":"Get Preorder Traversal","text":""},{"location":"lessons/ai-for-beginners/","title":"README","text":"<p>This lesson is forked from AI for Beginners from Microsoft under MIT license.</p> <p> </p> <p> </p> <p></p>"},{"location":"lessons/ai-for-beginners/#artificial-intelligence-for-beginners-a-curriculum","title":"Artificial Intelligence for Beginners - A Curriculum","text":"AI For Beginners - Sketchnote by @girlie_mac <p>Explore the world of Artificial Intelligence (AI) with our 12-week, 24-lesson curriculum!  It includes practical lessons, quizzes, and labs. The curriculum is beginner-friendly and covers tools like TensorFlow and PyTorch, as well as ethics in AI</p>"},{"location":"lessons/ai-for-beginners/#what-you-will-learn","title":"What you will learn","text":"<p>Mindmap of the Course</p> <p>In this curriculum, you will learn:</p> <ul> <li>Different approaches to Artificial Intelligence, including the \"good old\" symbolic approach with Knowledge Representation and reasoning (GOFAI).</li> <li>Neural Networks and Deep Learning, which are at the core of modern AI. We will illustrate the concepts behind these important topics using code in two of the most popular frameworks - TensorFlow and PyTorch.</li> <li>Neural Architectures for working with images and text. We will cover recent models but may be a bit lacking in the state-of-the-art.</li> <li>Less popular AI approaches, such as Genetic Algorithms and Multi-Agent Systems.</li> </ul> <p>What we will not cover in this curriculum:</p> <p>Find all additional resources for this course in our Microsoft Learn collection</p> <ul> <li>Business cases of using AI in Business. Consider taking Introduction to AI for business users learning path on Microsoft Learn, or AI Business School, developed in cooperation with INSEAD.</li> <li>Classic Machine Learning, which is well described in our Machine Learning for Beginners Curriculum.</li> <li>Practical AI applications built using Cognitive Services. For this, we recommend that you start with modules Microsoft Learn for vision, natural language processing, Generative AI with Azure OpenAI Service and others.</li> <li>Specific ML Cloud Frameworks, such as Azure Machine Learning, Microsoft Fabric, or Azure Databricks. Consider using Build and operate machine learning solutions with Azure Machine Learning and Build and Operate Machine Learning Solutions with Azure Databricks learning paths.</li> <li>Conversational AI and Chat Bots. There is a separate Create conversational AI solutions learning path, and you can also refer to this blog post for more detail.</li> <li>Deep Mathematics behind deep learning. For this, we would recommend Deep Learning by Ian Goodfellow, Yoshua Bengio and Aaron Courville, which is also available online at https://www.deeplearningbook.org/.</li> </ul> <p>For a gentle introduction to AI in the Cloud topics you may consider taking the Get started with artificial intelligence on Azure Learning Path.</p>"},{"location":"lessons/ai-for-beginners/#content","title":"Content","text":"Lesson Link PyTorch/Keras/TensorFlow Lab 0 Course Setup Setup Your Development Environment I Introduction to AI 01 Introduction and History of AI - - II Symbolic AI 02 Knowledge Representation and Expert Systems Expert Systems /  Ontology /Concept Graph III Introduction to Neural Networks 03 Perceptron Notebook Lab 04 Multi-Layered Perceptron and Creating our own Framework Notebook Lab 05 Intro to Frameworks (PyTorch/TensorFlow) and Overfitting PyTorch / Keras / TensorFlow Lab IV Computer Vision PyTorch / TensorFlow Explore Computer Vision on Microsoft Azure 06 Intro to Computer Vision. OpenCV Notebook Lab 07 Convolutional Neural Networks &amp;  CNN Architectures PyTorch /TensorFlow Lab 08 Pre-trained Networks and Transfer Learning and Training Tricks PyTorch / TensorFlow Lab 09 Autoencoders and VAEs PyTorch / TensorFlow 10 Generative Adversarial Networks &amp; Artistic Style Transfer PyTorch / TensorFlow 11 Object Detection TensorFlow Lab 12 Semantic Segmentation. U-Net PyTorch / TensorFlow V Natural Language Processing PyTorch /TensorFlow Explore Natural Language Processing on Microsoft Azure 13 Text Representation. Bow/TF-IDF PyTorch / TensorFlow 14 Semantic word embeddings. Word2Vec and GloVe PyTorch / TensorFlow 15 Language Modeling. Training your own embeddings PyTorch / TensorFlow Lab 16 Recurrent Neural Networks PyTorch / TensorFlow 17 Generative Recurrent Networks PyTorch / TensorFlow Lab 18 Transformers. BERT. PyTorch /TensorFlow 19 Named Entity Recognition TensorFlow Lab 20 Large Language Models, Prompt Programming and Few-Shot Tasks PyTorch VI Other AI Techniques 21 Genetic Algorithms Notebook 22 Deep Reinforcement Learning PyTorch /TensorFlow Lab 23 Multi-Agent Systems VII AI Ethics 24 AI Ethics and Responsible AI Microsoft Learn: Responsible AI Principles IX Extras 25 Multi-Modal Networks, CLIP and VQGAN Notebook"},{"location":"lessons/ai-for-beginners/#each-lesson-contains","title":"Each lesson contains","text":"<ul> <li>Pre-reading material</li> <li>Executable Jupyter Notebooks, which are often specific to the framework (PyTorch or TensorFlow). The executable notebook also contains a lot of theoretical material, so to understand the topic you need to go through at least one version of the notebook (either PyTorch or TensorFlow).</li> <li>Labs available for some topics, which give you an opportunity to try applying the material you have learned to a specific problem.</li> <li>Some sections contain links to MS Learn modules that cover related topics.</li> </ul>"},{"location":"lessons/ai-for-beginners/#getting-started","title":"Getting Started","text":"<ul> <li>We have created a setup lesson to help you with setting up your development environment. - For Educators, we have created a curricula setup lesson for you too!</li> <li>How to Run the code in a VSCode or a Codepace</li> </ul> <p>Follow these steps:</p> <p>Fork the Repository: Click on the \"Fork\" button at the top-right corner of this page.</p> <p>Clone the Repository: <code>git clone https://github.com/microsoft/AI-For-Beginners.git</code></p> <p>Don't forget to star (\ud83c\udf1f) this repo to find it easier later.</p>"},{"location":"lessons/ai-for-beginners/#meet-other-learners","title":"Meet other Learners","text":"<p>Join our official AI Discord server to meet and network with other learners taking this course and get support.</p>"},{"location":"lessons/ai-for-beginners/#quizzes","title":"Quizzes","text":"<p>A note about quizzes: All quizzes are contained in the Quiz-app folder in etc\\quiz-app, They are linked from within the lessons the quiz app can be run locally or deployed to Azure; follow the instruction in the <code>quiz-app</code> folder. They are gradually being localized.</p>"},{"location":"lessons/ai-for-beginners/#help-wanted","title":"Help Wanted","text":"<p>Do you have suggestions or found spelling or code errors? Raise an issue or create a pull request.</p>"},{"location":"lessons/ai-for-beginners/#special-thanks","title":"Special Thanks","text":"<ul> <li>\u270d\ufe0f Primary Author: Dmitry Soshnikov, PhD</li> <li>\ud83d\udd25 Editor: Jen Looper, PhD</li> <li>\ud83c\udfa8 Sketchnote illustrator: Tomomi Imura</li> <li>\u2705 Quiz Creator: Lateefah Bello, MLSA</li> <li>\ud83d\ude4f Core Contributors: Evgenii Pishchik</li> </ul>"},{"location":"lessons/ai-for-beginners/#other-curricula","title":"Other Curricula","text":"<p>Our team produces other curricula! Check out:</p> <ul> <li>Data Science for Beginners</li> <li>Version 2.0 Generative AI for Beginners</li> <li>NEW Cybersecurity for Beginners</li> <li>Web Dev for Beginners</li> <li>IoT for Beginners</li> <li>Machine Learning for Beginners</li> <li>XR Development for Beginners</li> <li>Mastering GitHub Copilot for AI Paired Programming</li> </ul>"},{"location":"lessons/ai-for-beginners/0-course-setup/how-to-run/","title":"How to run","text":""},{"location":"lessons/ai-for-beginners/0-course-setup/how-to-run/#how-to-run-the-code","title":"How to Run the Code","text":"<p>This curriculum contains a lot of executable examples and labs that you would want to run. In order to do this, you need the ability to execute Python code in Jupyter Notebooks provided as part of this curriculum. You have several options for running the code:</p>"},{"location":"lessons/ai-for-beginners/0-course-setup/how-to-run/#run-locally-on-your-computer","title":"Run locally on your computer","text":"<p>To run the code locally on your computer, you would need to have some version of Python installed. I personally recommend installing miniconda - it is rather lightweight installation that supports <code>conda</code> package manager for different Python virtual environments.</p> <p>After you install miniconda, you need to clone the repository and create a virtual environment to be used for this course:</p> <pre><code>git clone http://github.com/microsoft/ai-for-beginners\ncd ai-for-beginners\nconda env create --name ai4beg --file .devcontainer/environment.yml\nconda activate ai4beg\n</code></pre>"},{"location":"lessons/ai-for-beginners/0-course-setup/how-to-run/#using-visual-studio-code-with-python-extension","title":"Using Visual Studio Code with Python Extension","text":"<p>Probably the best way to use the curriculum is to open it in Visual Studio Code with Python Extension.</p> <p>Note: Once you clone and open the directory in VS Code, it will automatically suggest you to install Python extensions. You would also have to install miniconda as described above.</p> <p>Note: If VS Code suggests you to re-open the repository in container, you need to decline this to use local Python installation. </p>"},{"location":"lessons/ai-for-beginners/0-course-setup/how-to-run/#using-jupyter-in-the-browser","title":"Using Jupyter in the Browser","text":"<p>You can also use Jupyter environment right from the browser on your own computer. Actually, both classical Jupyter and Jupyer Hub provide quite convenient development environment with auto-completion, code highlighting, etc.</p> <p>To start Jupyter locally, go to the directory of the course, and execute:</p> <p></p><pre><code>jupyter notebook\n</code></pre> or <pre><code>jupyterhub\n</code></pre> You then can navigate to any of the <code>.ipynb</code> files, open them and start working."},{"location":"lessons/ai-for-beginners/0-course-setup/how-to-run/#running-in-container","title":"Running in container","text":"<p>One alternative to Python installation would be to run the code in container. Since our repository contains special <code>.devcontainer</code> folder that instructs how to build a container for this repo, VS Code would offer you to re-open the code in container. This will require Docker installation, and also would be more complex, so we recommend this to more experienced users.</p>"},{"location":"lessons/ai-for-beginners/0-course-setup/how-to-run/#running-in-the-cloud","title":"Running in the Cloud","text":"<p>If you do not want to install Python locally, and have access to some cloud resources - a good alternative would be to run the code in the cloud. There are several ways you can do this:</p> <ul> <li>Using GitHub Codespaces, which is a virtual environment created for you on GitHub, accessible through VS Code browser interface. If you have access to Codespaces, you can just click Code button in the repo, start a codespace, and get running in no time.</li> <li>Using Binder. Binder is a free computing resources provided in the cloud for people like you to test out some code on GitHub. There is a button at the front page to open the repository in Binder - this should quickly take you to the binder site, which will build underlying container and start Jupyter web interface for you seamlessly.</li> </ul> <p>Note: To prevent misuse, Binder has access to some web resources blocked. This may prevent some of the code working, which fetches models and/or datasets from public Internet. You may need to find some workarounds. Also, compute resources provided by Binder are pretty basic, so training will be slow, especially in later more complex lessons.</p>"},{"location":"lessons/ai-for-beginners/0-course-setup/how-to-run/#running-in-the-cloud-with-gpu","title":"Running in the Cloud with GPU","text":"<p>Some of the later lessons in this curriculum would greatly benefit from GPU support, because otherwise training will be painfully slow. There are a few options you can follow, especially if you have access to the cloud either through Azure for Students, or through your institution:</p> <ul> <li>Create Data Science Virtual Machine and connect to it through Jupyter. You can then clone the repo right onto the machine, and start learning. NC-series VMs have GPU support.</li> </ul> <p>Note: Some subscriptions, including Azure for Students, do not provide GPU support out of the box. You may need to request additional GPU cores through technical support request.</p> <ul> <li>Create Azure Machine Learning Workspace and then use Notebook feature there. This video shows how to clone a repository into Azure ML notebook and start using it.</li> </ul> <p>You can also use Google Colab, which comes with some free GPU support, and upload Jupyter Notebooks there to execute them one-by-one.</p>"},{"location":"lessons/ai-for-beginners/0-course-setup/setup/","title":"Setup","text":""},{"location":"lessons/ai-for-beginners/0-course-setup/setup/#getting-started-with-this-curricula","title":"Getting Started with this Curricula","text":""},{"location":"lessons/ai-for-beginners/0-course-setup/setup/#are-you-a-student","title":"Are you a student?","text":"<p>Get started with the following resources:</p> <ul> <li>Student Hub page On this page, you will find beginner resources, Student packs, and even ways to get a free cert voucher. This is one page you want to bookmark and check from time to time as we switch out content at least monthly.</li> <li>Microsoft Student Learn ambassadors Join a global community of student ambassadors, this could be your way into Microsoft.</li> </ul> <p>Students, there are a couple of ways to use the curriculum. First of all, you can just read the text and look through the code directly on GitHub. If you want to run the code in any of the notebooks - read our instructions, and find more advice on how to do it in this blog post.</p> <p>Note: Instructions on how to run the code in this curriculum</p>"},{"location":"lessons/ai-for-beginners/0-course-setup/setup/#self-study","title":"Self Study","text":"<p>However, if you would like to take the course as a self-study project, we suggest that you fork the entire repo to your own GitHub account and complete the exercises on your own or with a group:</p> <ul> <li>Start with a pre-lecture quiz.</li> <li>Read the intro text for the lecture.</li> <li>If the lecture has additional notebooks, go through them, reading and executing the code. If both TensorFlow and PyTorch notebooks are provided, you can focus on one of them - choose your favorite framework.</li> <li>Notebooks often contain some of the challenges that require you to tweak the code a little bit to experiment.</li> <li>Take the post-lecture quiz.</li> <li>If there is a lab attached to the module - complete the assignment.</li> <li>Visit the Discussion board to \"learn out loud\".</li> </ul> <p>For further study, we recommend following these Microsoft Learn modules and learning paths.</p> <p>Teachers, we have included some suggestions on how to use this curriculum.</p>"},{"location":"lessons/ai-for-beginners/0-course-setup/setup/#pedagogy","title":"Pedagogy","text":"<p>We have chosen two pedagogical tenets while building this curriculum: ensuring that it is hands-on project-based and that it includes frequent quizzes.</p> <p>By ensuring that the content aligns with projects, the process is made more engaging for students and retention of concepts will be augmented. In addition, a low-stakes quiz before a class sets the intention of the student towards learning a topic, while a second quiz after class ensures further retention. This curriculum was designed to be flexible and fun and can be taken in whole or in part. The projects start small and become increasingly complex by the end of the 12-week cycle.</p> <p>A note about quizzes: All quizzes are contained in this app, for 50 total quizzes of three questions each. They are linked from within the lessons but the quiz app can be run locally; follow the instructions in the <code>etc/quiz-app</code> folder.</p>"},{"location":"lessons/ai-for-beginners/0-course-setup/setup/#offline-access","title":"Offline access","text":"<p>You can run this documentation offline by using Docsify. Fork this repo, install Docsify on your local machine, and then in the <code>etc/docsify</code> folder of this repo, type <code>docsify serve</code>. The website will be served on port 3000 on your localhost: <code>localhost:3000</code>. A pdf of the curriculum is available at this link.</p>"},{"location":"lessons/ai-for-beginners/1-Intro/","title":"Lesson 1","text":""},{"location":"lessons/ai-for-beginners/1-Intro/#introduction-to-ai","title":"Introduction to AI","text":"<p>Sketchnote by Tomomi Imura</p>"},{"location":"lessons/ai-for-beginners/1-Intro/#pre-lecture-quiz","title":"Pre-lecture quiz","text":"<p>Artificial Intelligence is an exciting scientific discipline that studies how we can make computers exhibit intelligent behavior, e.g. do those things that human beings are good at doing.</p> <p>Originally, computers were invented by Charles Babbage to operate on numbers following a well-defined procedure - an algorithm. Modern computers, even though significantly more advanced than the original model proposed in the 19th century, still follow the same idea of controlled computations. Thus it is possible to program a computer to do something if we know the exact sequence of steps that we need to do in order to achieve the goal.</p> <p></p> <p>Photo by Vickie Soshnikova</p> <p>\u2705 Defining the age of a person from his or her photograph is a task that cannot be explicitly programmed, because we do not know how we come up with a number inside our head when we do it.</p> <p>There are some tasks, however, that we do not explicitly know how to solve. Consider determining the age of a person from his/her photograph. We somehow learn to do it, because we have seen many examples of people of different age, but we cannot explicitly explain how we do it, nor can we program the computer to do it. This is exactly the kind of task that are of interest to Artificial Intelligence (AI for short).</p> <p>\u2705 Think of some tasks that you could offload to a computer that would benefit from AI. Consider the fields of finance, medicine, and the arts - how are these fields benefiting today from AI?</p>"},{"location":"lessons/ai-for-beginners/1-Intro/#weak-ai-vs-strong-ai","title":"Weak AI vs. Strong AI","text":"Weak AI Strong AI Weak AI refers to AI systems that are designed and trained for a specific task or a narrow set of tasks. Strong AI, or Artificial General Intelligence (AGI), refers to AI systems with human-level intelligence and understanding. These AI systems are not generally intelligent; they excel in performing a predefined task but lack true understanding or consciousness. These AI systems have the ability to perform any intellectual task that a human being can do, adapt to different domains, and possess a form of consciousness or self-awareness. Examples of weak AI include virtual assistants like Siri or Alexa, recommendation algorithms used by streaming services, and chatbots that are designed for specific customer service tasks. Achieving Strong AI is a long-term goal of AI research and would require the development of AI systems that can reason, learn, understand, and adapt across a wide range of tasks and contexts. Weak AI is highly specialized and does not possess human-like cognitive abilities or general problem-solving capabilities beyond its narrow domain. Strong AI is currently a theoretical concept, and no AI system has reached this level of general intelligence <p>for more infomation refer Artificial General Intelligence (AGI).</p>"},{"location":"lessons/ai-for-beginners/1-Intro/#the-definition-of-intelligence-and-the-turing-test","title":"The Definition of Intelligence and the Turing Test","text":"<p>One of the problems when dealing with the term Intelligence is that there is no clear definition of this term. One can argue that intelligence is connected to abstract thinking, or to self-awareness, but we cannot properly define it.</p> <p></p> <p>Photo by Amber Kipp from Unsplash</p> <p>To see the ambiguity of a term intelligence, try answering a question: \"Is a cat intelligent?\". Different people tend to give different answers to this question, as there is no universally accepted test to prove the assertion is true or not. And if you think there is - try running your cat through an IQ test...</p> <p>\u2705 Think for a minute about how you define intelligence. Is a crow who can solve a maze and get at some food intelligent? Is a child intelligent?</p> <p>When speaking about AGI we need to have some way to tell if we have created a truly intelligent system. Alan Turing proposed a way called a Turing Test, which also acts like a definition of intelligence. The test compares a given system to something inherently intelligent - a real human being, and because any automatic comparison can be bypassed by a computer program, we use a human interrogator. So, if a human being is unable to distinguish between a real person and a computer system in text-based dialogue - the system is considered intelligent.</p> <p>A chat-bot called Eugene Goostman, developed in St.Petersburg, came close to passing the Turing test in 2014 by using a clever personality trick. It announced up front that it was a 13-year old Ukrainian boy, which would explain the lack of knowledge and some discrepancies in the text. The bot convinced 30% of the judges that it was human after a 5 minute dialogue, a metric that Turing believed a machine would be able to pass by 2000. However, one should understand that this does not indicate that we have created an intelligent system, or that a computer system has fooled the human interrogator - the system didn't fool the humans, but rather the bot creators did!</p> <p>\u2705 Have you ever been fooled by a chat bot into thinking that you are speaking to a human? How did it convince you?</p>"},{"location":"lessons/ai-for-beginners/1-Intro/#different-approaches-to-ai","title":"Different Approaches to AI","text":"<p>If we want a computer to behave like a human, we need somehow to model inside a computer our way of thinking. Consequently, we need to try to understand what makes a human being intelligent.</p> <p>To be able to program intelligence into a machine, we need to understand how our own processes of making decisions work. If you do a little self-introspection, you will realize that there are some processes that happen subconsciously \u2013 eg. we can distinguish a cat from a dog without thinking about it - while some others involve reasoning.</p> <p>There are two possible approaches to this problem:</p> Top-down Approach (Symbolic Reasoning) Bottom-up Approach (Neural Networks) A top-down approach models the way a person reasons to solve a problem. It involves extracting knowledge from a human being, and representing it in a computer-readable form. We also need to develop a way to model reasoning inside a computer. A bottom-up approach models the structure of a human brain, consisting of a huge number of simple units called neurons. Each neuron acts like a weighted average of its inputs, and we can train a network of neurons to solve useful problems by providing training data. <p>There are also some other possible approaches to intelligence:</p> <ul> <li> <p>An Emergent, Synergetic or multi-agent approach are based on the fact that complex intelligent behaviour can be obtained by an interaction of a large number of simple agents. According to evolutionary cybernetics, intelligence can emerge from more simple, reactive behaviour in the process of metasystem transition.</p> </li> <li> <p>An Evolutionary approach, or genetic algorithm is an optimization process based on the principles of evolution.</p> </li> </ul> <p>We will consider those approaches later in the course, but right now we will focus on two main directions: top-down and bottom-up.</p>"},{"location":"lessons/ai-for-beginners/1-Intro/#the-top-down-approach","title":"The Top-Down Approach","text":"<p>In a top-down approach, we try to model our reasoning.  Because we can follow our thoughts when we reason, we can try to formalize this process and program it inside the computer. This is called symbolic reasoning.</p> <p>People tend to have some rules in their head that guide their decision making processes. For example, when a doctor is diagnosing a patient, he or she may realize that a person has a fever, and thus there might be some inflammation going on inside the body. By applying a large set of rules to a specific problem a doctor may be able to come up with the final diagnosis.</p> <p>This approach relies heavily on knowledge representation and reasoning. Extracting knowledge from a human expert might be the most difficult part, because a doctor in many cases would not know exactly why he or she is coming up with a particular diagnosis. Sometimes the solution just comes up in his or her head without explicit thinking. Some tasks, such as determining the age of a person from a photograph, cannot be at all reduced to manipulating knowledge.</p>"},{"location":"lessons/ai-for-beginners/1-Intro/#bottom-up-approach","title":"Bottom-Up Approach","text":"<p>Alternately, we can try to model the simplest elements inside our brain \u2013 a neuron. We can construct a so-called artificial neural network inside a computer, and then try to teach it to solve problems by giving it examples. This process is similar to how a newborn child learns about his or her surroundings by making observations.</p> <p>\u2705 Do a little research on how babies learn. What are the basic elements of a baby's brain?</p> What about ML? Part of Artificial Intelligence that is based on computer learning to solve a problem based on some data is called Machine Learning. We will not consider classical machine learning in this course - we refer you to a separate Machine Learning for Beginners curriculum."},{"location":"lessons/ai-for-beginners/1-Intro/#a-brief-history-of-ai","title":"A Brief History of AI","text":"<p>Artificial Intelligence was started as a field in the middle of the twentieth century. Initially, symbolic reasoning was a prevalent approach, and it led to a number of important successes, such as expert systems \u2013 computer programs that were able to act as an expert in some limited problem domains. However, it soon became clear that such approach does not scale well. Extracting the knowledge from an expert, representing it in a computer, and keeping that knowledgebase accurate turns out to be a very complex task, and too expensive to be practical in many cases. This led to so-called AI Winter in the 1970s.</p> <p></p> <p>Image by Dmitry Soshnikov</p> <p>As time passed, computing resources became cheaper, and more data has become available, so neural network approaches started demonstrating great performance in competing with human beings in many areas, such as computer vision or speech understanding. In the last decade, the term Artificial Intelligence has been mostly used as a synonym for Neural Networks, because most of the AI successes that we hear about are based on them.</p> <p>We can observe how the approaches changed, for example, in creating a chess playing computer program:</p> <ul> <li>Early chess programs were based on search \u2013 a program explicitly tried to estimate possible moves of an opponent for a given number of next moves, and selected an optimal move based on the optimal position that can be achieved in a few moves. It led to the development of the so-called alpha-beta pruning search algorithm.</li> <li>Search strategies work well toward the end of the game, where the search space is limited by a small number of possible moves. However, at the beginning of the game, the search space is huge, and the algorithm can be improved by learning from existing matches between human players. Subsequent experiments employed so-called case-based reasoning, where the program looked for cases in the knowledge base very similar to the current position in the game.</li> <li>Modern programs that win over human players are based on neural networks and reinforcement learning, where the programs learn to play solely by playing a long time against themselves and learning from their own mistakes \u2013 much like human beings do when learning to play chess. However, a computer program can play many more games in much less time, and thus can learn much faster.</li> </ul> <p>\u2705 Do a little research on other games that have been played by AI.</p> <p>Similarly, we can see how the approach towards creating \u201ctalking programs\u201d (that might pass the Turing test) changed:</p> <ul> <li>Early programs of this kind such as Eliza, were based on very simple grammatical rules and the re-formulation of the input sentence into a question.</li> <li>Modern assistants, such as Cortana, Siri or Google Assistant are all hybrid systems that use Neural networks to convert speech into text and recognize our intent, and then employ some reasoning or explicit algorithms to perform required actions.</li> <li>In the future, we may expect a complete neural-based model to handle dialogue by itself. The recent GPT and Turing-NLG family of neural networks show great success in this.</li> </ul> <p></p> <p>Image by Dmitry Soshnikov, photo by Marina Abrosimova, Unsplash</p>"},{"location":"lessons/ai-for-beginners/1-Intro/#recent-ai-research","title":"Recent AI Research","text":"<p>The huge recent growth in neural network research started around 2010, when large public datasets started to become available. A huge collection of images called ImageNet, which contains around 14 million annotated images, gave birth to the ImageNet Large Scale Visual Recognition Challenge.</p> <p></p> <p>Image by Dmitry Soshnikov</p> <p>In 2012, Convolutional Neural Networks were first used in image classification, which led to a significant drop in classification errors (from almost 30% to 16.4%). In 2015, ResNet architecture from Microsoft Research achieved human-level accuracy.</p> <p>Since then, Neural Networks demonstrated very successful behaviour in many tasks:</p> Year Human Parity achieved 2015 Image Classification 2016 Conversational Speech Recognition 2018 Automatic Machine Translation (Chinese-to-English) 2020 Image Captioning <p>Over the past few years we have witnessed huge successes with large language models, such as BERT and GPT-3. This happened mostly due to the fact that there is a lot of general text data available that allows us to train models to capture the structure and meaning of texts, pre-train them on general text collections, and then specialize those models for more specific tasks. We will learn more about Natural Language Processing later in this course.</p>"},{"location":"lessons/ai-for-beginners/1-Intro/#challenge","title":"\ud83d\ude80 Challenge","text":"<p>Do a tour of the internet to determine where, in your opinion, AI is most effectively used. Is it in a Mapping app, or some speech-to-text service or a video game? Research how the system was built.</p>"},{"location":"lessons/ai-for-beginners/1-Intro/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"lessons/ai-for-beginners/1-Intro/#review-self-study","title":"Review &amp; Self Study","text":"<p>Review the history of AI and ML by reading through this lesson. Take an element from the sketchnote at the top of that lesson or this one and research it in more depth to understand the cultural context informing its evolution.</p> <p>Assignment: Game Jam</p>"},{"location":"lessons/ai-for-beginners/2-Symbolic/","title":"Lesson 2","text":""},{"location":"lessons/ai-for-beginners/2-Symbolic/#knowledge-representation-and-expert-systems","title":"Knowledge Representation and Expert Systems","text":"<p>Sketchnote by Tomomi Imura</p> <p>The quest for artificial intelligence is based on a search for knowledge, to make sense of the world similar to how humans do. But how can you go about doing this?</p>"},{"location":"lessons/ai-for-beginners/2-Symbolic/#pre-lecture-quiz","title":"Pre-lecture quiz","text":"<p>In the early days of AI, the top-down approach to creating intelligent systems (discussed in the previous lesson) was popular. The idea was to extract the knowledge from people into some machine-readable form, and then use it to automatically solve problems. This approach was based on two big ideas:</p> <ul> <li>Knowledge Representation</li> <li>Reasoning</li> </ul>"},{"location":"lessons/ai-for-beginners/2-Symbolic/#knowledge-representation","title":"Knowledge Representation","text":"<p>One of the important concepts in Symbolic AI is knowledge. It is important to differentiate knowledge from information or data. For example, one can say that books contain knowledge, because one can study books and become an expert. However, what books contain is actually called data, and by reading books and integrating this data into our world model we convert this data to knowledge.</p> <p>\u2705 Knowledge is something which is contained in our head and represents our understanding of the world. It is obtained by an active learning process, which integrates pieces of information that we receive into our active model of the world.</p> <p>Most often, we do not strictly define knowledge, but we align it with other related concepts using DIKW Pyramid. It contains the following concepts:</p> <ul> <li>Data is something represented in physical media, such as written text or spoken words. Data exists independently of human beings and can be passed between people.</li> <li>Information is how we interpret data in our head. For example, when we hear the word computer, we have some understanding of what it is.</li> <li>Knowledge is information being integrated into our world model. For example, once we learn what a computer is, we start having some ideas about how it works, how much it costs, and what it can be used for. This network of interrelated concepts forms our knowledge.</li> <li>Wisdom is yet one more level of our understanding of the world, and it represents meta-knowledge, eg. some notion on how and when the knowledge should be used.</li> </ul> <p></p> <p>Image from Wikipedia, By Longlivetheux - Own work, CC BY-SA 4.0</p> <p>Thus, the problem of knowledge representation is to find some effective way to represent knowledge inside a computer in the form of data, to make it automatically usable. This can be seen as a spectrum:</p> <p></p> <p>Image by Dmitry Soshnikov</p> <ul> <li>On the left, there are very simple types of knowledge representations that can be effectively used by computers. The simplest one is algorithmic, when knowledge is represented by a computer program. This, however, is not the best way to represent knowledge, because it is not flexible. Knowledge inside our head is often non-algorithmic.</li> <li>On the right, there are representations such as natural text. It is the most powerful, but cannot be used for automatic reasoning.</li> </ul> <p>\u2705 Think for a minute about how you represent knowledge in your head and convert it to notes. Is there a particular format that works well for you to aid in retention?</p>"},{"location":"lessons/ai-for-beginners/2-Symbolic/#classifying-computer-knowledge-representations","title":"Classifying Computer Knowledge Representations","text":"<p>We can classify different computer knowledge representation methods in the following categories:</p> <ul> <li> <p>Network representations are based on the fact that we have a network of interrelated concepts inside our head. We can try to reproduce the same networks as a graph inside a computer - a so-called semantic network.</p> </li> <li> <p>Object-Attribute-Value triplets or attribute-value pairs. Since a graph can be represented inside a computer as a list of nodes and edges, we can represent a semantic network by a list of triplets, containing objects, attributes, and values. For example, we build the following triplets about programming languages:</p> </li> </ul> Object Attribute Value Python is Untyped-Language Python invented-by Guido van Rossum Python block-syntax indentation Untyped-Language doesn't have type definitions <p>\u2705 Think how triplets can be used to represent other types of knowledge.</p> <ol> <li> <p>Hierarchical representations emphasize the fact that we often create a hierarchy of objects inside our head. For example, we know that canary is a bird, and all birds have wings. We also have some idea about what colour a canary usually is, and what is their flight speed.</p> </li> <li> <p>Frame representation is based on representing each object or class of objects as a frame which contains slots. Slots have possible default values, value restrictions, or stored procedures that can be called to obtain the value of a slot. All frames form a hierarchy similar to an object hierarchy in object-oriented programming languages.</p> </li> <li>Scenarios are special kind of frames that represent complex situations that can unfold in time.</li> </ol> <p>Python</p> Slot Value Default value Interval Name Python Is-A Untyped-Language Variable Case CamelCase Program Length 5-5000 lines Block Syntax Indent <ol> <li>Procedural representations are based on representing knowledge by a list of actions that can be executed when a certain condition occurs.</li> <li>Production rules are if-then statements that allow us to draw conclusions. For example, a doctor can have a rule saying that IF a patient has high fever OR high level of C-reactive protein in blood test THEN he has an inflammation. Once we encounter one of the conditions, we can make a conclusion about inflammation, and then use it in further reasoning.</li> <li> <p>Algorithms can be considered another form of procedural representation, although they are almost never used directly in knowledge-based systems.</p> </li> <li> <p>Logic was originally proposed by Aristotle as a way to represent universal human knowledge.</p> </li> <li>Predicate Logic as a mathematical theory is too rich to be computable, therefore some subset of it is normally used, such as Horn clauses used in Prolog.</li> <li>Descriptive Logic is a family of logical systems used to represent and reason about hierarchies of objects distributed knowledge representations such as semantic web.</li> </ol>"},{"location":"lessons/ai-for-beginners/2-Symbolic/#expert-systems","title":"Expert Systems","text":"<p>One of the early successes of symbolic AI were so-called expert systems - computer systems that were designed to act as an expert in some limited problem domain. They were based on a knowledge base extracted from one or more human experts, and they contained an inference engine that performed some reasoning on top of it.</p> Simplified structure of a human neural system Architecture of a knowledge-based system <p>Expert systems are built like the human reasoning system, which contains short-term memory and long-term memory. Similarly, in knowledge-based systems we distinguish the following components:</p> <ul> <li>Problem memory: contains the knowledge about the problem being currently solved, i.e. the temperature or blood pressure of a patient, whether he has inflammation or not, etc. This knowledge is also called static knowledge, because it contains a snapshot of what we currently know about the problem - the so-called problem state.</li> <li>Knowledge base: represents long-term knowledge about a problem domain. It is extracted manually from human experts, and does not change from consultation to consultation. Because it allows us to navigate from one problem state to another, it is also called dynamic knowledge.</li> <li>Inference engine: orchestrates the whole process of searching in the problem state space, asking questions of the user when necessary. It is also responsible for finding the right rules to be applied to each state.</li> </ul> <p>As an example, let's consider the following expert system of determining an animal based on its physical characteristics:</p> <p></p> <p>Image by Dmitry Soshnikov</p> <p>This diagram is called an AND-OR tree, and it is a graphical representation of a set of production rules. Drawing a tree is useful at the beginning of extracting knowledge from the expert. To represent the knowledge inside the computer it is more convenient to use rules:</p> <pre><code>IF the animal eats meat\nOR (animal has sharp teeth\n    AND animal has claws\n    AND animal has forward-looking eyes\n) \nTHEN the animal is a carnivore\n</code></pre> <p>You can notice that each condition on the left-hand-side of the rule and the action are essentially object-attribute-value (OAV) triplets. Working memory contains the set of OAV triplets that correspond to the problem currently being solved. A rules engine looks for rules for which a condition is satisfied and applies them, adding another triplet to the working memory.</p> <p>\u2705 Write your own AND-OR tree on a topic you like!</p>"},{"location":"lessons/ai-for-beginners/2-Symbolic/#forward-vs-backward-inference","title":"Forward vs. Backward Inference","text":"<p>The process described above is called forward inference. It starts with some initial data about the problem available in the working memory, and then executes the following reasoning loop:</p> <ol> <li>If the target attribute is present in the working memory - stop and give the result</li> <li>Look for all the rules whose condition is currently satisfied - obtain conflict set of rules.</li> <li>Perform conflict resolution - select one rule that will be executed on this step. There could be different conflict resolution strategies:</li> <li>Select the first applicable rule in the knowledge base</li> <li>Select a random rule</li> <li>Select a more specific rule, i.e. the one meeting the most conditions in the \"left-hand-side\" (LHS)</li> <li>Apply selected rule and insert new piece of knowledge into the problem state</li> <li>Repeat from step 1.</li> </ol> <p>However, in some cases we might want to start with an empty knowledge about the problem, and ask questions that will help us arrive to the conclusion. For example, when doing medical diagnosis, we usually do not perform all medical analyses in advance before starting diagnosing the patient. We rather want to perform analyses when a decision needs to be made.</p> <p>This process can be modeled using backward inference. It is driven by the goal - the attribute value that we are looking to find:</p> <ol> <li>Select all rules that can give us the value of a goal (i.e. with the goal on the RHS (\"right-hand-side\")) - a conflict set</li> <li>If there are no rules for this attribute, or there is a rule saying that we should ask the value from the user - ask for it, otherwise:</li> <li>Use conflict resolution strategy to select one rule that we will use as hypothesis - we will try to prove it</li> <li>Recurrently repeat the process for all attributes in the LHS of the rule, trying to prove them as goals</li> <li>If at any point the process fails - use another rule at step 3.</li> </ol> <p>\u2705 In which situations is forward inference more appropriate? How about backward inference?</p>"},{"location":"lessons/ai-for-beginners/2-Symbolic/#implementing-expert-systems","title":"Implementing Expert Systems","text":"<p>Expert systems can be implemented using different tools:</p> <ul> <li>Programming them directly in some high level programming language. This is not the best idea, because the main advantage of a knowledge-based system is that knowledge is separated from inference, and potentially a problem domain expert should be able to write rules without understanding the details of the inference process</li> <li>Using expert systems shell, i.e. a system specifically designed to be populated by knowledge using some knowledge representation language.</li> </ul>"},{"location":"lessons/ai-for-beginners/2-Symbolic/#exercise-animal-inference","title":"\u270d\ufe0f Exercise: Animal Inference","text":"<p>See Animals.ipynb for an example of implementing forward and backward inference expert system.</p> <p>Note: This example is rather simple, and only gives the idea of how an expert system looks like. Once you start creating such a system, you will only notice some intelligent behaviour from it once you reach certain number of rules, around 200+. At some point, rules become too complex to keep all of them in mind, and at this point you may start wondering why a system makes certain decisions. However, the important characteristics of knowledge-based systems is that you can always explain exactly how any of the decisions were made.</p>"},{"location":"lessons/ai-for-beginners/2-Symbolic/#ontologies-and-the-semantic-web","title":"Ontologies and the Semantic Web","text":"<p>At the end of 20th century there was an initiative to use knowledge representation to annotate Internet resources, so that it would be possible to find resources that correspond to very specific queries. This motion was called Semantic Web, and it relied on several concepts:</p> <ul> <li>A special knowledge representation based on description logics (DL). It is similar to frame knowledge representation, because it builds a hierarchy of objects with properties, but it has formal logical semantics and inference. There is a whole family of DLs which balance between expressiveness and algorithmic complexity of inference.</li> <li>Distributed knowledge representation, where all concepts are represented by a global URI identifier, making it possible to create knowledge hierarchies that span the internet.</li> <li>A family of XML-based languages for knowledge description: RDF (Resource Description Framework), RDFS (RDF Schema), OWL (Ontology Web Language).</li> </ul> <p>A core concept in the Semantic Web is a concept of Ontology. It refers to a explicit specification of a problem domain using some formal knowledge representation. The simplest ontology can be just a hierarchy of objects in a problem domain, but more complex ontologies will include rules that can be used for inference.</p> <p>In the semantic web, all representations are based on triplets. Each object and each relation are uniquely identified by the URI. For example, if we want to state the fact that this AI Curriculum has been developed by Dmitry Soshnikov on Jan 1st, 2022 - here are the triplets we can use:</p> <p></p> <pre><code>http://github.com/microsoft/ai-for-beginners http://www.example.com/terms/creation-date \u201cJan 13, 2007\u201d\nhttp://github.com/microsoft/ai-for-beginners http://purl.org/dc/elements/1.1/creator http://soshnikov.com\n</code></pre> <p>\u2705 Here <code>http://www.example.com/terms/creation-date</code> and <code>http://purl.org/dc/elements/1.1/creator</code> are some well-known and universally accepted URIs to express the concepts of creator and creation date.</p> <p>In a more complex case, if we want to define a list of creators, we can use some data structures defined in RDF.</p> <p></p> <p>Diagrams above by Dmitry Soshnikov</p> <p>The progress of building the Semantic Web was somehow slowed down by the success of search engines and natural language processing techniques, which allow extracting structured data from text. However, in some areas there are still significant efforts to maintain ontologies and knowledge bases. A few projects worth noting:</p> <ul> <li>WikiData is a collection of machine readable knowledge bases associated with Wikipedia. Most of the data is mined from Wikipedia InfoBoxes, pieces of structured content inside Wikipedia pages. You can query wikidata in SPARQL, a special query language for Semantic Web. Here is a sample query that displays most popular eye colors among humans:</li> </ul> <pre><code>#defaultView:BubbleChart\nSELECT ?eyeColorLabel (COUNT(?human) AS ?count)\nWHERE\n{\n  ?human wdt:P31 wd:Q5.       # human instance-of homo sapiens\n  ?human wdt:P1340 ?eyeColor. # human eye-color ?eyeColor\n  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n}\nGROUP BY ?eyeColorLabel\n</code></pre> <ul> <li>DBpedia is another effort similar to WikiData.</li> </ul> <p>\u2705 If you want to experiment with building your own ontologies, or opening existing ones, there is a great visual ontology editor called Prot\u00e9g\u00e9. Download it, or use it online.</p> <p></p> <p>Web Prot\u00e9g\u00e9 editor open with the Romanov Family ontology. Screenshot by Dmitry Soshnikov</p>"},{"location":"lessons/ai-for-beginners/2-Symbolic/#exercise-a-family-ontology","title":"\u270d\ufe0f Exercise: A Family Ontology","text":"<p>See FamilyOntology.ipynb for an example of using Semantic Web techniques to reason about family relationships. We will take a family tree represented in common GEDCOM format and an ontology of family relationships and build a graph of all family relationships for given set of individuals.</p>"},{"location":"lessons/ai-for-beginners/2-Symbolic/#microsoft-concept-graph","title":"Microsoft Concept Graph","text":"<p>In most of the cases, ontologies are carefully created by hand. However, it is also possible to mine ontologies from unstructured data, for example, from natural language texts.</p> <p>One such attempt was done by Microsoft Research, and resulted in Microsoft Concept Graph.</p> <p>It is a large collection of entities grouped together using <code>is-a</code> inheritance relationship. It allows answering questions like \"What is Microsoft?\" - the answer being something like \"a company with probability 0.87, and a brand with probability 0.75\".</p> <p>The Graph is available either as REST API, or as a large downloadable text file that lists all entity pairs.</p>"},{"location":"lessons/ai-for-beginners/2-Symbolic/#exercise-a-concept-graph","title":"\u270d\ufe0f Exercise: A Concept Graph","text":"<p>Try the MSConceptGraph.ipynb notebook to see how we can use Microsoft Concept Graph to group news articles into several categories.</p>"},{"location":"lessons/ai-for-beginners/2-Symbolic/#conclusion","title":"Conclusion","text":"<p>Nowadays, AI is often considered to be a synonym for Machine Learning or Neural Networks. However, a human being also exhibits explicit reasoning, which is something currently not being handled by neural networks. In real world projects, explicit reasoning is still used to perform tasks that require explanations, or being able to modify the behavior of the system in a controlled way.</p>"},{"location":"lessons/ai-for-beginners/2-Symbolic/#challenge","title":"\ud83d\ude80 Challenge","text":"<p>In the Family Ontology notebook associated to this lesson, there is an opportunity to experiment with other family relations. Try to discover new connections between people in the family tree.</p>"},{"location":"lessons/ai-for-beginners/2-Symbolic/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"lessons/ai-for-beginners/2-Symbolic/#review-self-study","title":"Review &amp; Self Study","text":"<p>Do some research on the internet to discover areas where humans have tried to quantify and codify knowledge. Take a look at Bloom's Taxonomy, and go back in history to learn how humans tried to make sense of their world. Explore the work of Linnaeus to create a taxonomy of organisms, and observe the way Dmitri Mendeleev created a way for chemical elements to be described and grouped. What other interesting examples can you find?</p> <p>Assignment: Build an Ontology</p>"},{"location":"lessons/ai-for-beginners/3-NeuralNetworks/","title":"Intro","text":""},{"location":"lessons/ai-for-beginners/3-NeuralNetworks/#introduction-to-neural-networks","title":"Introduction to Neural Networks","text":"<p>As we discussed in the introduction, one of the ways to achieve intelligence is to train a computer model or an artificial brain. Since the middle of 20th century, researchers tried different mathematical models, until in recent years this direction proved to by hugely successful. Such mathematical models of the brain are called neural networks.</p> <p>Sometimes neural networks are called Artificial Neural Networks, ANNs, in order to indicate that we are talking about models, not real networks of neurons.</p>"},{"location":"lessons/ai-for-beginners/3-NeuralNetworks/#machine-learning","title":"Machine Learning","text":"<p>Neural Networks are a part of a larger discipline called Machine Learning, whose goal is to use data to train computer models that are able to solve problems. Machine Learning constitutes a large part of Artificial Intelligence, however, we do not cover classical ML in this curricula.</p> <p>Visit our separate Machine Learning for Beginners curriculum to learn more about classic Machine Learning.</p> <p>In Machine Learning, we assume that we have some dataset of examples X, and corresponding output values Y. Examples are often N-dimensional vectors that consist of features, and outputs are called labels.</p> <p>We will consider the two most common machine learning problems:</p> <ul> <li>Classification, where we need to classify an input object into two or more classes.</li> <li>Regression, where we need to predict a numerical number for each of the input samples.</li> </ul> <p>When representing inputs and outputs as tensors, the input dataset is a matrix of size M\u00d7N, where M is number of samples and N is the number of features. Output labels Y is the vector of size M.</p> <p>In this curriculum, we will only focus on neural network models.</p>"},{"location":"lessons/ai-for-beginners/3-NeuralNetworks/#a-model-of-a-neuron","title":"A Model of a Neuron","text":"<p>From biology we know that our brain consists of neural cells, each of them having multiple \"inputs\" (axons), and an output (dendrite). Axons and dendrites can conduct electrical signals, and connections between axons and dendrites can exhibit different degrees of conductivity (controlled by neuromediators).</p> Real Neuron (Image from Wikipedia) Artificial Neuron (Image by Author) <p>Thus, the simplest mathematical model of a neuron contains several inputs X<sub>1</sub>, ..., X<sub>N</sub> and an output Y, and a series of weights W<sub>1</sub>, ..., W<sub>N</sub>. An output is calculated as:</p> <p></p> <p>where f is some non-linear activation function.</p> <p>Early models of neuron were described in the classical paper A logical calculus of the ideas immanent in nervous activity by Warren McCullock and Walter Pitts in 1943. Donald Hebb in his book \"The Organization of Behavior: A Neuropsychological Theory\" proposed the way those networks can be trained.</p>"},{"location":"lessons/ai-for-beginners/3-NeuralNetworks/#in-this-section","title":"In this Section","text":"<p>In this section we will learn about: * Perceptron, one of the earliest neural network models for two-class classification * Multi-layered networks with a paired notebook how to build our own framework * Neural Network Frameworks, with these notebooks: PyTorch and Keras/Tensorflow * Overfitting</p>"},{"location":"lessons/ai-for-beginners/3-NeuralNetworks/03-Perceptron/","title":"Perceptron","text":""},{"location":"lessons/ai-for-beginners/3-NeuralNetworks/03-Perceptron/#introduction-to-neural-networks-perceptron","title":"Introduction to Neural Networks: Perceptron","text":""},{"location":"lessons/ai-for-beginners/3-NeuralNetworks/03-Perceptron/#pre-lecture-quiz","title":"Pre-lecture quiz","text":"<p>One of the first attempts to implement something similar to a modern neural network was done by Frank Rosenblatt from Cornell Aeronautical Laboratory in 1957. It was a hardware implementation called \"Mark-1\", designed to recognize primitive geometric figures, such as triangles, squares and circles.</p> <p>Images from Wikipedia</p> <p>An input image was represented by 20x20 photocell array, so the neural network had 400 inputs and one binary output. A simple network contained one neuron, also called a threshold logic unit. Neural network weights acted like potentiometers that required manual adjustment during the training phase.</p> <p>\u2705 A potentiometer is a device that allows the user to adjust the resistance of a circuit.</p> <p>The New York Times wrote about perceptron at that time: the embryo of an electronic computer that [the Navy] expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence.</p>"},{"location":"lessons/ai-for-beginners/3-NeuralNetworks/03-Perceptron/#perceptron-model","title":"Perceptron Model","text":"<p>Suppose we have N features in our model, in which case the input vector would be a vector of size N. A perceptron is a binary classification model, i.e. it can distinguish between two classes of input data. We will assume that for each input vector x the output of our perceptron would be either +1 or -1, depending on the class. The output will be computed using the formula:</p> <p>y(x) = f(w<sup>T</sup>x)</p> <p>where f is a step activation function</p> <p></p>"},{"location":"lessons/ai-for-beginners/3-NeuralNetworks/03-Perceptron/#training-the-perceptron","title":"Training the Perceptron","text":"<p>To train a perceptron we need to find a weights vector w that classifies most of the values correctly, i.e. results in the smallest error. This error E is defined by perceptron criterion in the following manner:</p> <p>E(w) = -\u2211w<sup>T</sup>x<sub>i</sub>t<sub>i</sub></p> <p>where:</p> <ul> <li>the sum is taken on those training data points i that result in the wrong classification</li> <li>x<sub>i</sub> is the input data, and t<sub>i</sub> is either -1 or +1 for negative and positive examples accordingly.</li> </ul> <p>This criteria is considered as a function of weights w, and we need to minimize it. Often, a method called gradient descent is used, in which we start with some initial weights w<sup>(0)</sup>, and then at each step update the weights according to the formula:</p> <p>w<sup>(t+1)</sup> = w<sup>(t)</sup> - \u03b7\u2207E(w)</p> <p>Here \u03b7 is the so-called learning rate, and \u2207E(w) denotes the gradient of E. After we calculate the gradient, we end up with</p> <p>w<sup>(t+1)</sup> = w<sup>(t)</sup> + \u2211\u03b7x<sub>i</sub>t<sub>i</sub></p> <p>The algorithm in Python looks like this:</p> <pre><code>def train(positive_examples, negative_examples, num_iterations = 100, eta = 1):\n\n    weights = [0,0,0] # Initialize weights (almost randomly :)\n\n    for i in range(num_iterations):\n        pos = random.choice(positive_examples)\n        neg = random.choice(negative_examples)\n\n        z = np.dot(pos, weights) # compute perceptron output\n        if z &lt; 0: # positive example classified as negative\n            weights = weights + eta*weights.shape\n\n        z  = np.dot(neg, weights)\n        if z &gt;= 0: # negative example classified as positive\n            weights = weights - eta*weights.shape\n\n    return weights\n</code></pre>"},{"location":"lessons/ai-for-beginners/3-NeuralNetworks/03-Perceptron/#conclusion","title":"Conclusion","text":"<p>In this lesson, you learned about a perceptron, which is a binary classification model, and how to train it by using a weights vector.</p>"},{"location":"lessons/ai-for-beginners/3-NeuralNetworks/03-Perceptron/#challenge","title":"\ud83d\ude80 Challenge","text":"<p>If you'd like to try to build your own perceptron, try this lab on Microsoft Learn which uses the Azure ML designer.</p>"},{"location":"lessons/ai-for-beginners/3-NeuralNetworks/03-Perceptron/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"lessons/ai-for-beginners/3-NeuralNetworks/03-Perceptron/#review-self-study","title":"Review &amp; Self Study","text":"<p>To see how we can use perceptron to solve a toy problem as well as real-life problems, and to continue learning - go to Perceptron notebook.</p> <p>Here's an interesting article about perceptrons as well.</p>"},{"location":"lessons/ai-for-beginners/3-NeuralNetworks/03-Perceptron/#assignment","title":"Assignment","text":"<p>In this lesson, we have implemented a perceptron for binary classification task, and we have used it to classify between two handwritten digits. In this lab, you are asked to solve the problem of digit classification entirely, i.e. determine which digit is most likely to correspond to a given image.</p> <ul> <li>Instructions</li> <li>Notebook</li> </ul>"},{"location":"lessons/ai-for-beginners/3-NeuralNetworks/04-OwnFramework/","title":"Own Framework","text":""},{"location":"lessons/ai-for-beginners/3-NeuralNetworks/04-OwnFramework/#introduction-to-neural-networks-multi-layered-perceptron","title":"Introduction to Neural Networks. Multi-Layered Perceptron","text":"<p>In the previous section, you learned about the simplest neural network model - one-layered perceptron, a linear two-class classification model.</p> <p>In this section we will extend this model into a more flexible framework, allowing us to:</p> <ul> <li>perform multi-class classification in addition to two-class</li> <li>solve regression problems in addition to classification</li> <li>separate classes that are not linearly separable</li> </ul> <p>We will also develop our own modular framework in Python that will allow us to construct different neural network architectures.</p>"},{"location":"lessons/ai-for-beginners/3-NeuralNetworks/04-OwnFramework/#pre-lecture-quiz","title":"Pre-lecture quiz","text":""},{"location":"lessons/ai-for-beginners/3-NeuralNetworks/04-OwnFramework/#formalization-of-machine-learning","title":"Formalization of Machine Learning","text":"<p>Let's start with formalizing the Machine Learning problem. Suppose we have a training dataset X with labels Y, and we need to build a model f that will make most accurate predictions. The quality of predictions is measured by Loss function &amp;lagran;. The following loss functions are often used:</p> <ul> <li>For regression problem, when we need to predict a number, we can use absolute error \u2211<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, or squared error \u2211<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup></li> <li>For classification, we use 0-1 loss (which is essentially the same as accuracy of the model), or logistic loss.</li> </ul> <p>For one-level perceptron, function f was defined as a linear function f(x)=wx+b (here w is the weight matrix, x is the vector of input features, and b is bias vector). For different neural network architectures, this function can take more complex form.</p> <p>In the case of classification, it is often desirable to get probabilities of corresponding classes as network output. To convert arbitrary numbers to probabilities (eg. to normalize the output), we often use softmax function \u03c3, and the function f becomes f(x)=\u03c3(wx+b)</p> <p>In the definition of f above, w and b are called parameters \u03b8=\u27e8w,b\u27e9. Given the dataset \u27e8X,Y\u27e9, we can compute an overall error on the whole dataset as a function of parameters \u03b8.</p> <p>\u2705 The goal of neural network training is to minimize the error by varying parameters \u03b8</p>"},{"location":"lessons/ai-for-beginners/3-NeuralNetworks/04-OwnFramework/#gradient-descent-optimization","title":"Gradient Descent Optimization","text":"<p>There is a well-known method of function optimization called gradient descent. The idea is that we can compute a derivative (in multi-dimensional case called gradient) of loss function with respect to parameters, and vary parameters in such a way that the error would decrease. This can be formalized as follows:</p> <ul> <li>Initialize parameters by some random values w<sup>(0)</sup>, b<sup>(0)</sup></li> <li>Repeat the following step many times:<ul> <li>w<sup>(i+1)</sup> = w<sup>(i)</sup>-\u03b7\u2202&amp;lagran;/\u2202w</li> <li>b<sup>(i+1)</sup> = b<sup>(i)</sup>-\u03b7\u2202&amp;lagran;/\u2202b</li> </ul> </li> </ul> <p>During training, the optimization steps are supposed to be calculated considering the whole dataset (remember that loss is calculated as a sum through all training samples). However, in real life we take small portions of the dataset called minibatches, and calculate gradients based on a subset of data. Because subset is taken randomly each time, such method is called stochastic gradient descent (SGD).</p>"},{"location":"lessons/ai-for-beginners/3-NeuralNetworks/04-OwnFramework/#multi-layered-perceptrons-and-backpropagation","title":"Multi-Layered Perceptrons and Backpropagation","text":"<p>One-layer network, as we have seen above, is capable of classifying linearly separable classes. To build a richer model, we can combine several layers of the network. Mathematically it would mean that the function f would have a more complex form, and will be computed in several steps: * z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub> * z<sub>2</sub>=w<sub>2</sub>\u03b1(z<sub>1</sub>)+b<sub>2</sub> * f = \u03c3(z<sub>2</sub>)</p> <p>Here, \u03b1 is a non-linear activation function, \u03c3 is a softmax function, and parameters \u03b8=&lt;w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>&gt;.</p> <p>The gradient descent algorithm would remain the same, but it would be more difficult to calculate gradients. Given the chain differentiation rule, we can calculate derivatives as:</p> <ul> <li>\u2202&amp;lagran;/\u2202w<sub>2</sub> = (\u2202&amp;lagran;/\u2202\u03c3)(\u2202\u03c3/\u2202z<sub>2</sub>)(\u2202z<sub>2</sub>/\u2202w<sub>2</sub>)</li> <li>\u2202&amp;lagran;/\u2202w<sub>1</sub> = (\u2202&amp;lagran;/\u2202\u03c3)(\u2202\u03c3/\u2202z<sub>2</sub>)(\u2202z<sub>2</sub>/\u2202\u03b1)(\u2202\u03b1/\u2202z<sub>1</sub>)(\u2202z<sub>1</sub>/\u2202w<sub>1</sub>)</li> </ul> <p>\u2705 The chain differentiation rule is used to calculate derivatives of the loss function with respect to parameters.</p> <p>Note that the left-most part of all those expressions is the same, and thus we can effectively calculate derivatives starting from the loss function and going \"backwards\" through the computational graph. Thus the method of training a multi-layered perceptron is called backpropagation, or 'backprop'.</p> <p></p> <p>TODO: image citation</p> <p>\u2705 We will cover backprop in much more detail in our notebook example.  </p>"},{"location":"lessons/ai-for-beginners/3-NeuralNetworks/04-OwnFramework/#conclusion","title":"Conclusion","text":"<p>In this lesson, we have built our own neural network library, and we have used it for a simple two-dimensional classification task.</p>"},{"location":"lessons/ai-for-beginners/3-NeuralNetworks/04-OwnFramework/#challenge","title":"\ud83d\ude80 Challenge","text":"<p>In the accompanying notebook, you will implement your own framework for building and training multi-layered perceptrons. You will be able to see in detail how modern neural networks operate.</p> <p>Proceed to the OwnFramework notebook and work through it.</p>"},{"location":"lessons/ai-for-beginners/3-NeuralNetworks/04-OwnFramework/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"lessons/ai-for-beginners/3-NeuralNetworks/04-OwnFramework/#review-self-study","title":"Review &amp; Self Study","text":"<p>Backpropagation is a common algorithm used in AI and ML, worth studying in more detail</p>"},{"location":"lessons/ai-for-beginners/3-NeuralNetworks/04-OwnFramework/#assignment","title":"Assignment","text":"<p>In this lab, you are asked to use the framework you constructed in this lesson to solve MNIST handwritten digit classification.</p> <ul> <li>Instructions</li> <li>Notebook</li> </ul>"},{"location":"lessons/ai-for-beginners/3-NeuralNetworks/05-Frameworks/","title":"Frameworks","text":""},{"location":"lessons/ai-for-beginners/3-NeuralNetworks/05-Frameworks/#neural-network-frameworks","title":"Neural Network Frameworks","text":"<p>As we have learned already, to be able to train neural networks efficiently we need to do two things:</p> <ul> <li>To operate on tensors, eg. to multiply, add, and compute some functions such as sigmoid or softmax</li> <li>To compute gradients of all expressions, in order to perform gradient descent optimization</li> </ul>"},{"location":"lessons/ai-for-beginners/3-NeuralNetworks/05-Frameworks/#pre-lecture-quiz","title":"Pre-lecture quiz","text":"<p>While the <code>numpy</code> library can do the first part, we need some mechanism to compute gradients. In our framework that we have developed in the previous section we had to manually program all derivative functions inside the <code>backward</code> method, which does backpropagation. Ideally, a framework should give us the opportunity to compute gradients of any expression that we can define.</p> <p>Another important thing is to be able to perform computations on GPU, or any other specialized compute units, such as TPU. Deep neural network training requires a lot of computations, and to be able to parallelize those computations on GPUs is very important.</p> <p>\u2705 The term 'parallelize' means to distribute the computations over multiple devices.</p> <p>Currently, the two most popular neural frameworks are: TensorFlow and PyTorch. Both provide a low-level API to operate with tensors on both CPU and GPU. On top of the low-level API, there is also higher-level API, called Keras and PyTorch Lightning correspondingly.</p> Low-Level API TensorFlow PyTorch High-level API Keras PyTorch Lightning <p>Low-level APIs in both frameworks allow you to build so-called computational graphs. This graph defines how to compute the output (usually the loss function) with given input parameters, and can be pushed for computation on GPU, if it is available. There are functions to differentiate this computational graph and compute gradients, which can then be used for optimizing model parameters.</p> <p>High-level APIs pretty much consider neural networks as a sequence of layers, and make constructing most of the neural networks much easier. Training the model usually requires preparing the data and then calling a <code>fit</code> function to do the job.</p> <p>The high-level API allows you to construct typical neural networks very quickly without worrying about lots of details. At the same time, low-level API offer much more control over the training process, and thus they are used a lot in research, when you are dealing with new neural network architectures.</p> <p>It is also important to understand that you can use both APIs together, eg. you can develop your own network layer architecture using low-level API, and then use it inside the larger network constructed and trained with the high-level API. Or you can define a network using the high-level API as a sequence of layers, and then use your own low-level training loop to perform optimization. Both APIs use the same basic underlying concepts, and they are designed to work well together.</p>"},{"location":"lessons/ai-for-beginners/3-NeuralNetworks/05-Frameworks/#learning","title":"Learning","text":"<p>In this course, we offer most of the content both for PyTorch and TensorFlow. You can choose your preferred framework and only go through the corresponding notebooks. If you are not sure which framework to choose, read some discussions on the internet regarding PyTorch vs. TensorFlow. You can also have a look at both frameworks to get better understanding.</p> <p>Where possible, we will use High-Level APIs for simplicity. However, we believe it is important to understand how neural networks work from the ground up, thus in the beginning we start by working with low-level API and tensors. However, if you want to get going fast and do not want to spend a lot of time on learning these details, you can skip those and go straight into high-level API notebooks.</p>"},{"location":"lessons/ai-for-beginners/3-NeuralNetworks/05-Frameworks/#exercises-frameworks","title":"\u270d\ufe0f Exercises: Frameworks","text":"<p>Continue your learning in the following notebooks:</p> Low-Level API TensorFlow+Keras Notebook PyTorch High-level API Keras PyTorch Lightning <p>After mastering the frameworks, let's recap the notion of overfitting.</p>"},{"location":"lessons/ai-for-beginners/3-NeuralNetworks/05-Frameworks/#overfitting","title":"Overfitting","text":"<p>Overfitting is an extremely important concept in machine learning, and it is very important to get it right!</p> <p>Consider the following problem of approximating 5 dots (represented by <code>x</code> on the graphs below):</p> Linear model, 2 parameters Non-linear model, 7 parameters Training error = 5.3 Training error = 0 Validation error = 5.1 Validation error = 20 <ul> <li>On the left, we see a good straight line approximation. Because the number of parameters is adequate, the model gets the idea behind point distribution right.</li> <li>On the right, the model is too powerful. Because we only have 5 points and the model has 7 parameters, it can adjust in such a way as to pass through all points, making training the error to be 0. However, this prevents the model from understanding the correct pattern behind data, thus the validation error is very high.</li> </ul> <p>It is very important to strike a correct balance between the richness of the model (number of parameters) and the number of training samples.</p>"},{"location":"lessons/ai-for-beginners/3-NeuralNetworks/05-Frameworks/#why-overfitting-occurs","title":"Why overfitting occurs","text":"<ul> <li>Not enough training data</li> <li>Too powerful model</li> <li>Too much noise in input data</li> </ul>"},{"location":"lessons/ai-for-beginners/3-NeuralNetworks/05-Frameworks/#how-to-detect-overfitting","title":"How to detect overfitting","text":"<p>As you can see from the graph above, overfitting can be detected by a very low training error, and a high validation error. Normally during training we will see both training and validation errors starting to decrease, and then at some point validation error might stop decreasing and start rising. This will be a sign of overfitting, and the indicator that we should probably stop training at this point (or at least make a snapshot of the model).</p> <p></p>"},{"location":"lessons/ai-for-beginners/3-NeuralNetworks/05-Frameworks/#how-to-prevent-overfitting","title":"How to prevent overfitting","text":"<p>If you can see that overfitting occurs, you can do one of the following:</p> <ul> <li>Increase the amount of training data</li> <li>Decrease the complexity of the model</li> <li>Use some regularization technique, such as Dropout, which we will consider later.</li> </ul>"},{"location":"lessons/ai-for-beginners/3-NeuralNetworks/05-Frameworks/#overfitting-and-bias-variance-tradeoff","title":"Overfitting and Bias-Variance Tradeoff","text":"<p>Overfitting is actually a case of a more generic problem in statistics called Bias-Variance Tradeoff. If we consider the possible sources of error in our model, we can see two types of errors:</p> <ul> <li>Bias errors are caused by our algorithm not being able to capture the relationship between training data correctly. It can result from the fact that our model is not powerful enough (underfitting).</li> <li>Variance errors, which are caused by the model approximating noise in the input data instead of meaningful relationship (overfitting).</li> </ul> <p>During training, bias error decreases (as our model learns to approximate the data), and variance error increases. It is important to stop training - either manually (when we detect overfitting) or automatically (by introducing regularization) - to prevent overfitting.</p>"},{"location":"lessons/ai-for-beginners/3-NeuralNetworks/05-Frameworks/#conclusion","title":"Conclusion","text":"<p>In this lesson, you learned about the differences between the various APIs for the two most popular AI frameworks, TensorFlow and PyTorch. In addition, you learned about a very important topic, overfitting.</p>"},{"location":"lessons/ai-for-beginners/3-NeuralNetworks/05-Frameworks/#challenge","title":"\ud83d\ude80 Challenge","text":"<p>In the accompanying notebooks, you will find 'tasks' at the bottom; work through the notebooks and complete the tasks.</p>"},{"location":"lessons/ai-for-beginners/3-NeuralNetworks/05-Frameworks/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"lessons/ai-for-beginners/3-NeuralNetworks/05-Frameworks/#review-self-study","title":"Review &amp; Self Study","text":"<p>Do some research on the following topics:</p> <ul> <li>TensorFlow</li> <li>PyTorch</li> <li>Overfitting</li> </ul> <p>Ask yourself the following questions:</p> <ul> <li>What is the difference between TensorFlow and PyTorch?</li> <li>What is the difference between overfitting and underfitting?</li> </ul>"},{"location":"lessons/ai-for-beginners/3-NeuralNetworks/05-Frameworks/#assignment","title":"Assignment","text":"<p>In this lab, you are asked to solve two classification problems using single- and multi-layered fully-connected networks using PyTorch or TensorFlow.</p> <ul> <li>Instructions</li> <li>Notebook</li> </ul>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/","title":"Lesson 4","text":""},{"location":"lessons/ai-for-beginners/4-ComputerVision/#computer-vision","title":"Computer Vision","text":"<p>In this section we will learn about:</p> <ul> <li>Intro to Computer Vision and OpenCV</li> <li>Convolutional Neural Networks</li> <li>Pre-trained Networks and Transfer Learning </li> <li>Autoencoders</li> <li>Generative Adversarial Networks</li> <li>Object Detection</li> <li>Semantic Segmentation</li> </ul>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/06-IntroCV/","title":"Index","text":""},{"location":"lessons/ai-for-beginners/4-ComputerVision/06-IntroCV/#introduction-to-computer-vision","title":"Introduction to Computer Vision","text":"<p>Computer Vision is a discipline whose aim is to allow computers to gain high-level understanding of digital images. This is quite a broad definition, because understanding can mean many different things, including finding an object on a picture (object detection), understanding what is happening (event detection), describing a picture in text, or reconstructing a scene in 3D. There are also special tasks related to human images: age and emotion estimation, face detection and identification, and 3D pose estimation, to name a few.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/06-IntroCV/#pre-lecture-quiz","title":"Pre-lecture quiz","text":"<p>One of the simplest tasks of computer vision is image classification.</p> <p>Computer vision is often considered to be a branch of AI. Nowadays, most of computer vision tasks are solved using neural networks. We will learn more about the special type of neural networks used for computer vision, convolutional neural networks, throughout this section.</p> <p>However, before you pass the image to a neural network, in many cases it makes sense to use some algorithmic techniques to enhance the image.</p> <p>There are several Python libraries available for image processing:</p> <ul> <li>imageio can be used for reading/writing different image formats. It also support ffmpeg, a useful tool to convert video frames to images.</li> <li>Pillow (also known as PIL) is a bit more powerful, and also supports some image manipulation such as morphing, palette adjustments, and more.</li> <li>OpenCV is a powerful image processing library written in C++, which has become the de facto standard for image processing. It has a convenient Python interface.</li> <li>dlib is a C++ library that implements many machine learning algorithms, including some of the Computer Vision algorithms. It also has a Python interface, and can be used for challenging tasks such as face and facial landmark detection.</li> </ul>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/06-IntroCV/#opencv","title":"OpenCV","text":"<p>OpenCV is considered to be the de facto standard for image processing. It contains a lot of useful algorithms, implemented in C++. You can call OpenCV from Python as well.</p> <p>A good place to learn OpenCV is this Learn OpenCV course. In our curriculum, our goal is not to learn OpenCV, but to show you some examples when it can be used, and how.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/06-IntroCV/#loading-images","title":"Loading Images","text":"<p>Images in Python can be conveniently represented by NumPy arrays. For example, grayscale images with the size of 320x200 pixels would be stored in a 200x320 array, and color images of the same dimension would have shape of 200x320x3 (for 3 color channels). To load an image, you can use the following code:</p> <pre><code>import cv2\nimport matplotlib.pyplot as plt\n\nim = cv2.imread('image.jpeg')\nplt.imshow(im)\n</code></pre> <p>Traditionally, OpenCV uses BGR (Blue-Green-Red) encoding for color images, while the rest of Python tools use the more traditional RGB (Red-Green-Blue). For the image to look right, you need to convert it to the RGB color space, either by swapping dimensions in the NumPy array, or by calling an OpenCV function:</p> <pre><code>im = cv2.cvtColor(im,cv2.COLOR_BGR2RGB)\n</code></pre> <p>The same <code>cvtColor</code> function can be used to perform other color space transformations such as converting an image to grayscale or to the HSV (Hue-Saturation-Value) color space.</p> <p>You can also use OpenCV to load video frame-by-frame - an example is given in the exercise OpenCV Notebook.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/06-IntroCV/#image-processing","title":"Image Processing","text":"<p>Before feeding an image to a neural network, you may want to apply several pre-processing steps. OpenCV can do many things, including:</p> <ul> <li>Resizing the image using <code>im = cv2.resize(im, (320,200),interpolation=cv2.INTER_LANCZOS)</code></li> <li>Blurring the image using <code>im = cv2.medianBlur(im,3)</code> or <code>im = cv2.GaussianBlur(im, (3,3), 0)</code></li> <li>Changing the brightness and contrast of the image can be done by NumPy array manipulations, as described in this Stackoverflow note.</li> <li>Using thresholding by calling <code>cv2.threshold</code>/<code>cv2.adaptiveThreshold</code> functions, which is often preferable to adjusting brightness or contrast.</li> <li>Applying different transformations to the image:<ul> <li>Affine transformations can be useful if you need to combine rotation, resizing and skewing to the image and you know the source and destination location of three points in the image. Affine transformations keep parallel lines parallel.</li> <li>Perspective transformations can be useful when you know the source and destination positions of 4 points in the image. For example, if you take a picture of a rectangular document via a smartphone camera from some angle, and you want to make a rectangular image of the document itself.</li> </ul> </li> <li>Understanding movement inside the image by using optical flow.</li> </ul>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/06-IntroCV/#examples-of-using-computer-vision","title":"Examples of using Computer Vision","text":"<p>In our OpenCV Notebook, we give some examples of when computer vision can be used to perform specific tasks:</p> <ul> <li>Pre-processing a photograph of a Braille book. We focus on how we can use thresholding, feature detection, perspective transformation and NumPy manipulations to separate individual Braille symbols for further classification by a neural network.</li> </ul> <p>Image from OpenCV.ipynb</p> <ul> <li>Detecting motion in video using frame difference. If the camera is fixed, then frames from the camera feed should be pretty similar to each other. Since frames are represented as arrays, just by subtracting those arrays for two subsequent frames we will get the pixel difference, which should be low for static frames, and become higher once there is substantial motion in the image.</li> </ul> <p></p> <p>Image from OpenCV.ipynb</p> <ul> <li> <p>Detecting motion using Optical Flow. Optical flow allows us to understand how individual pixels on video frames move. There are two types of optical flow:</p> </li> <li> <p>Dense Optical Flow computes the vector field that shows for each pixel where is it moving</p> </li> <li>Sparse Optical Flow is based on taking some distinctive features in the image (eg. edges), and building their trajectory from frame to frame.</li> </ul> <p></p> <p>Image from OpenCV.ipynb</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/06-IntroCV/#example-notebooks-opencv-try-opencv-in-action","title":"\u270d\ufe0f Example Notebooks: OpenCV try OpenCV in Action","text":"<p>Let's do some experiments with OpenCV by exploring OpenCV Notebook</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/06-IntroCV/#conclusion","title":"Conclusion","text":"<p>Sometimes, relatively complex tasks such as movement detection or fingertip detection can be solved purely by computer vision. Thus, it is very helpful to know the basic techniques of computer vision, and what libraries like OpenCV can do.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/06-IntroCV/#challenge","title":"\ud83d\ude80 Challenge","text":"<p>Watch this video from the AI show to learn about the Cortic Tigers project and how they built a block-based solution to democratize computer vision tasks via a robot. Do some research on other projects like this that help onboard new learners into the field.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/06-IntroCV/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"lessons/ai-for-beginners/4-ComputerVision/06-IntroCV/#review-self-study","title":"Review &amp; Self Study","text":"<p>Read more on optical flow in this great tutorial.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/06-IntroCV/#assignment","title":"Assignment","text":"<p>In this lab, you will take a video with simple gestures, and your goal is to extract up/down/left/right movements using optical flow.</p> <p></p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/07-ConvNets/","title":"Index","text":""},{"location":"lessons/ai-for-beginners/4-ComputerVision/07-ConvNets/#convolutional-neural-networks","title":"Convolutional Neural Networks","text":"<p>We have seen before that neural networks are quite good at dealing with images, and even one-layer perceptron is able to recognize handwritten digits from MNIST dataset with reasonable accuracy. However, the MNIST dataset is very special, and all digits are centered inside the image, which makes the task simpler.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/07-ConvNets/#pre-lecture-quiz","title":"Pre-lecture quiz","text":"<p>In real life, we want to be able to recognize objects on a picture regardless of their exact location in the image. Computer vision is different from generic classification, because when we are trying to find a certain object in the picture, we are scanning the image looking for some specific patterns and their combinations. For example, when looking for a cat, we first may look for horizontal lines, which can form whiskers, and then certain a combination of whiskers can tell us that it is actually a picture of a cat. Relative position and presence of certain patterns is important, and not their exact position on the image.</p> <p>To extract patterns, we will use the notion of convolutional filters. As you know, an image is represented by a 2D-matrix, or a 3D-tensor with color depth. Applying a filter means that we take relatively small filter kernel matrix, and for each pixel in the original image we compute the weighted average with neighboring points. We can view this like a small window sliding over the whole image, and averaging out all pixels according to the weights in the filter kernel matrix.</p> <p>Image by Dmitry Soshnikov</p> <p>For example, if we apply 3x3 vertical edge and horizontal edge filters to the MNIST digits, we can get highlights (e.g. high values) where there are vertical and horizontal edges in our original image. Thus those two filters can be used to \"look for\" edges. Similarly, we can design different filters to look for other low-level patterns:</p> <p></p> <p>Image of Leung-Malik Filter Bank</p> <p>However, while we can design the filters to extract some patterns manually, we can also design the network in such a way that it will learn the patterns automatically. It is one of the main ideas behind the CNN.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/07-ConvNets/#main-ideas-behind-cnn","title":"Main ideas behind CNN","text":"<p>The way CNNs work is based on the following important ideas:</p> <ul> <li>Convolutional filters can extract patterns</li> <li>We can design the network in such a way that filters are trained automatically</li> <li>We can use the same approach to find patterns in high-level features, not only in the original image. Thus CNN feature extraction work on a hierarchy of features, starting from low-level pixel combinations, up to higher level combination of picture parts.</li> </ul> <p></p> <p>Image from a paper by Hislop-Lynch, based on their research</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/07-ConvNets/#exercises-convolutional-neural-networks","title":"\u270d\ufe0f Exercises: Convolutional Neural Networks","text":"<p>Let's continue exploring how convolutional neural networks work, and how we can achieve trainable filters, by working through the corresponding notebooks:</p> <ul> <li>Convolutional Neural Networks - PyTorch</li> <li>Convolutional Neural Networks - TensorFlow</li> </ul>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/07-ConvNets/#pyramid-architecture","title":"Pyramid Architecture","text":"<p>Most of the CNNs used for image processing follow a so-called pyramid architecture. The first convolutional layer applied to the original images typically has a relatively low number of filters (8-16), which correspond to different pixel combinations, such as horizontal/vertical lines of strokes. At the next level, we reduce the spatial dimension of the network, and increase the number of filters, which corresponds to more possible combinations of simple features. With each layer, as we move towards the final classifier, spatial dimensions of the image decrease, and the number of filters grow.</p> <p>As an example, let's look at the architecture of VGG-16, a network that achieved 92.7% accuracy in ImageNet's top-5 classification in 2014:</p> <p></p> <p></p> <p>Image from Researchgate</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/07-ConvNets/#best-known-cnn-architectures","title":"Best-Known CNN Architectures","text":"<p>Continue your study about the best-known CNN architectures</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/07-ConvNets/CNN_Architectures/","title":"CNN Architectures","text":""},{"location":"lessons/ai-for-beginners/4-ComputerVision/07-ConvNets/CNN_Architectures/#well-known-cnn-architectures","title":"Well-Known CNN Architectures","text":""},{"location":"lessons/ai-for-beginners/4-ComputerVision/07-ConvNets/CNN_Architectures/#vgg-16","title":"VGG-16","text":"<p>VGG-16 is a network that achieved 92.7% accuracy in ImageNet top-5 classification in 2014. It has the following layer structure:</p> <p></p> <p>As you can see, VGG follows a traditional pyramid architecture, which is a sequence of convolution-pooling layers.</p> <p></p> <p>Image from Researchgate</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/07-ConvNets/CNN_Architectures/#resnet","title":"ResNet","text":"<p>ResNet is a family of models proposed by Microsoft Research in 2015. The main idea of ResNet is to use residual blocks:</p> <p></p> <p>Image from this paper</p> <p>The reason for using identity pass-through is to have our layer predict the difference between the result of a previous layer and the output of the residual block - hence the name residual. Those blocks are much easier to train, and one can construct networks with several hundreds of those blocks (most common variants are ResNet-52, ResNet-101 and ResNet-152).</p> <p>You can also think of this network as being able to adjust its complexity to the dataset. Initially, when you are starting to train the network, the weights values are small, and most of the signal goes through passthrough identity layers. As training progresses and weights become larger, the significance of network parameters grow, and the networks adjusts to accommodate required expressive power to correctly classify training images.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/07-ConvNets/CNN_Architectures/#google-inception","title":"Google Inception","text":"<p>Google Inception architecture takes this idea one step further, and builds each network layer as a combination of several different paths:</p> <p></p> <p>Image from Researchgate</p> <p>Here, we need to emphasize the role of 1x1 convolutions, because at first they do not make sense. Why would we need to run through the image with 1x1 filter? However, you need to remember that convolution filters also work with several depth channels (originally - RGB colors, in subsequent layers - channels for different filters), and 1x1 convolution is used to mix those input channels together using different trainable weights. It can be also viewed as downsampling (pooling) over channel dimension.</p> <p>Here is a good blog post on the subject, and the original paper.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/07-ConvNets/CNN_Architectures/#mobilenet","title":"MobileNet","text":"<p>MobileNet is a family of models with reduced size, suitable for mobile devices. Use them if you are short in resources, and can sacrifice a little bit of accuracy. The main idea behind them is so-called depthwise separable convolution, which allows representing convolution filters by a composition of spatial convolutions and 1x1 convolution over depth channels. This significantly reduces the number of parameters, making the network smaller in size, and also easier to train with less data.</p> <p>Here is a good blog post on MobileNet.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/07-ConvNets/CNN_Architectures/#conclusion","title":"Conclusion","text":"<p>In this unit, you have learned the main concept behind computer vision neural networks - convolutional networks. Real-life architectures that power image classification, object detection, and even image generation networks are all based on CNNs, just with more layers and some additional training tricks.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/07-ConvNets/CNN_Architectures/#challenge","title":"\ud83d\ude80 Challenge","text":"<p>In the accompanying notebooks, there are notes at the bottom about how to obtain greater accuracy. Do some experiments to see if you can achieve higher accuracy.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/07-ConvNets/CNN_Architectures/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"lessons/ai-for-beginners/4-ComputerVision/07-ConvNets/CNN_Architectures/#review-self-study","title":"Review &amp; Self Study","text":"<p>While CNNs are most often used for Computer Vision tasks, they are generally good for extracting fixed-sized patterns. For example, if we are dealing with sounds, we may also want to use CNNs to look for some specific patterns in audio signal - in which case filters would be 1-dimensional (and this CNN would be called 1D-CNN). Also, sometimes 3D-CNN is used to extract features in multi-dimensional space, such as certain events occurring on video - CNN can capture certain patterns of feature changing over time. Do some review and self-study about other tasks that can be done with CNNs.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/07-ConvNets/CNN_Architectures/#assignment","title":"Assignment","text":"<p>In this lab, you are tasked with classifying different cat and dog breeds. These images are more complex than the MNIST dataset and of higher dimensions, and there are more than 10 classes.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/08-TransferLearning/","title":"Index","text":""},{"location":"lessons/ai-for-beginners/4-ComputerVision/08-TransferLearning/#pre-trained-networks-and-transfer-learning","title":"Pre-trained Networks and Transfer Learning","text":"<p>Training CNNs can take a lot of time, and a lot of data is required for that task. However, much of the time is spent learning the best low-level filters that a network can use to extract patterns from images. A natural question arises - can we use a neural network trained on one dataset and adapt it to classify different images without requiring a full training process?</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/08-TransferLearning/#pre-lecture-quiz","title":"Pre-lecture quiz","text":"<p>This approach is called transfer learning, because we transfer some knowledge from one neural network model to another. In transfer learning, we typically start with a pre-trained model, which has been trained on some large image dataset, such as ImageNet. Those models can already do a good job extracting different features from generic images, and in many cases just building a classifier on top of those extracted features can yield a good result.</p> <p>\u2705 Transfer Learning is a term you find in other academic fields, such as Education. It refers to the process of taking knowledge from one domain and applying it to another.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/08-TransferLearning/#pre-trained-models-as-feature-extractors","title":"Pre-Trained Models as Feature Extractors","text":"<p>The convolutional networks that we have talked about in the previous section contained a number of layers, each of which is supposed to extract some features from the image, starting from low-level pixel combinations (such as horizontal/vertical line or stroke), up to higher level combinations of features, corresponding to things like an eye of a flame. If we train CNN on sufficiently large dataset of generic and diverse images, the network should learn to extract those common features.</p> <p>Both Keras and PyTorch contain functions to easily load pre-trained neural network weights for some common architectures, most of which were trained on ImageNet images. The most often used ones are described on the CNN Architectures page from the prior lesson. In particular, you may want to consider using one of the following:</p> <ul> <li>VGG-16/VGG-19 which are relatively simple models that still give good accuracy. Often using VGG as a first attempt is a good choice to see how transfer learning is working.</li> <li>ResNet is a family of models proposed by Microsoft Research in 2015. They have more layers, and thus take more resources.</li> <li>MobileNet is a family of models with reduced size, suitable for mobile devices. Use them if you are short in resources and can sacrifice a little bit of accuracy.</li> </ul> <p>Here are sample features extracted from a picture of a cat by VGG-16 network:</p> <p></p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/08-TransferLearning/#cats-vs-dogs-dataset","title":"Cats vs. Dogs Dataset","text":"<p>In this example, we will use a dataset of Cats and Dogs, which is very close to a real-life image classification scenario.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/08-TransferLearning/#exercise-transfer-learning","title":"\u270d\ufe0f Exercise: Transfer Learning","text":"<p>Let's see transfer learning in action in corresponding notebooks:</p> <ul> <li>Transfer Learning - PyTorch</li> <li>Transfer Learning - TensorFlow</li> </ul>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/08-TransferLearning/#visualizing-adversarial-cat","title":"Visualizing Adversarial Cat","text":"<p>Pre-trained neural network contains different patterns inside it's brain, including notions of ideal cat (as well as ideal dog, ideal zebra, etc.). It would be interesting to somehow visualize this image. However, it is not simple, because patterns are spread all over the network weights, and also organized in a hierarchical structure.</p> <p>One approach we can take is to start with a random image, and then try to use gradient descent optimization technique to adjust that image in such a way, that the network starts thinking that it's a cat. </p> <p></p> <p>However, if we do this, we will receive something very similar to a random noise. This is because there are many ways to make network think the input image is a cat, including some that do not make sense visually. While those images contain a lot of patterns typical for a cat, there is nothing to constrain them to be visually distinctive.</p> <p>To improve the result, we can add another term into the loss function, which is called variation loss. It is a metric that shows how similar neighboring pixels of the image are. Minimizing variation loss makes image smoother, and gets rid of noise - thus revealing more visually appealing patterns. Here is an example of such \"ideal\" images, that are classified as cat and as zebra with high probability:</p> Ideal Cat Ideal Zebra <p>Similar approach can be used to perform so-called adversarial attacks on a neural network. Suppose we want to fool a neural network and make a dog look like a cat. If we take dog's image, which is recognized by a network as a dog, we can then tweak it a little but using gradient descent optimization, until the network starts classifying it as a cat:</p> Original picture of a dog Picture of a dog classified as a cat <p>See the code to reproduce the results above in the following notebook:</p> <ul> <li>Ideal and Adversarial Cat - TensorFlow</li> </ul>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/08-TransferLearning/#conclusion","title":"Conclusion","text":"<p>Using transfer learning, you are able to quickly put together a classifier for a custom object classification task and achieve high accuracy. You can see that more complex tasks that we are solving now require higher computational power, and cannot be easily solved on the CPU. In the next unit, we will try to use a more lightweight implementation to train the same model using lower compute resources, which results in just slightly lower accuracy.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/08-TransferLearning/#challenge","title":"\ud83d\ude80 Challenge","text":"<p>In the accompanying notebooks, there are notes at the bottom about how transfer knowledge works best with somewhat similar training data (a new type of animal, perhaps). Do some experimentation with completely new types of images to see how well or poorly your transfer knowledge models perform.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/08-TransferLearning/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"lessons/ai-for-beginners/4-ComputerVision/08-TransferLearning/#review-self-study","title":"Review &amp; Self Study","text":"<p>Read through TrainingTricks.md to deepen your knowledge of some other way to train your models.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/08-TransferLearning/#assignment","title":"Assignment","text":"<p>In this lab, we will use real-life Oxford-IIIT pets dataset with 35 breeds of cats and dogs, and we will build a transfer learning classifier.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/08-TransferLearning/TrainingTricks/","title":"TrainingTricks","text":""},{"location":"lessons/ai-for-beginners/4-ComputerVision/08-TransferLearning/TrainingTricks/#deep-learning-training-tricks","title":"Deep Learning Training Tricks","text":"<p>As neural networks become deeper, the process of their training becomes more and more challenging. One major problem is so-called vanishing gradients or exploding gradients. This post gives a good introduction into those problems.</p> <p>To make training deep networks more efficient, there are a few techniques that can be used.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/08-TransferLearning/TrainingTricks/#keeping-values-in-reasonable-interval","title":"Keeping values in reasonable interval","text":"<p>To make numerical computations more stable, we want to make sure that all values within our neural network are within reasonable scale, typically [-1..1] or [0..1]. It is not a very strict requirement, but the nature of floating point computations is such that values of different magnitudes cannot be accurately manipulated together. For example, if we add 10<sup>-10</sup> and 10<sup>10</sup>, we are likely to get 10<sup>10</sup>, because smaller value would be \"converted\" to the same order as the larger one, and thus mantissa would be lost.</p> <p>Most activation functions have non-linearities around [-1..1], and thus it makes sense to scale all input data to [-1..1] or [0..1] interval.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/08-TransferLearning/TrainingTricks/#initial-weight-initialization","title":"Initial Weight Initialization","text":"<p>Ideally, we want the values to be in the same range after passing through network layers. Thus it is important to initialize weights in such a way as to preserve the distribution of values.</p> <p>Normal distribution N(0,1) is not a good idea, because if we have n inputs, the standard deviation of output would be n, and values are likely to jump out of [0..1] interval.</p> <p>The following initializations are often used:</p> <ul> <li>Uniform distribution -- <code>uniform</code></li> <li>N(0,1/n) -- <code>gaussian</code></li> <li>N(0,1/\u221an_in) guarantees that for inputs with zero mean and standard deviation of 1 the same mean/standard deviation would remain</li> <li>N(0,\u221a2/(n_in+n_out)) -- so-called Xavier initialization (<code>glorot</code>), it helps to keep the signals in range during both forward and backward propagation</li> </ul>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/08-TransferLearning/TrainingTricks/#batch-normalization","title":"Batch Normalization","text":"<p>Even with proper weight initialization, weights can get arbitrary big or small during the training, and they will bring signals out of proper range. We can bring signals back by using one of normalization techniques. While there are several of them (Weight normalization, Layer Normalization), the most often used is Batch Normalization.</p> <p>The idea of batch normalization is to take into account all values across the minibatch, and perform normalization (i.e. subtract mean and divide by standard deviation) based on those values. It is implemented as a network layer that does this normalization after applying the weights, but before activation function. As a result, we are likely to see higher final accuracy and faster training.</p> <p>Here is the original paper on batch normalization, the explanation on Wikipedia, and a good introductory blog post (and the one in Russian).</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/08-TransferLearning/TrainingTricks/#dropout","title":"Dropout","text":"<p>Dropout is an interesting technique that removes a certain percentage of random neurons during training. It is also implemented as a layer with one parameter (percentage of neurons to remove, typically 10%-50%), and during training it zeroes random elements of the input vector, before passing it to the next layer.</p> <p>While this may sound like a strange idea, you can see the effect of dropout on training MNIST digit classifier in <code>Dropout.ipynb</code> notebook. It speeds up training and allows us to achieve higher accuracy in less training epochs.</p> <p>This effect can be explained in several ways:</p> <ul> <li>It can be considered to be a random shocking factor to the model, which takes optimiation out of local minimum</li> <li>It can be considered as implicit model averaging, because we can say that during dropout we are training slightly different model</li> </ul> <p>Some people say that when a drunk person tries to learn something, he will remember this better next morning, comparing to a sober person, because a brain with some malfunctioning neurons tries to adapt better to gasp the meaning. We never tested ourselves if this is true of not</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/08-TransferLearning/TrainingTricks/#preventing-overfitting","title":"Preventing overfitting","text":"<p>One of the very important aspect of deep learning is too be able to prevent overfitting. While it might be tempting to use very powerful neural network model, we should always balance the number of model parameters with the number of training samples.</p> <p>Make sure you understand the concept of overfitting we have introduced earlier!</p> <p>There are several ways to prevent overfitting:</p> <ul> <li>Early stopping -- continuously monitor error on validation set and stopping training when validation error starts to increase.</li> <li>Explicit Weight Decay / Regularization -- adding an extra penalty to the loss function for high absolute values of weights, which prevents the model of getting very unstable results</li> <li>Model Averaging -- training several models and then averaging the result. This helps to minimize the variance.</li> <li>Dropout (Implicit Model Averaging)</li> </ul>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/08-TransferLearning/TrainingTricks/#optimizers-training-algorithms","title":"Optimizers / Training Algorithms","text":"<p>Another important aspect of training is to chose good training algorithm. While classical gradient descent is a reasonable choice, it can sometimes be too slow, or result in other problems.</p> <p>In deep learning, we use Stochastic Gradient Descent (SGD), which is a gradient descent applied to minibatches, randomly selected from the training set. Weights are adjusted using this formula:</p> <p>w<sup>t+1</sup> = w<sup>t</sup> - \u03b7\u2207&amp;lagran;</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/08-TransferLearning/TrainingTricks/#momentum","title":"Momentum","text":"<p>In momentum SGD, we are keeping a portion of a gradient from previous steps. It is similar to when we are moving somewhere with inertia, and we receive a punch in a different direction, our trajectory does not change immediately, but keeps some part of the original movement. Here we introduce another vector v to represent the speed:</p> <ul> <li>v<sup>t+1</sup> = \u03b3 v<sup>t</sup> - \u03b7\u2207&amp;lagran;</li> <li>w<sup>t+1</sup> = w<sup>t</sup>+v<sup>t+1</sup></li> </ul> <p>Here parameter \u03b3 indicates the extent to which we take inertia into account: \u03b3=0 corresponds to classical SGD; \u03b3=1 is a pure motion equation.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/08-TransferLearning/TrainingTricks/#adam-adagrad-etc","title":"Adam, Adagrad, etc.","text":"<p>Since in each layer we multiply signals by some matrix W<sub>i</sub>, depending on ||W<sub>i</sub>||, the gradient can either diminish and be close to 0, or rise indefinitely. It is the essence of Exploding/Vanishing Gradients problem.</p> <p>One of the solutions to this problem is to use only direction of the gradient in the equation, and ignore the absolute value, i.e.</p> <p>w<sup>t+1</sup> = w<sup>t</sup> - \u03b7(\u2207&amp;lagran;/||\u2207&amp;lagran;||), where ||\u2207&amp;lagran;|| = \u221a\u2211(\u2207&amp;lagran;)<sup>2</sup></p> <p>This algorithm is called Adagrad. Another algorithms that use the same idea: RMSProp, Adam</p> <p>Adam is considered to be a very efficient algorithm for many applications, so if you are not sure which one to use - use Adam.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/08-TransferLearning/TrainingTricks/#gradient-clipping","title":"Gradient clipping","text":"<p>Gradient clipping is an extension the idea above. When the ||\u2207&amp;lagran;|| \u2264 \u03b8, we consider the original gradient in the weight optimization, and when ||\u2207&amp;lagran;|| &gt; \u03b8 - we divide the gradient by it's norm. Here \u03b8 is a parameter, in most cases we can take \u03b8=1 or \u03b8=10.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/08-TransferLearning/TrainingTricks/#learning-rate-decay","title":"Learning rate decay","text":"<p>Training success often depends on the learning rate parameter \u03b7. It is logical to assume that larger values of \u03b7 result in faster training, which is something we typically want in the beginning of the training, and then smaller value of \u03b7 allow us to fine-tune the network. Thus, in most of the cases we want to decrease \u03b7 in the process of the training.</p> <p>This can be done by multiplying \u03b7 by some number (eg. 0.98) after each epoch of the training, or by using more complicated learning rate schedule.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/08-TransferLearning/TrainingTricks/#different-network-architectures","title":"Different Network Architectures","text":"<p>Selecting right network architecture for your problem can be tricky. Normally, we would take an architecture that has proven to work for our specific task (or similar one). Here is a good overview or neural network architectures for computer vision.</p> <p>It is important to select an architecture that will be powerful enough for the number of training samples that we have. Selecting too powerful model can result in overfitting</p> <p>Another good way would be to use and architecture that will automatically adjust to the required complexity. To some extent, ResNet architecture and Inception are self-adjusting. More on computer vision architectures</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/09-Autoencoders/","title":"Index","text":""},{"location":"lessons/ai-for-beginners/4-ComputerVision/09-Autoencoders/#autoencoders","title":"Autoencoders","text":"<p>When training CNNs, one of the problems is that we need a lot of labeled data. In the case of image classification, we need to separate images into different classes, which is a manual effort.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/09-Autoencoders/#pre-lecture-quiz","title":"Pre-lecture quiz","text":"<p>However, we might want to use raw (unlabeled) data for training CNN feature extractors, which is called self-supervised learning. Instead of labels, we will use training images as both network input and output. The main idea of autoencoder is that we will have an encoder network that converts input image into some latent space (normally it is just a vector of some smaller size), then the decoder network, whose goal would be to reconstruct the original image.</p> <p>\u2705 An autoencoder is \"a type of artificial neural network used to learn efficient codings of unlabeled data.\"</p> <p>Since we are training an autoencoder to capture as much of the information from the original image as possible for accurate reconstruction, the network tries to find the best embedding of input images to capture the meaning.\u043b.</p> <p></p> <p>Image from Keras blog</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/09-Autoencoders/#scenarios-for-using-autoencoders","title":"Scenarios for using Autoencoders","text":"<p>While reconstructing original images does not seem useful in its own right, there are a few scenarios where autoencoders are especially useful:</p> <ul> <li>Lowering the dimension of images for visualization or training image embeddings. Usually autoencoders give better results than PCA, because it takes into account spatial nature of images and hierarchical features.</li> <li>Denoising, i.e. removing noise from the image. Because noise carries out a lot of useless information, autoencoder cannot fit it all into relatively small latent space, and thus it captures only important part of the image. When training denoisers, we start with original images, and use images with artificially added noise as input for autoencoder.</li> <li>Super-resolution, increasing image resolution. We start with high-resolution images, and use the image with lower resolution as the autoencoder input.</li> <li>Generative models. Once we train the autoencoder, the decoder part can be used to create new objects starting from random latent vectors.</li> </ul>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/09-Autoencoders/#variational-autoencoders-vae","title":"Variational Autoencoders (VAE)","text":"<p>Traditional autoencoders reduce the dimension of the input data somehow, figuring out the important features of input images. However, latent vectors ofter do not make much sense. In other words, taking MNIST dataset as an example, figuring out which digits correspond to different latent vectors is not an easy task, because close latent vectors would not necessarily correspond to the same digits.</p> <p>On the other hand, to train generative models it is better to have some understanding of the latent space. This idea leads us to variational auto-encoder (VAE).</p> <p>VAE is the autoencoder that learns to predict statistical distribution of the latent parameters, so-called latent distribution. For example, we may want latent vectors to be distributed normally with some mean z<sub>mean</sub> and standard deviation z<sub>sigma</sub> (both mean and standard deviation are vectors of some dimensionality d). Encoder in VAE learns to predict those parameters, and then decoder takes a random vector from this distribution to reconstruct the object.</p> <p>To summarize:</p> <ul> <li>From input vector, we predict <code>z_mean</code> and <code>z_log_sigma</code> (instead of predicting the standard deviation itself, we predict its logarithm)</li> <li>We sample a vector <code>sample</code> from the distribution N(z<sub>mean</sub>,exp(z<sub>log_sigma</sub>))</li> <li>The decoder tries to decode the original image using <code>sample</code> as an input vector</li> </ul> <p></p> <p>Image from this blog post by Isaak Dykeman</p> <p>Variational auto-encoders use a complex loss function that consists of two parts:</p> <ul> <li>Reconstruction loss is the loss function that shows how close a reconstructed image is to the target (it can be Mean Squared Error, or MSE). It is the same loss function as in normal autoencoders.</li> <li>KL loss, which ensures that latent variable distributions stays close to normal distribution. It is based on the notion of Kullback-Leibler divergence - a metric to estimate how similar two statistical distributions are.</li> </ul> <p>One important advantage of VAEs is that they allow us to generate new images relatively easily, because we know which distribution from which to sample latent vectors. For example, if we train VAE with 2D latent vector on MNIST, we can then vary components of the latent vector to get different digits:</p> <p></p> <p>Image by Dmitry Soshnikov</p> <p>Observe how images blend into each other, as we start getting latent vectors from the different portions of the latent parameter space. We can also visualize this space in 2D:</p> <p> </p> <p>Image by Dmitry Soshnikov</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/09-Autoencoders/#exercises-autoencoders","title":"\u270d\ufe0f Exercises: Autoencoders","text":"<p>Learn more about autoencoders in these corresponding notebooks:</p> <ul> <li>Autoencoders in TensorFlow</li> <li>Autoencoders in PyTorch</li> </ul>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/09-Autoencoders/#properties-of-autoencoders","title":"Properties of Autoencoders","text":"<ul> <li>Data Specific - they only work well with the type of images they have been trained on. For example, if we train a super-resolution network on flowers, it will not work well on portraits. This is because the network can produce higher resolution image by taking fine details from features learned from the training dataset.</li> <li>Lossy - the reconstructed image is not the same as the original image. The nature of loss is defined by the loss function used during training</li> <li>Works on unlabeled data</li> </ul>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/09-Autoencoders/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"lessons/ai-for-beginners/4-ComputerVision/09-Autoencoders/#conclusion","title":"Conclusion","text":"<p>In this lesson, you learned about the various types of autoencoders available to the AI scientist. You learned how to build them, and how to use them to reconstruct images. You also learned about the VAE and how to use it to generate new images.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/09-Autoencoders/#challenge","title":"\ud83d\ude80 Challenge","text":"<p>In this lesson, you learned about using autoencoders for images. But they can also be used for music! Check out the Magenta project's MusicVAE project, which uses autoencoders to learn to reconstruct music. Do some experiments with this library to see what you can create.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/09-Autoencoders/#post-lecture-quiz_1","title":"Post-lecture quiz","text":""},{"location":"lessons/ai-for-beginners/4-ComputerVision/09-Autoencoders/#review-self-study","title":"Review &amp; Self Study","text":"<p>For reference, read more about autoencoders in these resources:</p> <ul> <li>Building Autoencoders in Keras</li> <li>Blog post on NeuroHive</li> <li>Variational Autoencoders Explained</li> <li>Conditional Variational Autoencoders</li> </ul>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/09-Autoencoders/#assignment","title":"Assignment","text":"<p>At the end of this notebook using TensorFlow, you will find a 'task' - use this as your assignment.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/10-GANs/","title":"Index","text":""},{"location":"lessons/ai-for-beginners/4-ComputerVision/10-GANs/#generative-adversarial-networks","title":"Generative Adversarial Networks","text":"<p>In the previous section, we learned about generative models: models that can generate new images similar to the ones in the training dataset. VAE was a good example of a generative model.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/10-GANs/#pre-lecture-quiz","title":"Pre-lecture quiz","text":"<p>However, if we try to generate something really meaningful, like a painting at reasonable resolution, with VAE, we will see that training does not converge well. For this use case, we should learn about another architecture specifically targeted at generative models - Generative Adversarial Networks, or GANs.</p> <p>The main idea of a GAN is to have two neural networks that will be trained against each other:</p> <p></p> <p>Image by Dmitry Soshnikov</p> <p>\u2705 A little vocabulary: * Generator is a network that takes some random vector, and produces the image as a result * Discriminator is a network that takes an image, and it should tell whether it is a real image (from training dataset), or it was generated by a generator. It is essentially an image classifier.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/10-GANs/#discriminator","title":"Discriminator","text":"<p>The architecture of discriminator does not differ from an ordinary image classification network. In the simplest case it can be fully-connected classifier, but most probably it will be a convolutional network.</p> <p>\u2705 A GAN based on convolutional networks is called a DCGAN</p> <p>A CNN discriminator consists of the following layers: several convolutions+poolings (with decreasing spatial size) and, one-or-more fully-connected layers to get \"feature vector\", final binary classifier.</p> <p>\u2705 A 'pooling' in this context is a technique that reduces the size of the image. \"Pooling layers reduce the dimensions of data by combining the outputs of neuron clusters at one layer into a single neuron in the next layer.\" - source</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/10-GANs/#generator","title":"Generator","text":"<p>A Generator is slightly more tricky. You can consider it to be a reversed discriminator. Starting from a latent vector (in place of a feature vector), it has a fully-connected layer to convert it into the required size/shape, followed by deconvolutions+upscaling. This is similar to decoder part of autoencoder.</p> <p>\u2705 Because the convolution layer is implemented as a linear filter traversing the image, deconvolution is essentially similar to convolution, and can be implemented using the same layer logic.</p> <p></p> <p>Image by Dmitry Soshnikov</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/10-GANs/#training-the-gan","title":"Training the GAN","text":"<p>GANs are called adversarial because there is a constant competition between the generator and the discriminator. During this competition, both generator and discriminator improve, thus the network learns to produce better and better pictures.</p> <p>The training happens in two stages:</p> <ul> <li>Training the discriminator. This task is pretty straightforward: we generate a batch of images by the generator, labeling them 0, which stands for fake image, and taking a batch of images from the input dataset (with label 1, real image). We obtain some discriminator loss, and perform backprop.</li> <li>Training the generator. This is slightly more tricky, because we do not know the expected output for the generator directly. We take the whole GAN network consisting of a generator followed by discriminator, feed it with some random vectors, and expect the result to be 1 (corresponding to real images). We then freeze the parameters of the discriminator (we do not want it to be trained at this step), and perform the backprop.</li> </ul> <p>During this process, both the generator and the discriminator losses are not going down significantly. In the ideal situation, they should oscillate, corresponding to both networks improving their performance.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/10-GANs/#exercises-gans","title":"\u270d\ufe0f Exercises: GANs","text":"<ul> <li>GAN Notebook in TensorFlow/Keras</li> <li>GAN Notebook in PyTorch</li> </ul>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/10-GANs/#problems-with-gan-training","title":"Problems with GAN training","text":"<p>GANs are known to be especially difficult to train. Here are a few problems:</p> <ul> <li>Mode Collapse. By this term we mean that the generator learns to produce one successful image that tricks the generator, and not a variety of different images.</li> <li>Sensitivity to hyperparameters. Often you can see that a GAN does not converge at all, and then suddenly decreases in the learning rate leading to convergence.</li> <li>Keeping a balance between the generator and the discriminator. In many cases discriminator loss can drop to zero relatively quickly, which results in the generator being unable to train further. To overcome this, we can try setting different learning rates for the generator and discriminator, or skip discriminator training if the loss is already too low.</li> <li>Training for high resolution. Reflecting the same problem as with autoencoders, this problem is triggered because reconstructing too many layers of convolutional network leads to artifacts. This problem is typically solved with so-called progressive growing, when first a few layers are trained on low-res images, and then layers are \"unblocked\" or added. Another solution would be adding extra connections between layers and training several resolutions at once - see this Multi-Scale Gradient GANs paper for details.</li> </ul>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/10-GANs/#style-transfer","title":"Style Transfer","text":"<p>GANs is a great way to generate artistic images. Another interesting technique is so-called style transfer, which takes one content image, and re-draws it in a different style, applying filters from style image. </p> <p>The way it works is the following: * We start with a random noise image (or with a content image, but for the sake of understanding it is easier to start from random noise) * Our goal would be to create such an image, that would be close to both content image and style image. This would be determined by two loss functions:    - Content loss is computed based on the features extracted by the CNN at some layers from current image and content image    - Style loss is computed between current image and style image in a clever way using Gram matrices (more details in the example notebook) * To make the image smoother and remove noise, we also introduce Variation loss, which computes average distance between neighboring pixels * The main optimization loop adjusts current image using gradient descent (or some other optimization algorithm) to minimize the total loss, which is a weighted sum of all three losses. </p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/10-GANs/#example-style-transfer","title":"\u270d\ufe0f Example: Style Transfer","text":""},{"location":"lessons/ai-for-beginners/4-ComputerVision/10-GANs/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"lessons/ai-for-beginners/4-ComputerVision/10-GANs/#conclusion","title":"Conclusion","text":"<p>In this lesson, you learned about GANS and how to train them. You also learned about the special challenges that this type of Neural Network can face, and some strategies on how to move past them.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/10-GANs/#challenge","title":"\ud83d\ude80 Challenge","text":"<p>Run through the Style Transfer notebook using your own images.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/10-GANs/#review-self-study","title":"Review &amp; Self Study","text":"<p>For reference, read more about GANs in these resources:</p> <ul> <li>Marco Pasini, 10 Lessons I Learned Training GANs for one Year</li> <li>StyleGAN, a de facto GAN architecture to consider</li> <li>Creating Generative Art using GANs on Azure ML</li> </ul>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/10-GANs/#assignment","title":"Assignment","text":"<p>Revisit one of the two notebooks associated to this lesson and retrain the GAN on your own images. What can you create?</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/11-ObjectDetection/","title":"Index","text":""},{"location":"lessons/ai-for-beginners/4-ComputerVision/11-ObjectDetection/#object-detection","title":"Object Detection","text":"<p>The image classification models we have dealt with so far took an image and produced a categorical result, such as the class 'number' in a MNIST problem. However, in many cases we do not want just to know that a picture portrays objects - we want to be able to determine their precise location. This is exactly the point of object detection.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/11-ObjectDetection/#pre-lecture-quiz","title":"Pre-lecture quiz","text":"<p>Image from YOLO v2 web site</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/11-ObjectDetection/#a-naive-approach-to-object-detection","title":"A Naive Approach to Object Detection","text":"<p>Assuming we wanted to find a cat on a picture, a very naive approach to object detection would be the following:</p> <ol> <li>Break the picture down to a number of tiles</li> <li>Run image classification on each tile.</li> <li>Those tiles that result in sufficiently high activation can be considered to contain the object in question.</li> </ol> <p></p> <p>Image from Exercise Notebook</p> <p>However, this approach is far from ideal, because it only allows the algorithm to locate the object's bounding box very imprecisely. For more precise location, we need to run some sort of regression to predict the coordinates of bounding boxes - and for that, we need specific datasets.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/11-ObjectDetection/#regression-for-object-detection","title":"Regression for Object Detection","text":"<p>This blog post has a great gentle introduction to detecting shapes.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/11-ObjectDetection/#datasets-for-object-detection","title":"Datasets for Object Detection","text":"<p>You might run across the following datasets for this task:</p> <ul> <li>PASCAL VOC - 20 classes</li> <li>COCO - Common Objects in Context. 80 classes, bounding boxes and segmentation masks</li> </ul> <p></p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/11-ObjectDetection/#object-detection-metrics","title":"Object Detection Metrics","text":""},{"location":"lessons/ai-for-beginners/4-ComputerVision/11-ObjectDetection/#intersection-over-union","title":"Intersection over Union","text":"<p>While for image classification it is easy to measure how well the algorithm performs, for object detection we need to measure both the correctness of the class, as well as the precision of the inferred bounding box location. For the latter, we use the so-called Intersection over Union (IoU), which measures how well two boxes (or two arbitrary areas) overlap.</p> <p></p> <p>Figure 2 from this excellent blog post on IoU</p> <p>The idea is simple - we divide the area of intersection between two figures by the area of their union. For two identical areas, IoU would be 1, while for completely disjointed areas it will be 0. Otherwise it will vary from 0 to 1. We typically only consider those bounding boxes for which IoU is over a certain value.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/11-ObjectDetection/#average-precision","title":"Average Precision","text":"<p>Suppose we want to measure how well a given class of objects $C$ is recognized. To measure it, we use Average Precision metrics, which is calculated as follows:</p> <ol> <li>Consider Precision-Recall curve shows the accuracy depending on a detection threshold value (from 0 to 1).</li> <li>Depending on the threshold, we will get more or less objects detected in the image, and different values of precision and recall.</li> <li>The curve will look like this:</li> </ol> <p></p> <p>Image from NeuroWorkshop</p> <p>The average Precision for a given class $C$ is the area under this curve. More precisely, Recall axis is typically divided into 10 parts, and Precision is averaged over all those points:</p> <p>$$ AP = {1\\over11}\\sum_{i=0}^{10}\\mbox{Precision}(\\mbox{Recall}={i\\over10}) $$</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/11-ObjectDetection/#ap-and-iou","title":"AP and IoU","text":"<p>We shall consider only those detections, for which IoU is above a certain value. For example, in PASCAL VOC dataset typically $\\mbox{IoU Threshold} = 0.5$ is assumed, while in COCO AP is measured for different values of $\\mbox{IoU Threshold}$.</p> <p></p> <p>Image from NeuroWorkshop</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/11-ObjectDetection/#mean-average-precision-map","title":"Mean Average Precision - mAP","text":"<p>The main metric for Object Detection is called Mean Average Precision, or mAP. It is the value of Average Precision, average across all object classes, and sometimes also over $\\mbox{IoU Threshold}$. In more detail, the process of calculating mAP is described in this blog post), and also here with code samples.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/11-ObjectDetection/#different-object-detection-approaches","title":"Different Object Detection Approaches","text":"<p>There are two broad classes of object detection algorithms:</p> <ul> <li>Region Proposal Networks (R-CNN, Fast R-CNN, Faster R-CNN). The main idea is to generate Regions of Interests (ROI) and run CNN over them, looking for maximum activation. It is a bit similar to the naive approach, with the exception that ROIs are generated in a more clever way. One of the majors drawbacks of such methods is that they are slow, because we need many passes of the CNN classifier over the image.</li> <li>One-pass (YOLO, SSD, RetinaNet) methods. In those architectures we design the network to predict both classes and ROIs in one pass.</li> </ul>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/11-ObjectDetection/#r-cnn-region-based-cnn","title":"R-CNN: Region-Based CNN","text":"<p>R-CNN uses Selective Search to generate hierarchical structure of ROI regions, which are then passed through CNN feature extractors and SVM-classifiers to determine the object class, and linear regression to determine bounding box coordinates. Official Paper</p> <p></p> <p>Image from van de Sande et al. ICCV\u201911</p> <p></p> <p>*Images from this blog</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/11-ObjectDetection/#f-rcnn-fast-r-cnn","title":"F-RCNN - Fast R-CNN","text":"<p>This approach is similar to R-CNN, but regions are defined after convolution layers have been applied.</p> <p></p> <p>Image from the Official Paper, arXiv, 2015</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/11-ObjectDetection/#faster-r-cnn","title":"Faster R-CNN","text":"<p>The main idea of this approach is to use neural network to predict ROIs - so-called Region Proposal Network. Paper, 2016</p> <p></p> <p>Image from the official paper</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/11-ObjectDetection/#r-fcn-region-based-fully-convolutional-network","title":"R-FCN: Region-Based Fully Convolutional Network","text":"<p>This algorithm is even faster than Faster R-CNN. The main idea is the following:</p> <ol> <li>We extract features using ResNet-101</li> <li>Features are processed by Position-Sensitive Score Map. Each object from $C$ classes is divided by $k\\times k$ regions, and we are training to predict parts of objects.</li> <li>For each part from $k\\times k$ regions all networks vote for object classes, and the object class with maximum vote is selected.</li> </ol> <p></p> <p>Image from official paper</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/11-ObjectDetection/#yolo-you-only-look-once","title":"YOLO - You Only Look Once","text":"<p>YOLO is a realtime one-pass algorithm. The main idea is the following:</p> <ul> <li>Image is divided into $S\\times S$ regions</li> <li>For each region, CNN predicts $n$ possible objects, bounding box coordinates and confidence=probability * IoU.</li> </ul> <p></p> <p>Image from official paper</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/11-ObjectDetection/#other-algorithms","title":"Other Algorithms","text":"<ul> <li>RetinaNet: official paper</li> <li>PyTorch Implementation in Torchvision</li> <li>Keras Implementation</li> <li>Object Detection with RetinaNet in Keras Samples</li> <li>SSD (Single Shot Detector): official paper</li> </ul>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/11-ObjectDetection/#exercises-object-detection","title":"\u270d\ufe0f Exercises: Object Detection","text":"<p>Continue your learning in the following notebook:</p> <p>ObjectDetection.ipynb</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/11-ObjectDetection/#conclusion","title":"Conclusion","text":"<p>In this lesson you took a whirlwind tour of all the various ways that object detection can be accomplished!</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/11-ObjectDetection/#challenge","title":"\ud83d\ude80 Challenge","text":"<p>Read through these articles and notebooks about YOLO and try them for yourself</p> <ul> <li>Good blog post describing YOLO</li> <li>Official site</li> <li>Yolo: Keras implementation, step-by-step notebook</li> <li>Yolo v2: Keras implementation, step-by-step notebook</li> </ul>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/11-ObjectDetection/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"lessons/ai-for-beginners/4-ComputerVision/11-ObjectDetection/#review-self-study","title":"Review &amp; Self Study","text":"<ul> <li>Object Detection by Nikhil Sardana</li> <li>A good comparison of object detection algorithms</li> <li>Review of Deep Learning Algorithms for Object Detection</li> <li>A Step-by-Step Introduction to the Basic Object Detection Algorithms</li> <li>Implementation of Faster R-CNN in Python for Object Detection</li> </ul>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/11-ObjectDetection/#assignment-object-detection","title":"Assignment: Object Detection","text":""},{"location":"lessons/ai-for-beginners/4-ComputerVision/12-Segmentation/","title":"Index","text":""},{"location":"lessons/ai-for-beginners/4-ComputerVision/12-Segmentation/#segmentation","title":"Segmentation","text":"<p>We have previously learned about Object Detection, which allows us to locate objects in the image by predicting their bounding boxes. However, for some tasks we do not only need bounding boxes, but also more precise object localization. This task is called  segmentation.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/12-Segmentation/#pre-lecture-quiz","title":"Pre-lecture quiz","text":"<p>Segmentation can be viewed as pixel classification, whereas for each pixel of image we must predict its class (background being one of the classes). There are two main segmentation algorithms:</p> <ul> <li>Semantic segmentation only tells the pixel class, and does not make a distinction between different objects of the same class</li> <li>Instance segmentation divides classes into different instances.</li> </ul> <p>For instance segmentation, these sheep are different objects, but for semantic segmentation all sheep are represented by one class.</p> <p></p> <p>Image from this blog post</p> <p>There are different neural architectures for segmentation, but they all have the same structure. In a way, it is similar to the autoencoder you learned about previously, but instead of deconstructing the original image, our goal is to deconstruct a mask. Thus, a segmentation network has the following parts:</p> <ul> <li>Encoder extracts features from input image</li> <li>Decoder transforms those features into the mask image, with the same size and number of channels corresponding to the number of classes.</li> </ul> <p></p> <p>Image from this publication</p> <p>We should especially mention the loss function that is used for segmentation. When using classical autoencoders, we need to measure the similarity between two images, and we can use mean square error (MSE) to do that. In segmentation, each pixel in the target mask image represents the class number (one-hot-encoded along the third dimension), so we need to use loss functions specific for classification - cross-entropy loss, averaged over all pixels. If the mask is binary - binary cross-entropy loss (BCE) is used.</p> <p>\u2705 One-hot encoding is a way to encode a class label into a vector of length equal to the number of classes. Take a look at this article on this technique.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/12-Segmentation/#segmentation-for-medical-imaging","title":"Segmentation for Medical Imaging","text":"<p>In this lesson, we will see the segmentation in action by training the network to recognize human nevi (also known as moles) on medical images. We will be using PH<sup>2</sup> Database of dermoscopy images as the image source. This dataset contains 200 images of three classes: typical nevus, atypical nevus, and melanoma. All images also contain a corresponding mask that outlines the nevus.</p> <p>\u2705 This technique is particularly appropriate for this type of medical imaging, but what other real-world applications could you envision?</p> <p></p> <p>Image from the PH<sup>2</sup> Database</p> <p>We will train a model to segment any nevus from its background.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/12-Segmentation/#exercises-semantic-segmentation","title":"\u270d\ufe0f Exercises: Semantic Segmentation","text":"<p>Open the notebooks below to learn more about different semantic segmentation architectures, practice working with them, and see them in action.</p> <ul> <li>Semantic Segmentation Pytorch</li> <li>Semantic Segmentation TensorFlow</li> </ul>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/12-Segmentation/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"lessons/ai-for-beginners/4-ComputerVision/12-Segmentation/#conclusion","title":"Conclusion","text":"<p>Segmentation is a very powerful technique for image classification, moving beyond bounding boxes to pixel-level classification. It is a technique used in medical imaging, among other applications.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/12-Segmentation/#challenge","title":"\ud83d\ude80 Challenge","text":"<p>Body segmentation is just one of the common tasks that we can do with images of people. Another important tasks include skeleton detection and pose detection. Try out OpenPose library to see how pose detection can be used.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/12-Segmentation/#review-self-study","title":"Review &amp; Self Study","text":"<p>This wikipedia article offers a good overview of the various applications of this technique. Learn more on your own about the subdomains of Instance segmentation and Panoptic segmentation in this field of inquiry.</p>"},{"location":"lessons/ai-for-beginners/4-ComputerVision/12-Segmentation/#assignment","title":"Assignment","text":"<p>In this lab, try human body segmentation using Segmentation Full Body MADS Dataset from Kaggle.</p>"},{"location":"lessons/ai-for-beginners/5-NLP/","title":"Lesson 5","text":""},{"location":"lessons/ai-for-beginners/5-NLP/#natural-language-processing","title":"Natural Language Processing","text":"<p>In this section, we will focus on using Neural Networks to handle tasks related to Natural Language Processing (NLP). There are many NLP problems that we want computers to be able to solve:</p> <ul> <li>Text classification is a typical classification problem pertaining to text sequences. Examples include classifying e-mail messages as spam vs. no-spam, or categorizing articles as sport, business, politics, etc. Also, when developing chat bots, we often need to understand what a user wanted to say -- in this case we are dealing with intent classification. Often, in intent classification we need to deal with many categories.</li> <li>Sentiment analysis is a typical regression problem, where we need to attribute a number (a sentiment) corresponding to how positive/negative the meaning of a sentence is. A more advanced version of sentiment analysis is aspect-based sentiment analysis (ABSA), where we attribute sentiment not to the whole sentence, but to different parts of it (aspects), eg. In this restaurant, I liked the cuisine, but the atmosphere was awful.</li> <li>Named Entity Recognition (NER) refers to the problem of extracting certain entities from text. For example, we might need to understand that in the phrase I need to fly to Paris tomorrow the word tomorrow refers to DATE, and Paris is a LOCATION.  </li> <li>Keyword extraction is similar to NER, but we need to extract words important to the meaning of the sentence automatically, without pre-training for specific entity types.</li> <li>Text clustering can be useful when we want to group together similar sentences, for example, similar requests in technical support conversations.</li> <li>Question answering refers to the ability of a model to answer a specific question. The model receives a text passage and a question as inputs, and it needs to provide a place in the text where the answer to the question is contained (or, sometimes, to generate the answer text).</li> <li>Text Generation is the ability of a model to generate new text. It can be considered as classification task that predicts next letter/word based on some text prompt. Advanced text generation models, such as GPT-3, are able to solve other NLP tasks such as classification using a technique called prompt programming or prompt engineering</li> <li>Text summarization is a technique when we want a computer to \"read\" long text and summarize it in a few sentences.</li> <li>Machine translation can be viewed as a combination of text understanding in one language, and text generation in another one.</li> </ul> <p>Initially, most of NLP tasks were solved using traditional methods such as grammars. For example, in machine translation parsers were used to transform initial sentence into a syntax tree, then higher level semantic structures were extracted to represent the meaning of the sentence, and based on this meaning and grammar of the target language the result was generated. Nowadays, many NLP tasks are more effectively solved using neural networks.</p> <p>Many classical NLP methods are implemented in Natural Language Processing Toolkit (NLTK) Python library. There is a great NLTK Book available online that covers how different NLP tasks can be solved using NLTK.</p> <p>In our course, we will mostly focus on using Neural Networks for NLP, and we will use NLTK where needed.</p> <p>We have already learned about using neural networks for dealing with tabular data and with images. The main difference between those types of data and text is that text is a sequence of variable length, while the input size in case of images is known in advance. While convolutional networks can extract patterns from input data, patterns in text are more complex. Eg., we can have negation being separated from the subject be arbitrary for many words (eg. I do not like oranges, vs. I do not like those big colorful tasty oranges), and that should still be interpreted as one pattern. Thus, to handle language we need to introduce new neural network types, such as recurrent networks and transformers.</p>"},{"location":"lessons/ai-for-beginners/5-NLP/#install-libraries","title":"Install Libraries","text":"<p>If you are using local Python installation to run this course, you may need to install all required libraries for NLP using the following commands:</p> <p>For PyTorch </p><pre><code>pip install -r requirements-torch.txt\n</code></pre> For TensorFlow <pre><code>pip install -r requirements-tf.txt\n</code></pre> <p>You can try NLP with TensorFlow on Microsoft Learn</p>"},{"location":"lessons/ai-for-beginners/5-NLP/#gpu-warning","title":"GPU Warning","text":"<p>In this section, in some of the examples we will be training quite large models. * Use a GPU-Enabled Computer: It's advisable to run your notebooks on a GPU-enabled computer to reduce waiting times when working with large models. * GPU Memory Constraints: Running on a GPU may lead to situations where you run out of GPU memory, especially when training large models. * GPU Memory Consumption: The amount of GPU memory consumed during training depends on various factors, including the minibatch size. * Minimize Minibatch Size: If you encounter GPU memory issues, consider reducing the minibatch size in your code as a potential solution. * TensorFlow GPU Memory Release: Older versions of TensorFlow may not release GPU memory correctly when training multiple models within one Python kernel. To manage GPU memory usage effectively, you can configure TensorFlow to allocate GPU memory only as needed. * Code Inclusion: To set TensorFlow to grow GPU memory allocation only when required, include the following code in your notebooks:</p> <pre><code>physical_devices = tf.config.list_physical_devices('GPU') \nif len(physical_devices)&gt;0:\n    tf.config.experimental.set_memory_growth(physical_devices[0], True) \n</code></pre> <p>If you're interested in learning about NLP from a classic ML perspective, visit this suite of lessons</p>"},{"location":"lessons/ai-for-beginners/5-NLP/#in-this-section","title":"In this Section","text":"<p>In this section we will learn about:</p> <ul> <li>Representing text as tensors</li> <li>Word Embeddings</li> <li>Language Modeling</li> <li>Recurrent Neural Networks</li> <li>Generative Networks</li> <li>Transformers</li> </ul>"},{"location":"lessons/ai-for-beginners/5-NLP/13-TextRep/","title":"Index","text":""},{"location":"lessons/ai-for-beginners/5-NLP/13-TextRep/#representing-text-as-tensors","title":"Representing Text as Tensors","text":""},{"location":"lessons/ai-for-beginners/5-NLP/13-TextRep/#pre-lecture-quiz","title":"Pre-lecture quiz","text":""},{"location":"lessons/ai-for-beginners/5-NLP/13-TextRep/#text-classification","title":"Text Classification","text":"<p>Throughout the first part of this section, we will focus on text classification task. We will use the AG News Dataset, which contains news articles like the following:</p> <ul> <li>Category: Sci/Tech</li> <li>Title: Ky. Company Wins Grant to Study Peptides (AP)</li> <li>Body: AP - A company founded by a chemistry researcher at the University of Louisville won a grant to develop...</li> </ul> <p>Our goal will be to classify the news item into one of the categories based on text.</p>"},{"location":"lessons/ai-for-beginners/5-NLP/13-TextRep/#representing-text","title":"Representing text","text":"<p>If we want to solve Natural Language Processing (NLP) tasks with neural networks, we need some way to represent text as tensors. Computers already represent textual characters as numbers that map to fonts on your screen using encodings such as ASCII or UTF-8.</p> <p></p> <p>Image source</p> <p>As humans, we understand what each letter represents, and how all characters come together to form the words of a sentence. However, computers by themselves do not have such an understanding, and neural network has to learn the meaning during training.</p> <p>Therefore, we can use different approaches when representing text:</p> <ul> <li>Character-level representation, when we represent text by treating each character as a number. Given that we have C different characters in our text corpus, the word Hello would be represented by 5xC tensor. Each letter would correspond to a tensor column in one-hot encoding.</li> <li>Word-level representation, in which we create a vocabulary of all words in our text, and then represent words using one-hot encoding. This approach is somehow better, because each letter by itself does not have much meaning, and thus by using higher-level semantic concepts - words - we simplify the task for the neural network. However, given the large dictionary size, we need to deal with high-dimensional sparse tensors.</li> </ul> <p>Regardless of the representation, we first need to convert the text into a sequence of tokens, one token being either a character, a word, or sometimes even part of a word. Then, we convert the token into a number, typically using vocabulary, and this number can be fed into a neural network using one-hot encoding.</p>"},{"location":"lessons/ai-for-beginners/5-NLP/13-TextRep/#n-grams","title":"N-Grams","text":"<p>In natural language, precise meaning of words can only be determined in context. For example, meanings of neural network and fishing network are completely different. One of the ways to take this into account is to build our model on pairs of words, and considering word pairs as separate vocabulary tokens. In this way, the sentence I like to go fishing will be represented by the following sequence of tokens: I like, like to, to go, go fishing. The problem with this approach is that the dictionary size grows significantly, and combinations like go fishing and go shopping are presented by different tokens, which do not share any semantic similarity despite the same verb.  </p> <p>In some cases, we may consider using tri-grams -- combinations of three words -- as well. Thus the approach is such is often called n-grams. Also, it makes sense to use n-grams with character-level representation, in which case n-grams will roughly correspond to different syllabi.</p>"},{"location":"lessons/ai-for-beginners/5-NLP/13-TextRep/#bag-of-words-and-tfidf","title":"Bag-of-Words and TF/IDF","text":"<p>When solving tasks like text classification, we need to be able to represent text by one fixed-size vector, which we will use as an input to final dense classifier. One of the simplest ways to do that is to combine all individual word representations, eg. by adding them. If we add one-hot encodings of each word, we will end up with a vector of frequencies, showing how many times each word appears inside the text. Such representation of text is called bag of words (BoW).</p> <p></p> <p>Image by the author</p> <p>A BoW essentially represents which words appear in text and in which quantities, which can indeed be a good indication of what the text is about. For example, news article on politics is likely to contains words such as president and country, while scientific publication would have something like collider, discovered, etc. Thus, word frequencies can in many cases be a good indicator of text content.</p> <p>The problem with BoW is that certain common words, such as and, is, etc. appear in most of the texts, and they have highest frequencies, masking out the words that are really important. We may lower the importance of those words by taking into account the frequency at which words occur in the whole document collection. This is the main idea behind TF/IDF approach, which is covered in more detail in the notebooks attached to this lesson.</p> <p>However, none of those approaches can fully take into account the semantics of text. We need more powerful neural networks models to do this, which we will discuss later in this section.</p>"},{"location":"lessons/ai-for-beginners/5-NLP/13-TextRep/#exercises-text-representation","title":"\u270d\ufe0f Exercises: Text Representation","text":"<p>Continue your learning in the following notebooks:</p> <ul> <li>Text Representation with PyTorch</li> <li>Text Representation with TensorFlow</li> </ul>"},{"location":"lessons/ai-for-beginners/5-NLP/13-TextRep/#conclusion","title":"Conclusion","text":"<p>So far, we have studied techniques that can add frequency weight to different words. They are, however, unable to represent meaning or order. As the famous linguist J. R. Firth said in 1935, \"The complete meaning of a word is always contextual, and no study of meaning apart from context can be taken seriously.\" We will learn later in the course how to capture contextual information from text using language modeling.</p>"},{"location":"lessons/ai-for-beginners/5-NLP/13-TextRep/#challenge","title":"\ud83d\ude80 Challenge","text":"<p>Try some other exercises using bag-of-words and different data models. You might be inspired by this competition on Kaggle</p>"},{"location":"lessons/ai-for-beginners/5-NLP/13-TextRep/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"lessons/ai-for-beginners/5-NLP/13-TextRep/#review-self-study","title":"Review &amp; Self Study","text":"<p>Practice your skills with text embeddings and bag-of-words techniques on Microsoft Learn</p>"},{"location":"lessons/ai-for-beginners/5-NLP/13-TextRep/#assignment-notebooks","title":"Assignment: Notebooks","text":""},{"location":"lessons/ai-for-beginners/5-NLP/14-Embeddings/","title":"Index","text":""},{"location":"lessons/ai-for-beginners/5-NLP/14-Embeddings/#embeddings","title":"Embeddings","text":""},{"location":"lessons/ai-for-beginners/5-NLP/14-Embeddings/#pre-lecture-quiz","title":"Pre-lecture quiz","text":"<p>When training classifiers based on BoW or TF/IDF, we operated on high-dimensional bag-of-words vectors with length <code>vocab_size</code>, and we were explicitly converting from low-dimensional positional representation vectors into sparse one-hot representation. This one-hot representation, however, is not memory-efficient. In addition, each word is treated independently from each other, i.e. one-hot encoded vectors do not express any semantic similarity between words.</p> <p>The idea of embedding is to represent words by lower-dimensional dense vectors, which somehow reflect the semantic meaning of a word. We will later discuss how to build meaningful word embeddings, but for now let's just think of embeddings as a way to lower dimensionality of a word vector.</p> <p>So, the embedding layer would take a word as an input, and produce an output vector of specified <code>embedding_size</code>. In a sense, it is very similar to a <code>Linear</code> layer, but instead of taking a one-hot encoded vector, it will be able to take a word number as an input, allowing us to avoid creating large one-hot-encoded vectors.</p> <p>By using an embedding layer as a first layer in our classifier network, we can switch from a bag-of-words to embedding bag model, where we first convert each word in our text into corresponding embedding, and then compute some aggregate function over all those embeddings, such as <code>sum</code>, <code>average</code> or <code>max</code>.  </p> <p></p> <p>Image by the author</p>"},{"location":"lessons/ai-for-beginners/5-NLP/14-Embeddings/#exercises-embeddings","title":"\u270d\ufe0f Exercises: Embeddings","text":"<p>Continue your learning in the following notebooks: * Embeddings with PyTorch * Embeddings TensorFlow</p>"},{"location":"lessons/ai-for-beginners/5-NLP/14-Embeddings/#semantic-embeddings-word2vec","title":"Semantic Embeddings: Word2Vec","text":"<p>While the embedding layer learned to map words to vector representation, however, this representation did not necessarily have much semantical meaning. It would be nice to learn a vector representation such that similar words or synonyms correspond to vectors that are close to each other in terms of some vector distance (eg. Euclidean distance).</p> <p>To do that, we need to pre-train our embedding model on a large collection of text in a specific way. One way to train semantic embeddings is called Word2Vec. It is based on two main architectures that are used to produce a distributed representation of words:</p> <ul> <li>Continuous bag-of-words (CBoW) \u2014 in this architecture, we train the model to predict a word from surrounding context. Given the ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$, the goal of the model is to predict $W_0$ from $(W_{-2},W_{-1},W_1,W_2)$.</li> <li>Continuous skip-gram is opposite to CBoW. The model uses surrounding window of context words to predict the current word.</li> </ul> <p>CBoW is faster, while skip-gram is slower, but does a better job of representing infrequent words.</p> <p></p> <p>Image from this paper</p> <p>Word2Vec pre-trained embeddings (as well as other similar models, such as GloVe) can also be used in place of embedding layer in neural networks. However, we need to deal with vocabularies, because the vocabulary used to pre-train Word2Vec/GloVe is likely to differ from the vocabulary in our text corpus. Have a look into the above Notebooks to see how this problem can be resolved.</p>"},{"location":"lessons/ai-for-beginners/5-NLP/14-Embeddings/#contextual-embeddings","title":"Contextual Embeddings","text":"<p>One key limitation of traditional pretrained embedding representations such as Word2Vec is the problem of word sense disambiguation. While pretrained embeddings can capture some of the meaning of words in context, every possible meaning of a word is encoded into the same embedding. This can cause problems in downstream models, since many words such as the word 'play' have different meanings depending on the context they are used in.</p> <p>For example word 'play' in those two different sentences have quite different meaning:</p> <ul> <li>I went to a play at the theatre.</li> <li>John wants to play with his friends.</li> </ul> <p>The pretrained embeddings above represent both of these meanings of the word 'play' in the same embedding. To overcome this limitation, we need to build embeddings based on the language model, which is trained on a large corpus of text, and knows how words can be put together in different contexts. Discussing contextual embeddings is out of scope for this tutorial, but we will come back to them when talking about language models later in the course.</p>"},{"location":"lessons/ai-for-beginners/5-NLP/14-Embeddings/#conclusion","title":"Conclusion","text":"<p>In this lesson, you discovered how to build and use embedding layers in TensorFlow and Pytorch to better reflect the semantic meanings of words.</p>"},{"location":"lessons/ai-for-beginners/5-NLP/14-Embeddings/#challenge","title":"\ud83d\ude80 Challenge","text":"<p>Word2Vec has been used for some interesting applications, including generating song lyrics and poetry. Take a look at this article which walks through how the author used Word2Vec to generate poetry. Watch this video by Dan Shiffmann as well to discover a different explanation of this technique. Then try to apply these techniques to your own text corpus, perhaps sourced from Kaggle.</p>"},{"location":"lessons/ai-for-beginners/5-NLP/14-Embeddings/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"lessons/ai-for-beginners/5-NLP/14-Embeddings/#review-self-study","title":"Review &amp; Self Study","text":"<p>Read through this paper on Word2Vec: Efficient Estimation of Word Representations in Vector Space</p>"},{"location":"lessons/ai-for-beginners/5-NLP/14-Embeddings/#assignment-notebooks","title":"Assignment: Notebooks","text":""},{"location":"lessons/ai-for-beginners/5-NLP/15-LanguageModeling/","title":"Index","text":""},{"location":"lessons/ai-for-beginners/5-NLP/15-LanguageModeling/#language-modeling","title":"Language Modeling","text":"<p>Semantic embeddings, such as Word2Vec and GloVe, are in fact a first step towards language modeling - creating models that somehow understand (or represent) the nature of the language.</p>"},{"location":"lessons/ai-for-beginners/5-NLP/15-LanguageModeling/#pre-lecture-quiz","title":"Pre-lecture quiz","text":"<p>The main idea behind language modeling is training them on unlabeled datasets in an unsupervised manner. This is important because we have huge amounts of unlabeled text available, while the amount of labeled text would always be limited by the amount of effort we can spend on labeling. Most often, we can build language models that can predict missing words in the text, because it is easy to mask out a random word in text and use it as a training sample.</p>"},{"location":"lessons/ai-for-beginners/5-NLP/15-LanguageModeling/#training-embeddings","title":"Training Embeddings","text":"<p>In our previous examples, we used pre-trained semantic embeddings, but it is interesting to see how those embeddings can be trained. There are several possible ideas the can be used:</p> <ul> <li>N-Gram language modeling, when we predict a token by looking at N previous tokens (N-gram)</li> <li>Continuous Bag-of-Words (CBoW), when we predict the middle token $W_0$ in a token sequence $W_{-N}$, ..., $W_N$.</li> <li>Skip-gram, where we predict a set of neighboring tokens {$W_{-N},\\dots, W_{-1}, W_1,\\dots, W_N$} from the middle token $W_0$.</li> </ul> <p></p> <p>Image from this paper</p>"},{"location":"lessons/ai-for-beginners/5-NLP/15-LanguageModeling/#example-notebooks-training-cbow-model","title":"\u270d\ufe0f Example Notebooks: Training CBoW model","text":"<p>Continue your learning in the following notebooks:</p> <ul> <li>Training CBoW Word2Vec with TensorFlow</li> <li>Training CBoW Word2Vec with PyTorch</li> </ul>"},{"location":"lessons/ai-for-beginners/5-NLP/15-LanguageModeling/#conclusion","title":"Conclusion","text":"<p>In the previous lesson we have seen that words embeddings work like magic! Now we know that training word embeddings is not a very complex task, and we should be able to train our own word embeddings for domain specific text if needed. </p>"},{"location":"lessons/ai-for-beginners/5-NLP/15-LanguageModeling/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"lessons/ai-for-beginners/5-NLP/15-LanguageModeling/#review-self-study","title":"Review &amp; Self Study","text":"<ul> <li>Official PyTorch tutorial on Language Modeling.</li> <li>Official TensorFlow tutorial on training Word2Vec model.</li> <li>Using the gensim framework to train most commonly used embeddings in a few lines of code is described in this documentation.</li> </ul>"},{"location":"lessons/ai-for-beginners/5-NLP/15-LanguageModeling/#assignment-train-skip-gram-model","title":"\ud83d\ude80 Assignment: Train Skip-Gram Model","text":"<p>In the lab, we challenge you to modify the code from this lesson to train skip-gram model instead of CBoW. Read the details</p>"},{"location":"lessons/ai-for-beginners/5-NLP/16-RNN/","title":"Index","text":""},{"location":"lessons/ai-for-beginners/5-NLP/16-RNN/#recurrent-neural-networks","title":"Recurrent Neural Networks","text":""},{"location":"lessons/ai-for-beginners/5-NLP/16-RNN/#pre-lecture-quiz","title":"Pre-lecture quiz","text":"<p>In previous sections, we have been using rich semantic representations of text and a simple linear classifier on top of the embeddings. What this architecture does is to capture the aggregated meaning of words in a sentence, but it does not take into account the order of words, because the aggregation operation on top of embeddings removed this information from the original text. Because these models are unable to model word ordering, they cannot solve more complex or ambiguous tasks such as text generation or question answering.</p> <p>To capture the meaning of text sequence, we need to use another neural network architecture, which is called a recurrent neural network, or RNN. In RNN, we pass our sentence through the network one symbol at a time, and the network produces some state, which we then pass to the network again with the next symbol.</p> <p></p> <p>Image by the author</p> <p>Given the input sequence of tokens X<sub>0</sub>,...,X<sub>n</sub>, RNN creates a sequence of neural network blocks, and trains this sequence end-to-end using backpropagation. Each network block takes a pair (X<sub>i</sub>,S<sub>i</sub>) as an input, and produces S<sub>i+1</sub> as a result. The final state S<sub>n</sub> or (output Y<sub>n</sub>) goes into a linear classifier to produce the result. All the network blocks share the same weights, and are trained end-to-end using one backpropagation pass.</p> <p>Because state vectors S<sub>0</sub>,...,S<sub>n</sub> are passed through the network, it is able to learn the sequential dependencies between words. For example, when the word not appears somewhere in the sequence, it can learn to negate certain elements within the state vector, resulting in negation.</p> <p>\u2705 Since the weights of all RNN blocks on the picture above are shared, the same picture can be represented as one block (on the right) with a recurrent feedback loop, which passes the output state of the network back to the input.</p>"},{"location":"lessons/ai-for-beginners/5-NLP/16-RNN/#anatomy-of-an-rnn-cell","title":"Anatomy of an RNN Cell","text":"<p>Let's see how a simple RNN cell is organized. It accepts the previous state S<sub>i-1</sub> and current symbol X<sub>i</sub> as inputs, and has to produce the output state S<sub>i</sub> (and, sometimes, we are also interested in some other output Y<sub>i</sub>, as in the case with generative networks).</p> <p>A simple RNN cell has two weight matrices inside: one transforms an input symbol (let's call it W), and another one transforms an input state (H). In this case the output of the network is calculated as \u03c3(W\u00d7X<sub>i</sub>+H\u00d7S<sub>i-1</sub>+b), where \u03c3 is the activation function and b is additional bias.</p> <p></p> <p>Image by the author</p> <p>In many cases, input tokens are passed through the embedding layer before entering the RNN to lower the dimensionality. In this case, if the dimension of the input vectors is emb_size, and state vector is hid_size - the size of W is emb_size\u00d7hid_size, and the size of H is hid_size\u00d7hid_size.</p>"},{"location":"lessons/ai-for-beginners/5-NLP/16-RNN/#long-short-term-memory-lstm","title":"Long Short Term Memory (LSTM)","text":"<p>One of the main problems of classical RNNs is the so-called vanishing gradients problem. Because RNNs are trained end-to-end in one backpropagation pass, it has difficulty propagating error to the first layers of the network, and thus the network cannot learn relationships between distant tokens. One of the ways to avoid this problem is to introduce explicit state management by using so called gates. There are two well-known architectures of this kind: Long Short Term Memory (LSTM) and Gated Relay Unit (GRU).</p> <p></p> <p>Image source TBD</p> <p>The LSTM Network is organized in a manner similar to RNN, but there are two states that are being passed from layer to layer: the actual state C, and the hidden vector H. At each unit, the hidden vector H<sub>i</sub> is concatenated with input X<sub>i</sub>, and they control what happens to the state C via gates. Each gate is a neural network with sigmoid activation (output in the range [0,1]), which can be thought of as a bitwise mask when multiplied by the state vector. There are the following gates (from left to right on the picture above):</p> <ul> <li>The forget gate takes a hidden vector and determines which components of the vector C we need to forget, and which to pass through.</li> <li>The input gate takes some information from the input and hidden vectors and inserts it into state.</li> <li>The output gate transforms state via a linear layer with tanh activation, then selects some of its components using a hidden vector H<sub>i</sub> to produce a new state C<sub>i+1</sub>.</li> </ul> <p>Components of the state C can be thought of as some flags that can be switched on and off. For example, when we encounter a name Alice in the sequence, we may want to assume that it refers to a female character, and raise the flag in the state that we have a female noun in the sentence. When we further encounter phrases and Tom, we will raise the flag that we have a plural noun. Thus by manipulating state we can supposedly keep track of the grammatical properties of sentence parts.</p> <p>\u2705 An excellent resource for understanding the internals of LSTM is this great article Understanding LSTM Networks by Christopher Olah.</p>"},{"location":"lessons/ai-for-beginners/5-NLP/16-RNN/#bidirectional-and-multilayer-rnns","title":"Bidirectional and Multilayer RNNs","text":"<p>We have discussed recurrent networks that operate in one direction, from beginning of a sequence to the end. It looks natural, because it resembles the way we read and listen to speech. However, since in many practical cases we have random access to the input sequence, it might make sense to run recurrent computation in both directions. Such networks are call bidirectional RNNs. When dealing with bidirectional network, we would need two hidden state vectors, one for each direction.</p> <p>A Recurrent network, either one-directional or bidirectional, captures certain patterns within a sequence, and can store them into a state vector or pass into output. As with convolutional networks, we can build another recurrent layer on top of the first one to capture higher level patterns and build from low-level patterns extracted by the first layer. This leads us to the notion of a multi-layer RNN which consists of two or more recurrent networks, where the output of the previous layer is passed to the next layer as input.</p> <p></p> <p>Picture from this wonderful post by Fernando L\u00f3pez</p>"},{"location":"lessons/ai-for-beginners/5-NLP/16-RNN/#exercises-embeddings","title":"\u270d\ufe0f Exercises: Embeddings","text":"<p>Continue your learning in the following notebooks:</p> <ul> <li>RNNs with PyTorch</li> <li>RNNs with TensorFlow</li> </ul>"},{"location":"lessons/ai-for-beginners/5-NLP/16-RNN/#conclusion","title":"Conclusion","text":"<p>In this unit, we have seen that RNNs can be used for sequence classification, but in fact, they can handle many more tasks, such as text generation, machine translation, and more. We will consider those tasks in the next unit.</p>"},{"location":"lessons/ai-for-beginners/5-NLP/16-RNN/#challenge","title":"\ud83d\ude80 Challenge","text":"<p>Read through some literature about LSTMs and consider their applications:</p> <ul> <li>Grid Long Short-Term Memory</li> <li>Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</li> </ul>"},{"location":"lessons/ai-for-beginners/5-NLP/16-RNN/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"lessons/ai-for-beginners/5-NLP/16-RNN/#review-self-study","title":"Review &amp; Self Study","text":"<ul> <li>Understanding LSTM Networks by Christopher Olah.</li> </ul>"},{"location":"lessons/ai-for-beginners/5-NLP/16-RNN/#assignment-notebooks","title":"Assignment: Notebooks","text":""},{"location":"lessons/ai-for-beginners/5-NLP/17-GenerativeNetworks/","title":"Index","text":""},{"location":"lessons/ai-for-beginners/5-NLP/17-GenerativeNetworks/#generative-networks","title":"Generative networks","text":""},{"location":"lessons/ai-for-beginners/5-NLP/17-GenerativeNetworks/#pre-lecture-quiz","title":"Pre-lecture quiz","text":"<p>Recurrent Neural Networks (RNNs) and their gated cell variants such as Long Short Term Memory Cells (LSTMs) and Gated Recurrent Units (GRUs) provided a mechanism for language modeling in that they can learn word ordering and provide predictions for the next word in a sequence. This allows us to use RNNs for generative tasks, such as ordinary text generation, machine translation, and even image captioning.</p> <p>\u2705 Think about all the times you've benefited from generative tasks such as text completion as you type. Do some research into your favorite applications to see if they leveraged RNNs.</p> <p>In RNN architecture we discussed in the previous unit, each RNN unit produced the next hidden state as an output. However, we can also add another output to each recurrent unit, which would allow us to output a sequence (which is equal in length to the original sequence). Moreover, we can use RNN units that do not accept an input at each step, and just take some initial state vector, and then produce a sequence of outputs.</p> <p>This allows for different neural architectures that are shown in the picture below:</p> <p></p> <p>Image from blog post Unreasonable Effectiveness of Recurrent Neural Networks by Andrej Karpaty</p> <ul> <li>One-to-one is a traditional neural network with one input and one output</li> <li>One-to-many is a generative architecture that accepts one input value, and generates a sequence of output values. For example, if we want to train an image captioning network that would produce a textual description of a picture, we can a picture as input, pass it through a CNN to obtain its hidden state, and then have a recurrent chain generate caption word-by-word</li> <li>Many-to-one corresponds to the RNN architectures we described in the previous unit, such as text classification</li> <li>Many-to-many, or sequence-to-sequence corresponds to tasks such as machine translation, where we have first RNN collect all information from the input sequence into the hidden state, and another RNN chain unrolls this state into the output sequence.</li> </ul> <p>In this unit, we will focus on simple generative models that help us generate text. For simplicity, we will use character-level tokenization.</p> <p>We will train this RNN to generate text step by step. On each step, we will take a sequence of characters of length <code>nchars</code>, and ask the network to generate the next output character for each input character:</p> <p></p> <p>When generating text (during inference), we start with some prompt, which is passed through RNN cells to generate its intermediate state, and then from this state the generation starts. We generate one character at a time, and pass the state and the generated character to another RNN cell to generate the next one, until we generate enough characters.</p> <p></p> <p>Image by the author</p>"},{"location":"lessons/ai-for-beginners/5-NLP/17-GenerativeNetworks/#exercises-generative-networks","title":"\u270d\ufe0f Exercises: Generative Networks","text":"<p>Continue your learning in the following notebooks:</p> <ul> <li>Generative Networks with PyTorch</li> <li>Generative Networks with TensorFlow</li> </ul>"},{"location":"lessons/ai-for-beginners/5-NLP/17-GenerativeNetworks/#soft-text-generation-and-temperature","title":"Soft text generation and temperature","text":"<p>The output of each RNN cell is a probability distribution of characters. If we always take the character with the highest probability as the next character in generated text, the text often can become \"cycled\" between the same character sequences again and again, like in this example:</p> <pre><code>today of the second the company and a second the company ...\n</code></pre> <p>However, if we look at the probability distribution for the next character, it could be that the difference between a few highest probabilities is not huge, e.g. one character can have probability 0.2, another - 0.19, etc. For example, when looking for the next character in the sequence 'play', next character can equally well be either space, or e (as in the word player).</p> <p>This leads us to the conclusion that it is not always \"fair\" to select the character with a higher probability, because choosing the second highest might still lead us to meaningful text. It is more wise to sample characters from the probability distribution given by the network output. We can also use a parameter, temperature, that will flatten out the probability distribution, in case we want to add more randomness, or make it more steep, if we want to stick more to the highest-probability characters.</p> <p>Explore how this soft text generation is implemented in the notebooks linked above.</p>"},{"location":"lessons/ai-for-beginners/5-NLP/17-GenerativeNetworks/#conclusion","title":"Conclusion","text":"<p>While text generation may be useful in its own right, the major benefits come from the ability to generate text using RNNs from some initial feature vector. For example, text generation is used as part of machine translation (sequence-to-sequence, in this case state vector from encoder is used to generate or decode translated message), or generating textual description of an image (in which case the feature vector would come from CNN extractor).</p>"},{"location":"lessons/ai-for-beginners/5-NLP/17-GenerativeNetworks/#challenge","title":"\ud83d\ude80 Challenge","text":"<p>Take some lessons on Microsoft Learn on this topic</p> <ul> <li>Text Generation with PyTorch/TensorFlow</li> </ul>"},{"location":"lessons/ai-for-beginners/5-NLP/17-GenerativeNetworks/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"lessons/ai-for-beginners/5-NLP/17-GenerativeNetworks/#review-self-study","title":"Review &amp; Self Study","text":"<p>Here are some articles to expand your knowledge</p> <ul> <li>Different approaches to text generation with Markov Chain, LSTM and GPT-2: blog post</li> <li>Text generation sample in Keras documentation</li> </ul>"},{"location":"lessons/ai-for-beginners/5-NLP/17-GenerativeNetworks/#assignment","title":"Assignment","text":"<p>We have seen how to generate text character-by-character. In the lab, you will explore word-level text generation.</p>"},{"location":"lessons/ai-for-beginners/5-NLP/18-Transformers/READMEtransformers/","title":"READMEtransformers","text":""},{"location":"lessons/ai-for-beginners/5-NLP/18-Transformers/READMEtransformers/#attention-mechanisms-and-transformers","title":"Attention Mechanisms and Transformers","text":""},{"location":"lessons/ai-for-beginners/5-NLP/18-Transformers/READMEtransformers/#pre-lecture-quiz","title":"Pre-lecture quiz","text":"<p>One of the most important problems in the NLP domain is machine translation, an essential task that underlies tools such as Google Translate. In this section, we will focus on machine translation, or, more generally, on any sequence-to-sequence task (which is also called sentence transduction).</p> <p>With RNNs, sequence-to-sequence is implemented by two recurrent networks, where one network, the encoder, collapses an input sequence into a hidden state, while another network, the decoder, unrolls this hidden state into a translated result. There are a couple of problems with this approach:</p> <ul> <li>The final state of the encoder network has a hard time remembering the beginning of a sentence, thus causing poor quality of the model for long sentences</li> <li>All words in a sequence have the same impact on the result. In reality, however, specific words in the input sequence often have more impact on sequential outputs than others.</li> </ul> <p>Attention Mechanisms provide a means of weighting the contextual impact of each input vector on each output prediction of the RNN. The way it is implemented is by creating shortcuts between intermediate states of the input RNN and the output RNN. In this manner, when generating output symbol y<sub>t</sub>, we will take into account all input hidden states h<sub>i</sub>, with different weight coefficients \u03b1<sub>t,i</sub>.</p> <p></p> <p>The encoder-decoder model with additive attention mechanism in Bahdanau et al., 2015, cited from this blog post</p> <p>The attention matrix {\u03b1<sub>i,j</sub>} would represent the degree that certain input words play in the generation of a given word in the output sequence. Below is an example of such a matrix:</p> <p></p> <p>Figure from Bahdanau et al., 2015 (Fig.3)</p> <p>Attention mechanisms are responsible for much of the current or near current state of the art in NLP. Adding attention however greatly increases the number of model parameters which led to scaling issues with RNNs. A key constraint of scaling RNNs is that the recurrent nature of the models makes it challenging to batch and parallelize training. In an RNN each element of a sequence needs to be processed in sequential order which means it cannot be easily parallelized.</p> <p></p> <p>Figure from Google's Blog</p> <p>The adoption of attention mechanisms combined with this constraint led to the creation of the now State of the Art Transformer Models that we know and use today such as BERT to Open-GPT3.</p>"},{"location":"lessons/ai-for-beginners/5-NLP/18-Transformers/READMEtransformers/#transformer-models","title":"Transformer models","text":"<p>One of the main ideas behind transformers is to avoid sequential nature of RNNs and to create a model that is parallelizable during training. This is achieved by implementing two ideas:</p> <ul> <li>positional encoding</li> <li>using self-attention mechanism to capture patterns instead of RNNs (or CNNs) (that is why the paper that introduces transformers is called Attention is all you need</li> </ul>"},{"location":"lessons/ai-for-beginners/5-NLP/18-Transformers/READMEtransformers/#positional-encodingembedding","title":"Positional Encoding/Embedding","text":"<p>The idea of positional encoding is the following.  1. When using RNNs, the relative position of the tokens is represented by the number of steps, and thus does not need to be explicitly represented.  2. However, once we switch to attention, we need to know the relative positions of tokens within a sequence.  3. To get positional encoding, we augment our sequence of tokens with a sequence of token positions in the sequence (i.e., a sequence of numbers 0,1, ...). 4. We then mix the token position with a token embedding vector. To transform the position (integer) into a vector, we can use different approaches:</p> <ul> <li>Trainable embedding, similar to token embedding. This is the approach we consider here. We apply embedding layers on top of both tokens and their positions, resulting in embedding vectors of the same dimensions, which we then add together.</li> <li>Fixed position encoding function, as proposed in the original paper.</li> </ul> <p></p> <p>Image by the author</p> <p>The result that we get with positional embedding embeds both the original token and its position within a sequence.</p>"},{"location":"lessons/ai-for-beginners/5-NLP/18-Transformers/READMEtransformers/#multi-head-self-attention","title":"Multi-Head Self-Attention","text":"<p>Next, we need to capture some patterns within our sequence. To do this, transformers use a self-attention mechanism, which is essentially attention applied to the same sequence as the input and output. Applying self-attention allows us to take into account context within the sentence, and see which words are inter-related. For example, it allows us to see which words are referred to by coreferences, such as it, and also take the context into account:</p> <p></p> <p>Image from the Google Blog</p> <p>In transformers, we use Multi-Head Attention in order to give the network the power to capture several different types of dependencies, eg. long-term vs. short-term word relations, co-reference vs. something else, etc.</p> <p>TensorFlow Notebook contains more detains on the implementation of transformer layers.</p>"},{"location":"lessons/ai-for-beginners/5-NLP/18-Transformers/READMEtransformers/#encoder-decoder-attention","title":"Encoder-Decoder Attention","text":"<p>In transformers, attention is used in two places:</p> <ul> <li>To capture patterns within the input text using self-attention</li> <li>To perform sequence translation - it is the attention layer between encoder and decoder.</li> </ul> <p>Encoder-decoder attention is very similar to the attention mechanism used in RNNs, as described in the beginning of this section. This animated diagram explains the role of encoder-decoder attention.</p> <p></p> <p>Since each input position is mapped independently to each output position, transformers can parallelize better than RNNs, which enables much larger and more expressive language models. Each attention head can be used to learn different relationships between words that improves downstream Natural Language Processing tasks.</p>"},{"location":"lessons/ai-for-beginners/5-NLP/18-Transformers/READMEtransformers/#bert","title":"BERT","text":"<p>BERT (Bidirectional Encoder Representations from Transformers) is a very large multi layer transformer network with 12 layers for BERT-base, and 24 for BERT-large. The model is first pre-trained on a large corpus of text data (WikiPedia + books) using unsupervised training (predicting masked words in a sentence). During pre-training the model absorbs significant levels of language understanding which can then be leveraged with other datasets using fine tuning. This process is called transfer learning.</p> <p></p> <p>Image source</p>"},{"location":"lessons/ai-for-beginners/5-NLP/18-Transformers/READMEtransformers/#exercises-transformers","title":"\u270d\ufe0f Exercises: Transformers","text":"<p>Continue your learning in the following notebooks:</p> <ul> <li>Transformers in PyTorch</li> <li>Transformers in TensorFlow</li> </ul>"},{"location":"lessons/ai-for-beginners/5-NLP/18-Transformers/READMEtransformers/#conclusion","title":"Conclusion","text":"<p>In this lesson you learned about Transformers and Attention Mechanisms, all essential tools in the NLP toolbox. There are many variations of Transformer architectures including BERT, DistilBERT. BigBird, OpenGPT3 and more that can be fine tuned. The HuggingFace package provides repository for training many of these architectures with both PyTorch and TensorFlow.</p>"},{"location":"lessons/ai-for-beginners/5-NLP/18-Transformers/READMEtransformers/#challenge","title":"\ud83d\ude80 Challenge","text":""},{"location":"lessons/ai-for-beginners/5-NLP/18-Transformers/READMEtransformers/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"lessons/ai-for-beginners/5-NLP/18-Transformers/READMEtransformers/#review-self-study","title":"Review &amp; Self Study","text":"<ul> <li>Blog post, explaining the classical Attention is all you need paper on transformers.</li> <li>A series of blog posts on transformers, explaining the architecture in detail.</li> </ul>"},{"location":"lessons/ai-for-beginners/5-NLP/18-Transformers/READMEtransformers/#assignment","title":"Assignment","text":""},{"location":"lessons/ai-for-beginners/5-NLP/19-NER/","title":"Index","text":""},{"location":"lessons/ai-for-beginners/5-NLP/19-NER/#named-entity-recognition","title":"Named Entity Recognition","text":"<p>Up to now, we have mostly been concentrating on one NLP task - classification. However, there are also other NLP tasks that can be accomplished with neural networks. One of those tasks is Named Entity Recognition (NER), which deals with recognizing specific entities within text, such as places, person names, date-time intervals, chemical formulae and so on.</p>"},{"location":"lessons/ai-for-beginners/5-NLP/19-NER/#pre-lecture-quiz","title":"Pre-lecture quiz","text":""},{"location":"lessons/ai-for-beginners/5-NLP/19-NER/#example-of-using-ner","title":"Example of Using NER","text":"<p>Suppose you want to develop a natural language chat bot, similar to Amazon Alexa or Google Assistant. The way intelligent chat bots work is to understand what the user wants by doing text classification on the input sentence. The result of this classification is so-called intent, which determines what a chat bot should do.</p> <p></p> <p>Image by the author</p> <p>However, a user may provide some parameters as part of the phrase. For example, when asking for the weather, she may specify a location or date. A bot should be able to understand those entities, and fill in the parameter slots accordingly before performing the action. This is exactly where NER comes in.</p> <p>\u2705 Another example would be analyzing scientific medical papers. One of the main things we need to look for are specific medical terms, such as diseases and medical substances. While a small number of diseases can probably be extracted using substring search, more complex entities, such as chemical compounds and medication names, need a more complex approach.</p>"},{"location":"lessons/ai-for-beginners/5-NLP/19-NER/#ner-as-token-classification","title":"NER as Token Classification","text":"<p>NER models are essentially token classification models, because for each of the input tokens we need to decide whether it belongs to an entity or not, and if it does - to which entity class.</p> <p>Consider the following paper title:</p> <p>Tricuspid valve regurgitation and lithium carbonate toxicity in a newborn infant.</p> <p>Entities here are:</p> <ul> <li>Tricuspid valve regurgitation is a disease (<code>DIS</code>)</li> <li>Lithium carbonate is a chemical substance (<code>CHEM</code>)</li> <li>Toxicity is also a disease (<code>DIS</code>)</li> </ul> <p>Notice that one entity can span several tokens. And, as in this case, we need to distinguish between two consecutive entities. Thus, it is common to use two classes for each entity - one specifying the first token of the entity (often the <code>B-</code> prefix is used, for beginning), and another - the continuation of an entity (<code>I-</code>, for inner token). We also use <code>O</code> as a class to represent all other tokens. Such token tagging is called BIO tagging (or IOB). When tagged, our title will look like this:</p> Token Tag Tricuspid B-DIS valve I-DIS regurgitation I-DIS and O lithium B-CHEM carbonate I-CHEM toxicity B-DIS in O a O newborn O infant O . O <p>Since we need to build a one-to-one correspondence between tokens and classes, we can train a rightmost many-to-many neural network model from this picture:</p> <p></p> <p>Image from this blog post by Andrej Karpathy. NER token classification models correspond to the right-most network architecture on this picture.</p>"},{"location":"lessons/ai-for-beginners/5-NLP/19-NER/#training-ner-models","title":"Training NER models","text":"<p>Since a NER model is essentially a token classification model, we can use RNNs that we are already familiar with for this task. In this case, each block of recurrent network will return the token ID. The following example notebook shows how to train LSTM for token classification.</p>"},{"location":"lessons/ai-for-beginners/5-NLP/19-NER/#example-notebooks-ner","title":"\u270d\ufe0f Example Notebooks: NER","text":"<p>Continue your learning in the following notebook:</p> <ul> <li>NER with TensorFlow</li> </ul>"},{"location":"lessons/ai-for-beginners/5-NLP/19-NER/#conclusion","title":"Conclusion","text":"<p>A NER model is a token classification model, which means that it can be used to perform token classification. This is a very common task in NLP, helping to recognize specific entities within text including places, names, dates, and more.</p>"},{"location":"lessons/ai-for-beginners/5-NLP/19-NER/#challenge","title":"\ud83d\ude80 Challenge","text":"<p>Complete the assignment linked below to train a named entity recognition model for medical terms, then try it on a different dataset.</p>"},{"location":"lessons/ai-for-beginners/5-NLP/19-NER/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"lessons/ai-for-beginners/5-NLP/19-NER/#review-self-study","title":"Review &amp; Self Study","text":"<p>Read through the blog The Unreasonable Effectiveness of Recurrent Neural Networks and follow along with the Further Reading section in that article to deepen your knowledge.</p>"},{"location":"lessons/ai-for-beginners/5-NLP/19-NER/#assignment","title":"Assignment","text":"<p>In the assignment for this lesson, you will have to train a medical entity recognition model. You can start with training an LSTM model as described in this lesson, and proceed with using the BERT transformer model. Read the instructions to get all the details.</p>"},{"location":"lessons/ai-for-beginners/5-NLP/20-LangModels/READMELargeLang/","title":"READMELargeLang","text":""},{"location":"lessons/ai-for-beginners/5-NLP/20-LangModels/READMELargeLang/#pre-trained-large-language-models","title":"Pre-Trained Large Language Models","text":"<p>In all of our previous tasks, we were training a neural network to perform a certain task using labeled dataset. With large transformer models, such as BERT, we use language modelling in self-supervised fashion to build a language model, which is then specialized for specific downstream task with further domain-specific training. However, it has been demonstrated that large language models can also solve many tasks without ANY domain-specific training. A family of models capable of doing that is called GPT: Generative Pre-Trained Transformer.</p>"},{"location":"lessons/ai-for-beginners/5-NLP/20-LangModels/READMELargeLang/#pre-lecture-quiz","title":"Pre-lecture quiz","text":""},{"location":"lessons/ai-for-beginners/5-NLP/20-LangModels/READMELargeLang/#text-generation-and-perplexity","title":"Text Generation and Perplexity","text":"<p>The idea of a neural network being able to do general tasks without downstream training is presented in Language Models are Unsupervised Multitask Learners paper. The main idea is the many other tasks can be modeled using text generation, because understanding text essentially means being able to produce it. Because the model is trained on a huge amount of text that encompasses human knowledge, it also becomes knowledgeable about wide variety of subjects.</p> <p>Understanding and being able to produce text also entails knowing something about the world around us. People  also learn by reading to the large extent, and GPT network is similar in this respect.</p> <p>Text generation networks work by predicting probability of the next word $$P(w_N)$$ However, unconditional probability of the next word equals to the frequency of the this word in the text corpus. GPT is able to give us conditional probability of the next word, given the previous ones: $$P(w_N | w_{n-1}, ..., w_0)$$</p> <p>You can read more about probabilities in our Data Science for Beginers Curriculum</p> <p>Quality of language generating model can be defined using perplexity. It is intrinsic metric that allows us to measure the model quality without any task-specific dataset. It is based on the notion of probability of a sentence - the model assigns high probability to a sentence that is likely to be real (i.e. the model is not perplexed by it), and low probability to sentences that make less sense (eg. Can it does what?). When we give our model sentences from real text corpus, we would expect them to have high probability, and low perplexity. Mathematically, it is defined as normalized inverse probability of the test set: $$ \\mathrm{Perplexity}(W) = \\sqrt[N]{1\\over P(W_1,...,W_N)} $$ </p> <p>You can experiment with text generation using GPT-powered text editor from Hugging Face. In this editor, you start writing your text, and pressing [TAB] will offer you several completion options. If they are too short, or you are not satisfied with them - press [TAB] again, and you will have more options, including longer pieces of text.</p>"},{"location":"lessons/ai-for-beginners/5-NLP/20-LangModels/READMELargeLang/#gpt-is-a-family","title":"GPT is a Family","text":"<p>GPT is not a single model, but rather a collection of models developed and trained by OpenAI. </p> <p>Under the GPT models, we have:</p> GPT-2 GPT 3 GPT-4 Language model with upto 1.5 billion parameters. Language model with up to 175 billion parameters 100T parameters and accepts both image and text inputs and outputs text. <p>The GPT-3 and GPT-4 models are available as a cognitive service from Microsoft Azure, and as OpenAI API.</p>"},{"location":"lessons/ai-for-beginners/5-NLP/20-LangModels/READMELargeLang/#prompt-engineering","title":"Prompt Engineering","text":"<p>Because GPT has been trained on a vast volumes of data to understand language and code, they provide outputs in response to inputs (prompts). Prompts are GPT inputs or queries whereby one provides instructions to models on tasks they next completed. To elicit a desired outcome, you need the most effective prompt which involves selecting the right words, formats, phrases or even symbols. This approach is Prompt Engineering</p> <p>This documentation provides you with more information on prompt engineering.</p>"},{"location":"lessons/ai-for-beginners/5-NLP/20-LangModels/READMELargeLang/#example-notebook-playing-with-openai-gpt","title":"\u270d\ufe0f Example Notebook: Playing with OpenAI-GPT","text":"<p>Continue your learning in the following notebooks:</p> <ul> <li>Generating text with OpenAI-GPT and Hugging Face Transformers</li> </ul>"},{"location":"lessons/ai-for-beginners/5-NLP/20-LangModels/READMELargeLang/#conclusion","title":"Conclusion","text":"<p>New general pre-trained language models do not only model language structure, but also contain vast amount of natural language. Thus, they can be effectively used to solve some NLP tasks in zero-shop or few-shot settings.</p>"},{"location":"lessons/ai-for-beginners/5-NLP/20-LangModels/READMELargeLang/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"lessons/ai-for-beginners/6-Other/21-GeneticAlgorithms/","title":"Genetic Algorithms","text":""},{"location":"lessons/ai-for-beginners/6-Other/21-GeneticAlgorithms/#genetic-algorithms","title":"Genetic Algorithms","text":""},{"location":"lessons/ai-for-beginners/6-Other/21-GeneticAlgorithms/#pre-lecture-quiz","title":"Pre-lecture quiz","text":"<p>Genetic Algorithms (GA) are based on an evolutionary approach to AI, in which methods of the evolution of a population is used to obtain an optimal solution for a given problem. They were proposed in 1975 by John Henry Holland.</p> <p>Genetic Algorithms are based on the following ideas:</p> <ul> <li>Valid solutions to the problem can be represented as genes</li> <li>Crossover allows us to combine two solutions together to obtain a new valid solution</li> <li>Selection is used to select more optimal solutions using some fitness function</li> <li>Mutations are introduced to destabilize optimization and get us out of the local minimum</li> </ul> <p>If you want to implement a Genetic Algorithm, you need the following:</p> <ul> <li>To find a method of coding our problem solutions using genes g&amp;in;\u0393</li> <li>On the set of genes \u0393 we need to define fitness function fit: \u0393&amp;rightarrow;R. Smaller function values correspond to better solutions.</li> <li>To define crossover mechanism to combine two genes together to get a new valid solution crossover: \u0393<sup>2&amp;rightarrow;\u0393.</sup></li> <li>To define mutation mechanism mutate: \u0393&amp;rightarrow;\u0393.</li> </ul> <p>In many cases, crossover and mutation are quite simple algorithms to manipulate genes as numeric sequences or bit vectors.</p> <p>The specific implementation of a genetic algorithm can vary from case to case, but the overall structure is the following:</p> <ol> <li>Select an initial population G&amp;subset;\u0393</li> <li>Randomly select one of the operations that will be performed at this step: crossover or mutation</li> <li>Crossover:</li> <li>Randomly select two genes g<sub>1</sub>, g<sub>2</sub> &amp;in; G</li> <li>Compute crossover g=crossover(g<sub>1</sub>,g<sub>2</sub>)</li> <li>If fit(g)&lt;fit(g<sub>1</sub>) or fit(g)&lt;fit(g<sub>2</sub>) - replace corresponding gene in the population by g.</li> <li>Mutation - select random gene g&amp;in;G and replace it by mutate(g)</li> <li>Repeat from step 2, until we get a sufficiently small value of fit, or until the limit on the number of steps is reached.</li> </ol>"},{"location":"lessons/ai-for-beginners/6-Other/21-GeneticAlgorithms/#typical-tasks","title":"Typical Tasks","text":"<p>Tasks typically solved by Genetic Algorithms include:</p> <ol> <li>Schedule optimization</li> <li>Optimal packing</li> <li>Optimal cutting</li> <li>Speeding up exhaustive search</li> </ol>"},{"location":"lessons/ai-for-beginners/6-Other/21-GeneticAlgorithms/#exercises-genetic-algorithms","title":"\u270d\ufe0f Exercises: Genetic Algorithms","text":"<p>Continue your learning in the following notebooks:</p> <p>Go to this notebook to see two examples of using Genetic Algorithms:</p> <ol> <li>Fair division of treasure</li> <li>8 Queens Problem</li> </ol>"},{"location":"lessons/ai-for-beginners/6-Other/21-GeneticAlgorithms/#conclusion","title":"Conclusion","text":"<p>Genetic Algorithms are used to solve many problems, including logistics and search problems. The field is Inspired by research that merged topics in Psychology and Computer Science. </p>"},{"location":"lessons/ai-for-beginners/6-Other/21-GeneticAlgorithms/#challenge","title":"\ud83d\ude80 Challenge","text":"<p>\"Genetic algorithms are simple to implement, but their behavior is difficult to understand.\" source Do some research to find an implementation of a genetic algorithm such as solving a Sudoku puzzle, and explain how it works as a sketch or flowchart.</p>"},{"location":"lessons/ai-for-beginners/6-Other/21-GeneticAlgorithms/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"lessons/ai-for-beginners/6-Other/21-GeneticAlgorithms/#review-self-study","title":"Review &amp; Self Study","text":"<p>Watch this great video talking about how computer can learn to play Super Mario using neural networks trained by genetic algorithms. We will learn more about computer learning to play games like that in the next section.</p>"},{"location":"lessons/ai-for-beginners/6-Other/21-GeneticAlgorithms/#assignment-diophantine-equation","title":"Assignment: Diophantine Equation","text":"<p>Your goal is to solve so-called Diophantine equation - an equation with integer roots. For example, consider the equation a+2b+3c+4d=30. You need to find the integer roots that satisfy this equation.</p> <p>This assignment is inspired by this post.</p> <p>Hints:</p> <ol> <li>You can consider roots to be in the interval [0;30]</li> <li>As a gene, consider using the list of root values</li> </ol> <p>Use Diophantine.ipynb as a starting point.</p>"},{"location":"lessons/ai-for-beginners/6-Other/22-DeepRL/","title":"Deep RL","text":""},{"location":"lessons/ai-for-beginners/6-Other/22-DeepRL/#deep-reinforcement-learning","title":"Deep Reinforcement Learning","text":"<p>Reinforcement learning (RL) is seen as one of the basic machine learning paradigms, next to supervised learning and unsupervised learning. While in supervised learning we rely on the dataset with known outcomes, RL is based on learning by doing. For example, when we first see a computer game, we start playing, even without knowing the rules, and soon we are able to improve our skills just by the process of playing and adjusting our behavior.</p>"},{"location":"lessons/ai-for-beginners/6-Other/22-DeepRL/#pre-lecture-quiz","title":"Pre-lecture quiz","text":"<p>To perform RL, we need:</p> <ul> <li>An environment or simulator that sets the rules of the game. We should be able to run the experiments in the simulator and observe the results.</li> <li>Some Reward function, which indicate how successful our experiment was. In case of learning to play a computer game, the reward would be our final score.</li> </ul> <p>Based on the reward function, we should be able to adjust our behavior and improve our skills, so that the next time we play better. The main difference between other types of machine learning and RL is that in RL we typically do not know whether we win or lose until we finish the game. Thus, we cannot say whether a certain move alone is good or not - we only receive a reward at the end of the game.</p> <p>During RL, we typically perform many experiments. During each experiment, we need to balance between following the optimal strategy that we have learned so far (exploitation) and exploring new possible states (exploration).</p>"},{"location":"lessons/ai-for-beginners/6-Other/22-DeepRL/#openai-gym","title":"OpenAI Gym","text":"<p>A great tool for RL is the OpenAI Gym - a simulation environment, which can simulate many different environments starting from Atari games, to the physics behind pole balancing. It is one of the most popular simulation environments for training reinforcement learning algorithms, and is maintained by OpenAI.</p> <p>Note: You can see all the environments available from OpenAI Gym here.</p>"},{"location":"lessons/ai-for-beginners/6-Other/22-DeepRL/#cartpole-balancing","title":"CartPole Balancing","text":"<p>You have probably all seen modern balancing devices such as the Segway or Gyroscooters. They are able to automatically balance by adjusting their wheels in response to a signal from an accelerometer or gyroscope. In this section, we will learn how to solve a similar problem - balancing a pole. It is similar to a situation when a circus performer needs to balance a pole on his hand - but this pole balancing only occurs in 1D.</p> <p>A simplified version of balancing is known as a CartPole problem. In the cartpole world, we have a horizontal slider that can move left or right, and the goal is to balance a vertical pole on top of the slider as it moves.</p> <p></p> <p>To create and use this environment, we need a couple of lines of Python code:</p> <pre><code>import gym\nenv = gym.make(\"CartPole-v1\")\n\nenv.reset()\ndone = False\ntotal_reward = 0\nwhile not done:\n   env.render()\n   action = env.action_space.sample()\n   observaton, reward, done, info = env.step(action)\n   total_reward += reward\n\nprint(f\"Total reward: {total_reward}\")\n</code></pre> <p>Each environment can be accessed exactly in the same way: * <code>env.reset</code> starts a new experiment * <code>env.step</code> performs a simulation step. It receives an action from the action space, and returns an observation (from the observation space), as well as a reward and a termination flag.</p> <p>In the example above we perform a random action at each step, which is why the experiment life is very short:</p> <p></p> <p>The goal of a RL algorithm is to train a model - the so called policy \u03c0 - which will return the action in response to a given state. We can also consider policy to be probabilistic, eg. for any state s and action a it will return the probability \u03c0(a|s) that we should take a in state s.</p>"},{"location":"lessons/ai-for-beginners/6-Other/22-DeepRL/#policy-gradients-algorithm","title":"Policy Gradients Algorithm","text":"<p>The most obvious way to model a policy is by creating a neural network that will take states as input, and return corresponding actions (or rather the probabilities of all actions). In a sense, it would be similar to a normal classification task, with a major difference - we do not know in advance which actions should we take at each of the steps.</p> <p>The idea here is to estimate those probabilities. We build a vector of cumulative rewards which shows our total reward at each step of the experiment. We also apply reward discounting by multiplying earlier rewards by some coefficient \u03b3=0.99, in order to diminish the role of earlier rewards. Then, we reinforce those steps along the experiment path that yield larger rewards.</p> <p>Learn more about the Policy Gradient algorithm and see it in action in the example notebook.</p>"},{"location":"lessons/ai-for-beginners/6-Other/22-DeepRL/#actor-critic-algorithm","title":"Actor-Critic Algorithm","text":"<p>An improved version of the Policy Gradients approach is called Actor-Critic. The main idea behind it is that the neural network would be trained to return two things:</p> <ul> <li>The policy, which determines which action to take. This part is called actor</li> <li>The estimation of the total reward we can expect to get at this state - this part is called critic.</li> </ul> <p>In a sense, this architecture resembles a GAN, where we have two networks that are trained against each other. In the actor-critic model, the actor proposes the action we need to take, and the critic tries to be critical and estimate the result. However, our goal is to train those networks in unison.</p> <p>Because we know both the real cumulative rewards and the results returned by the critic during the experiment, it is relatively easy to build a loss function that will minimize the difference between them. That would give us critic loss. We can compute actor loss by using the same approach as in the policy gradient algorithm.</p> <p>After running one of those algorithms, we can expect our CartPole to behave like this:</p> <p></p>"},{"location":"lessons/ai-for-beginners/6-Other/22-DeepRL/#exercises-policy-gradients-and-actor-critic-rl","title":"\u270d\ufe0f Exercises: Policy Gradients and Actor-Critic RL","text":"<p>Continue your learning in the following notebooks:</p> <ul> <li>RL in TensorFlow</li> <li>RL in PyTorch</li> </ul>"},{"location":"lessons/ai-for-beginners/6-Other/22-DeepRL/#other-rl-tasks","title":"Other RL Tasks","text":"<p>Reinforcement Learning nowadays is a fast growing field of research. Some of the interesting examples of reinforcement learning are:</p> <ul> <li>Teaching a computer to play Atari Games. The challenging part in this problem is that we do not have simple state represented as a vector, but rather a screenshot - and we need to use the CNN to convert this screen image to a feature vector, or to extract reward information. Atari games are available in the Gym.</li> <li>Teaching a computer to play board games, such as Chess and Go. Recently state-of-the-art programs like Alpha Zero were trained from scratch by two agents playing against each other, and improving at each step.</li> <li>In industry, RL is used to create control systems from simulation. A service called Bonsai is specifically designed for that.</li> </ul>"},{"location":"lessons/ai-for-beginners/6-Other/22-DeepRL/#conclusion","title":"Conclusion","text":"<p>We have now learned how to train agents to achieve good results just by providing them a reward function that defines the desired state of the game, and by giving them an opportunity to intelligently explore the search space. We have successfully tried two algorithms, and achieved a good result in a relatively short period of time. However, this is just the beginning of your journey into RL, and you should definitely consider taking a separate course is you want to dig deeper.</p>"},{"location":"lessons/ai-for-beginners/6-Other/22-DeepRL/#challenge","title":"\ud83d\ude80 Challenge","text":"<p>Explore the applications listed in the 'Other RL Tasks' section and try to implement one!</p>"},{"location":"lessons/ai-for-beginners/6-Other/22-DeepRL/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"lessons/ai-for-beginners/6-Other/22-DeepRL/#review-self-study","title":"Review &amp; Self Study","text":"<p>Learn more about classical reinforcement learning in our Machine Learning for Beginners Curriculum.</p> <p>Watch this great video talking about how a computer can learn to play Super Mario.</p>"},{"location":"lessons/ai-for-beginners/6-Other/22-DeepRL/#assignment-train-a-mountain-car","title":"Assignment: Train a Mountain Car","text":"<p>Your goal during this assignment would be to train a different Gym environment - Mountain Car.</p>"},{"location":"lessons/ai-for-beginners/6-Other/23-MultiagentSystems/","title":"Multi-Agent Systems","text":""},{"location":"lessons/ai-for-beginners/6-Other/23-MultiagentSystems/#multi-agent-systems","title":"Multi-Agent Systems","text":"<p>One of the possible ways of achieving intelligence is so-called emergent (or synergetic) approach, which is based on the fact that the combined behavior of many relatively simple agents can result in the overall more complex (or intelligent) behavior of the system as a whole. Theoretically, this is based on the principles of Collective Intelligence, Emergentism and Evolutionary Cybernetics, which state that higher-level systems gain some sort of added value when being properly combined from lower-level systems (so-called principle of metasystem transition).</p>"},{"location":"lessons/ai-for-beginners/6-Other/23-MultiagentSystems/#pre-lecture-quiz","title":"Pre-lecture quiz","text":"<p>The direction of Multi-Agent Systems has emerged in AI in 1990s as a response to growth of the Internet and distributed systems. On of the classical AI textbooks, Artificial Intelligence: A Modern Approach, focuses on the view of classical AI from the point of view of Multi-agent systems.</p> <p>Central to Multi-agent approach is the notion of Agent - an entity that lives in some environment, which it can perceive, and act upon. This is a very broad definition, and there could be many different types and classifications of agents:</p> <ul> <li>By their ability to reason:</li> <li>Reactive agents usually have simple request-response type of behavior</li> <li>Deliberative agents employ some sort of logical reasoning and/or planning capabilities</li> <li>By the place where agent execute its code:</li> <li>Static agents work on a dedicated network node</li> <li>Mobile agents can move their code between network nodes</li> <li>By their behavior:</li> <li>Passive agents do not have specific goals. Such agents can react to external stimuli, but will not initiate any actions themselves. </li> <li>Active agents have some goals which they pursue</li> <li>Cognitive agents involve complex planning and reasoning</li> </ul> <p>Multi-agent systems are nowadays used in a number of applications:</p> <ul> <li>In games, many non-player characters employ some sort of AI, and can be considered to be intelligent agents</li> <li>In video production, rendering complex 3D scenes that involve crowds is typically done using multi-agent simulation</li> <li>In systems modeling, multi-agent approach is used to simulate the behavior of a complex model. For example, multi-agent approach has been successfully used to predict the spread of COVID-19 disease worldwide. Similar approach can be used to model traffic in the city, and see how it reacts to changes in traffic rules.</li> <li>In complex automation systems, each device can act as an independent agent, which makes the whole system less monolith and more robust.</li> </ul> <p>We will not spend a lot of time going deep into multi-agent systems, but consider one example of Multi-Agent Modeling.</p>"},{"location":"lessons/ai-for-beginners/6-Other/23-MultiagentSystems/#netlogo","title":"NetLogo","text":"<p>NetLogo is a multi-agent modeling environment based on a modified version of the Logo programming language. This language was developed for teaching programming concepts to kids, and it allows you to control an agent called turtle, which can move, leaving a trace behind. This allows creating complex geometric figures, which is a very visual way to understand the behavior of an agent.</p> <p>In NetLogo, we can create many turtles by using the <code>create-turtles</code> command. We can then command all turtles to do some actions (in the example below - more 10 point forward):</p> <pre><code>create-turtles 10\nask turtles [\n  forward 10\n]\n</code></pre> <p>Of course, it is not interesting when all turtles do the same thing, so we can <code>ask</code> groups of turtles, eg. those who are in the vicinity of a certain point. We can also create turtles of different breeds using <code>breed [cats cat]</code> command. Here <code>cat</code> is the name of a breed, and we need to specify both singular and plural word, because different commands use different forms for clarity.</p> <p>\u2705 We will not go into learning the NetLogo language itself - you can visit the brilliant Beginner's Interactive NetLogo Dictionary resource if you are interested in learning more.</p> <p>You can download and install NetLogo to try it.</p>"},{"location":"lessons/ai-for-beginners/6-Other/23-MultiagentSystems/#models-library","title":"Models Library","text":"<p>A great thing about NetLogo is that it contains a library of working models that you can try. Go to File &amp;rightarrow; Models Library, and you have many categories of models to choose from.</p> <p></p> <p>A screenshot of the models library by Dmitry Soshnikov</p> <p>You can open one of the models, for example Biology &amp;rightarrow; Flocking.</p>"},{"location":"lessons/ai-for-beginners/6-Other/23-MultiagentSystems/#main-principles","title":"Main Principles","text":"<p>After opening the model, you are taken to the main NetLogo screen. Here is a sample model that describes the population of wolves and sheep, given finite resources (grass).</p> <p></p> <p>Screenshot by Dmitry Soshnikov</p> <p>On this screen, you can see:</p> <ul> <li>The Interface section which contains:</li> <li>The main field, where all agents live</li> <li>Different controls: buttons, sliders, etc.</li> <li>Graphs that you can use to display parameters of the simulation</li> <li>The Code tab which contains the editor, where you can type NetLogo program</li> </ul> <p>In most cases, the interface would have a Setup button, which initializes the simulation state, and a Go button that starts the execution. Those are handled by corresponding handlers in the code that look like this:</p> <pre><code>to go [\n...\n]\n</code></pre> <p>NetLogo's world consists of the following objects:</p> <ul> <li>Agents (turtles) that can move across the field and do something. You command agents by using <code>ask turtles [...]</code> syntax, and the code in brackets is executed by all agents in turtle mode.</li> <li>Patches are square areas of the field, on which agents live. You can refer to all agents on the same patch, or you can change patch colors and some other properties. You can also <code>ask patches</code> to do something.</li> <li>Observer is a unique agent that controls the world. All button handlers are executed in observer mode.</li> </ul> <p>\u2705 The beauty of a multi-agent environment is that the code that runs in turtle mode or in patch mode is executed at the same time by all agents in parallel. Thus, by writing a little code and programming the behavior of individual agent, you can create complex behavior of the simulation system as a whole.</p>"},{"location":"lessons/ai-for-beginners/6-Other/23-MultiagentSystems/#flocking","title":"Flocking","text":"<p>As an example of multi-agent behavior, let's consider Flocking. Flocking is a complex pattern that is very similar to how flocks of birds fly. Watching them fly you can think that they follow some kind of collective algorithm, or that they possess some form of collective intelligence. However, this complex behavior arises when each individual agent (in this case, a bird) only observes some other agents in a short distance from it, and follows three simple rules:</p> <ul> <li>Alignment - it steers towards the average heading of neighboring agents</li> <li>Cohesion - it tries to steer towards the average position of neighbors (long range attraction)</li> <li>Separation - when getting too close to other birds, it tries to move away (short range repulsion)</li> </ul> <p>You can run the flocking example and observe the behavior. You can also adjust parameters, such as degree of separation, or the viewing range, which defines how far each bird can see. Note that if you decrease the viewing range to 0, all birds become blind, and flocking stops. If you decrease separation to 0, all birds gather into a straight line.</p> <p>\u2705 Switch to the Code tab and see where three rules of flocking (alignment, cohesion and separation) are implemented in code. Note how we refer only to those agents that are in sight.</p>"},{"location":"lessons/ai-for-beginners/6-Other/23-MultiagentSystems/#other-models-to-see","title":"Other Models to see","text":"<p>There are a few more interesting models that you can experiment with:</p> <ul> <li>Art &amp;rightarrow; Fireworks shows how a firework can be considered a collective behavior of individual fire streams</li> <li>Social Science &amp;rightarrow; Traffic Basic and Social Science &amp;rightarrow; Traffic Grid show the model of city traffic in 1D and 2D Grid with or without traffic lights. Each car in the simulation follows the following rules:</li> <li>If the space in front of it is empty - accelerate (up to a certain max speed)</li> <li>If it sees the obstacle in front - brake (and you can adjust how far a driver can see)</li> <li>Social Science &amp;rightarrow; Party shows how people group together during a cocktail party. You can find the combination of parameters that lead to the fastest increase of happiness of the group.</li> </ul> <p>As you can see from these examples, multi-agent simulations can be quite a useful way to understand the behavior of a complex system consisting of individuals that follow the same or similar logic. It can also be used to control virtual agents, such as NPCs in computer games, or agents in 3D animated worlds.</p>"},{"location":"lessons/ai-for-beginners/6-Other/23-MultiagentSystems/#deliberative-agents","title":"Deliberative Agents","text":"<p>The agents described above are very simple, reacting to changes in environment using some kind of algorithm. As such they are reactive agents. However, sometimes agents can reason and plan their action, in which case they are called deliberative.</p> <p>A typical example would be a personal agent that receives an instruction from a human to book a vacation tour. Suppose that there are many agents that live on the internet, who can help it. It should then contact other agents to see which flights are available, what are the hotel prices for different dates, and try to negotiate the best price. When the vacation plan is complete and confirmed by the owner, it can proceed with booking.</p> <p>In order to do that, agents need to communicate. For successful communication they need:</p> <ul> <li>Some standard languages to exchange knowledge, such as Knowledge Interchange Format (KIF) and Knowledge Query and Manipulation Language (KQML). Those languages are designed based on Speech Act theory.</li> <li>Those languages should also include some protocols for negotiations, based on different auction types.</li> <li>A common ontology to use, so that they refer to the same concepts knowing their semantics</li> <li>A way to discover what different agents can do, also based on some sort of ontology</li> </ul> <p>Deliberative agents are much more complex than reactive, because they do not only react to changes in environment, they should also be able to intiate actions. One of the proposed architectures for deliberative agents is the so-called Belief-Desire-Intention (BDI) agent:</p> <ul> <li>Beliefs form a set of knowledge about an agent's environment. It can be structured as a knowledge base or set of rules that an agent can apply to a specific situation in the environment.</li> <li>Desires define what an agent wants to do, i.e. its goals. For example, the goal of the personal assistant agent above is to book a tour, and the goal of a hotel agent is to maximize profit.</li> <li>Intentions are specific actions that an agent plans to achieve its goals. Actions typically change the environment and cause communication with other agents.</li> </ul> <p>There are some platforms available for building multi-agent systems, such as JADE. This paper contains a review of multi-agent platforms, together with a brief history of multi-agent systems and their different usage scenarios.</p>"},{"location":"lessons/ai-for-beginners/6-Other/23-MultiagentSystems/#conclusion","title":"Conclusion","text":"<p>Multi-Agent systems can take very different forms and be used in many different applications.  They all tend to focus on the simpler behavior of an individual agent, and achieve more complex behavior of the overall system due to synergetic effect.</p>"},{"location":"lessons/ai-for-beginners/6-Other/23-MultiagentSystems/#challenge","title":"\ud83d\ude80 Challenge","text":"<p>Take this lesson to the real world and try to conceptualize a multi-agent system that can solve a problem. What, for example, would a multi-agent system need to do to optimize a school bus route? How could it work in a bakery?</p>"},{"location":"lessons/ai-for-beginners/6-Other/23-MultiagentSystems/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"lessons/ai-for-beginners/6-Other/23-MultiagentSystems/#review-self-study","title":"Review &amp; Self Study","text":"<p>Review the use of this type of system in industry. Pick a domain such as manufacturing or the video game industry and discover how multi-agent systems can be used to solve unique problems.</p>"},{"location":"lessons/ai-for-beginners/6-Other/23-MultiagentSystems/#netlogo-assignment","title":"NetLogo Assignment","text":""},{"location":"lessons/ai-for-beginners/7-Ethics/","title":"Lesson 7","text":""},{"location":"lessons/ai-for-beginners/7-Ethics/#ethical-and-responsible-ai","title":"Ethical and Responsible AI","text":"<p>You have almost finished this course, and I hope that by now you clearly see that AI is based on a number of formal mathematical methods that allow us to find relationships in data and train models to replicate some aspects of human behavior. At this point in history, we consider AI to be a very powerful tool to extract patterns from data, and to apply those patterns to solve new problems.</p>"},{"location":"lessons/ai-for-beginners/7-Ethics/#pre-lecture-quiz","title":"Pre-lecture quiz","text":"<p>However, in science fiction we often see stories where AI presents a danger to humankind. Usually those stories are centered around some sort of AI rebellion, when AI decides to confront human beings. This implies that AI has some sort of emotion or can take decisions unforeseen by its developers.</p> <p>The kind of AI that we have learned about in this course is nothing more than large matrix arithmetic. It is a very powerful tool to help us solve our problems, and as any other powerful tool - it can be used for good and for bad purposes. Importantly, it can be misused.</p>"},{"location":"lessons/ai-for-beginners/7-Ethics/#principles-of-responsible-ai","title":"Principles of Responsible AI","text":"<p>To avoid this accidental or purposeful misuse of AI, Microsoft states the important Principles of Responsible AI. The following concepts underpin these principles:</p> <ul> <li>Fairness is related to the important problem of model biases, which can be caused by using biased data for training. For example, when we try to predict the probability of getting a software developer job for a person, the model is likely to give higher preference to males - just because the training dataset was likely biased towards a male audience. We need to carefully balance training data and investigate the model to avoid biases, and make sure that the model takes into account more relevant features.</li> <li>Reliability and Safety. By their nature, AI models can make mistakes. A neural network returns probabilities, and we need to take it into account when making decisions. Every model has some precision and recall, and we need to understand that to prevent harm that wrong advice can cause.</li> <li>Privacy and Security have some AI-specific implications. For example, when we use some data for training a model, this data becomes somehow \"integrated\" into the model. On one hand, that increases security and privacy, on the other - we need to remember which data the model was trained on.</li> <li>Inclusiveness means that we are not building AI to replace people, but rather to augment people and make our work more creative. It is also related to fairness, because when dealing with underrepresented communities, most of the datasets we collect are likely to be biased, and we need to make sure that those communities are included and correctly handled by AI.</li> <li>Transparency. This includes making sure that we are always clear about AI being used. Also, wherever possible, we want to use AI systems that are interpretable.</li> <li>Accountability. When AI models come up with some decisions, it is not always clear who is responsible for those decisions. We need to make sure that we understand where responsibility of AI decisions lies. In most cases we would want to include human beings into the loop of making important decisions, so that actual people are made accountable.</li> </ul>"},{"location":"lessons/ai-for-beginners/7-Ethics/#tools-for-responsible-ai","title":"Tools for Responsible AI","text":"<p>Microsoft has developed the Responsible AI Toolbox which contains a set of tools:</p> <ul> <li>Interpretability Dashboard (InterpretML)</li> <li>Fairness Dashboard (FairLearn)</li> <li>Error Analysis Dashboard</li> <li> <p>Responsible AI Dashboard that includes</p> </li> <li> <p>EconML - tool for Causal Analysis, which focuses on what-if questions</p> </li> <li>DiCE - tool for Counterfactual Analysis allows you to see which features need to be changed to affect the decision of the model</li> </ul> <p>For more information about AI Ethics, please visit this lesson on the Machine Learning Curriculum which includes assignments.</p>"},{"location":"lessons/ai-for-beginners/7-Ethics/#review-self-study","title":"Review &amp; Self Study","text":"<p>Take this Learn Path to learn more about responsible AI.</p>"},{"location":"lessons/ai-for-beginners/7-Ethics/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"lessons/ai-for-beginners/X-Extras/X1-MultiModal/","title":"Lesson X","text":""},{"location":"lessons/ai-for-beginners/X-Extras/X1-MultiModal/#multi-modal-networks","title":"Multi-Modal Networks","text":"<p>After the success of transformer models for solving NLP tasks, the same or similar architectures have been applied to computer vision tasks. There is a growing interest in building models that would combine vision and natural language capabilities. One of such attempts was done by OpenAI, and it is called CLIP and DALL.E.</p>"},{"location":"lessons/ai-for-beginners/X-Extras/X1-MultiModal/#contrastive-image-pre-training-clip","title":"Contrastive Image Pre-Training (CLIP)","text":"<p>The main idea of CLIP is to be able to compare text prompts with an image and determine how well the image corresponds to the prompt.</p> <p></p> <p>Picture from this blog post</p> <p>The model is trained on images obtained from the Internet and their captions. For each batch, we take N pairs of (image, text), and convert them to some vector representations I<sub>1</sub>,..., I<sub>N</sub> / T<sub>1</sub>, ..., T<sub>N</sub>. Those representations are then matched together. The loss function is defined to maximize the cosine similarity between vectors corresponding to one pair (eg. I<sub>i</sub> and T<sub>i</sub>), and minimize cosine similarity between all other pairs. That is the reason this approach is called contrastive.</p> <p>CLIP model/library is available from OpenAI GitHub. The approach is described in this blog post, and in more detail in this paper.</p> <p>Once this model is pre-trained, we can give it a batch of images and a batch of text prompts, and it will return is the tensor with probabilities. CLIP can be used for several tasks:</p> <p>Image Classification</p> <p>Suppose we need to classify images between, say, cats, dogs and humans. In this case, we can give the model an image, and a series of text prompts: \"a picture of a cat\", \"a picture of a dog\", \"a picture of a human\". In the resulting vector of 3 probabilities we just need to select the index with a highest value.</p> <p></p> <p>Picture from this blog post</p> <p>Text-Based Image Search</p> <p>We can also do the opposite. If we have a collection of images, we can pass this collection to the model, and a text prompt - this will give us the image that is most similar to a given prompt.</p>"},{"location":"lessons/ai-for-beginners/X-Extras/X1-MultiModal/#example-using-clip-for-image-classification-and-image-search","title":"\u270d\ufe0f Example: Using CLIP for Image Classification and Image Search","text":"<p>Open the Clip.ipynb notebook to see CLIP in action.</p>"},{"location":"lessons/ai-for-beginners/X-Extras/X1-MultiModal/#image-generation-with-vqgan-clip","title":"Image Generation with VQGAN+ CLIP","text":"<p>CLIP can also be used for image generation from a text prompt. In order to do this, we need a generator model that will be able to generate images based on some vector input. One of such models is called VQGAN (Vector-Quantized GAN).</p> <p>The main ideas of VQGAN that differentiate it from ordinary GAN are the following: * Using autoregressive transformer architecture to generate a sequence of context-rich visual parts that compose the image. Those visual parts are in turn learned by CNN * Use sub-image discriminator that detects whether parts of the image are \"real\" of \"fake\" (unlike the \"all-or-nothing\" approach in traditional GAN).</p> <p>Learn more about VQGAN at the Taming Transformers web site.</p> <p>One of the important differences between VQGAN and traditional GAN is that the latter can produce a decent image from any input vector, while VQGAN is likely to produce an image that would not be coherent. Thus, we need to further guide the image creation process, and that can be done using CLIP. </p> <p></p> <p>To generate an image corresponding to a text prompt, we start with some random encoding vector that is passed through VQGAN to produce an image. Then CLIP is used to produce a loss function that shows how well the image corresponds to the text prompt. The goal then is to minimize this loss, using back propagation to adjust the input vector parameters.</p> <p>A great library that implements VQGAN+CLIP is Pixray</p> Picture generated from prompt a closeup watercolor portrait of young male teacher of literature with a book Picture generated from prompt a closeup oil portrait of young female teacher of computer science with a computer Picture generated from prompt a closeup oil portrait of old male teacher of mathematics in front of blackboard <p>Pictures from Artificial Teachers collection by Dmitry Soshnikov</p>"},{"location":"lessons/ai-for-beginners/X-Extras/X1-MultiModal/#dall-e","title":"DALL-E","text":""},{"location":"lessons/ai-for-beginners/X-Extras/X1-MultiModal/#dall-e-1","title":"DALL-E 1","text":"<p>DALL-E is a version of GPT-3 trained to generate images from prompts. It has been trained with 12-billion parameters.</p> <p>Unlike CLIP, DALL-E receives both text and image as a single stream of tokens for both images and text. Therefore, from multiple prompts, you can generate images based on the text.</p>"},{"location":"lessons/ai-for-beginners/X-Extras/X1-MultiModal/#dall-e-2","title":"DALL-E 2","text":"<p>The main difference between DALL.E 1 and 2, is that it generates more realistic images and art. </p> <p>Examples of image genrations with DALL-E:</p> Picture generated from prompt a closeup watercolor portrait of young male teacher of literature with a book Picture generated from prompt a closeup oil portrait of young female teacher of computer science with a computer Picture generated from prompt a closeup oil portrait of old male teacher of mathematics in front of blackboard"},{"location":"lessons/ai-for-beginners/X-Extras/X1-MultiModal/#references","title":"References","text":"<ul> <li>VQGAN Paper: Taming Transformers for High-Resolution Image Synthesis</li> <li>CLIP Paper: Learning Transferable Visual Models From Natural Language Supervision</li> </ul>"},{"location":"lessons/ai-for-beginners/sketchnotes/","title":"Index","text":"<p>All the curriculum's sketchnotes can be downloaded here.</p> <p>\ud83c\udfa8 Created by: Tomomi Imura (Twitter: @girlie_mac, GitHub: girliemac)</p> <p></p>"},{"location":"lessons/ai-for-beginners/sketchnotes/LICENSE/","title":"LICENSE","text":"<p>Attribution-ShareAlike 4.0 International</p> <p>=======================================================================</p> <p>Creative Commons Corporation (\"Creative Commons\") is not a law firm and does not provide legal services or legal advice. Distribution of Creative Commons public licenses does not create a lawyer-client or other relationship. Creative Commons makes its licenses and related information available on an \"as-is\" basis. Creative Commons gives no warranties regarding its licenses, any material licensed under their terms and conditions, or any related information. Creative Commons disclaims all liability for damages resulting from their use to the fullest extent possible.</p> <p>Using Creative Commons Public Licenses</p> <p>Creative Commons public licenses provide a standard set of terms and conditions that creators and other rights holders may use to share original works of authorship and other material subject to copyright and certain other rights specified in the public license below. The following considerations are for informational purposes only, are not exhaustive, and do not form part of our licenses.</p> <pre><code> Considerations for licensors: Our public licenses are\n intended for use by those authorized to give the public\n permission to use material in ways otherwise restricted by\n copyright and certain other rights. Our licenses are\n irrevocable. Licensors should read and understand the terms\n and conditions of the license they choose before applying it.\n Licensors should also secure all rights necessary before\n applying our licenses so that the public can reuse the\n material as expected. Licensors should clearly mark any\n material not subject to the license. This includes other CC-\n licensed material, or material used under an exception or\n limitation to copyright. More considerations for licensors:\nwiki.creativecommons.org/Considerations_for_licensors\n\n Considerations for the public: By using one of our public\n licenses, a licensor grants the public permission to use the\n licensed material under specified terms and conditions. If\n the licensor's permission is not necessary for any reason--for\n example, because of any applicable exception or limitation to\n copyright--then that use is not regulated by the license. Our\n licenses grant only permissions under copyright and certain\n other rights that a licensor has authority to grant. Use of\n the licensed material may still be restricted for other\n reasons, including because others have copyright or other\n rights in the material. A licensor may make special requests,\n such as asking that all changes be marked or described.\n Although not required by our licenses, you are encouraged to\n respect those requests where reasonable. More_considerations\n for the public:\nwiki.creativecommons.org/Considerations_for_licensees\n</code></pre> <p>=======================================================================</p> <p>Creative Commons Attribution-ShareAlike 4.0 International Public License</p> <p>By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (\"Public License\"). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.</p> <p>Section 1 -- Definitions.</p> <p>a. Adapted Material means material subject to Copyright and Similar      Rights that is derived from or based upon the Licensed Material      and in which the Licensed Material is translated, altered,      arranged, transformed, or otherwise modified in a manner requiring      permission under the Copyright and Similar Rights held by the      Licensor. For purposes of this Public License, where the Licensed      Material is a musical work, performance, or sound recording,      Adapted Material is always produced where the Licensed Material is      synched in timed relation with a moving image.</p> <p>b. Adapter's License means the license You apply to Your Copyright      and Similar Rights in Your contributions to Adapted Material in      accordance with the terms and conditions of this Public License.</p> <p>c. BY-SA Compatible License means a license listed at      creativecommons.org/compatiblelicenses, approved by Creative      Commons as essentially the equivalent of this Public License.</p> <p>d. Copyright and Similar Rights means copyright and/or similar rights      closely related to copyright including, without limitation,      performance, broadcast, sound recording, and Sui Generis Database      Rights, without regard to how the rights are labeled or      categorized. For purposes of this Public License, the rights      specified in Section 2(b)(1)-(2) are not Copyright and Similar      Rights.</p> <p>e. Effective Technological Measures means those measures that, in the      absence of proper authority, may not be circumvented under laws      fulfilling obligations under Article 11 of the WIPO Copyright      Treaty adopted on December 20, 1996, and/or similar international      agreements.</p> <p>f. Exceptions and Limitations means fair use, fair dealing, and/or      any other exception or limitation to Copyright and Similar Rights      that applies to Your use of the Licensed Material.</p> <p>g. License Elements means the license attributes listed in the name      of a Creative Commons Public License. The License Elements of this      Public License are Attribution and ShareAlike.</p> <p>h. Licensed Material means the artistic or literary work, database,      or other material to which the Licensor applied this Public      License.</p> <p>i. Licensed Rights means the rights granted to You subject to the      terms and conditions of this Public License, which are limited to      all Copyright and Similar Rights that apply to Your use of the      Licensed Material and that the Licensor has authority to license.</p> <p>j. Licensor means the individual(s) or entity(ies) granting rights      under this Public License.</p> <p>k. Share means to provide material to the public by any means or      process that requires permission under the Licensed Rights, such      as reproduction, public display, public performance, distribution,      dissemination, communication, or importation, and to make material      available to the public including in ways that members of the      public may access the material from a place and at a time      individually chosen by them.</p> <p>l. Sui Generis Database Rights means rights other than copyright      resulting from Directive 96/9/EC of the European Parliament and of      the Council of 11 March 1996 on the legal protection of databases,      as amended and/or succeeded, as well as other essentially      equivalent rights anywhere in the world.</p> <p>m. You means the individual or entity exercising the Licensed Rights      under this Public License. Your has a corresponding meaning.</p> <p>Section 2 -- Scope.</p> <p>a. License grant.</p> <pre><code>   1. Subject to the terms and conditions of this Public License,\n      the Licensor hereby grants You a worldwide, royalty-free,\n      non-sublicensable, non-exclusive, irrevocable license to\n      exercise the Licensed Rights in the Licensed Material to:\n\n        a. reproduce and Share the Licensed Material, in whole or\n           in part; and\n\n        b. produce, reproduce, and Share Adapted Material.\n\n   2. Exceptions and Limitations. For the avoidance of doubt, where\n      Exceptions and Limitations apply to Your use, this Public\n      License does not apply, and You do not need to comply with\n      its terms and conditions.\n\n   3. Term. The term of this Public License is specified in Section\n      6(a).\n\n   4. Media and formats; technical modifications allowed. The\n      Licensor authorizes You to exercise the Licensed Rights in\n      all media and formats whether now known or hereafter created,\n      and to make technical modifications necessary to do so. The\n      Licensor waives and/or agrees not to assert any right or\n      authority to forbid You from making technical modifications\n      necessary to exercise the Licensed Rights, including\n      technical modifications necessary to circumvent Effective\n      Technological Measures. For purposes of this Public License,\n      simply making modifications authorized by this Section 2(a)\n      (4) never produces Adapted Material.\n\n   5. Downstream recipients.\n\n        a. Offer from the Licensor -- Licensed Material. Every\n           recipient of the Licensed Material automatically\n           receives an offer from the Licensor to exercise the\n           Licensed Rights under the terms and conditions of this\n           Public License.\n\n        b. Additional offer from the Licensor -- Adapted Material.\n           Every recipient of Adapted Material from You\n           automatically receives an offer from the Licensor to\n           exercise the Licensed Rights in the Adapted Material\n           under the conditions of the Adapter's License You apply.\n\n        c. No downstream restrictions. You may not offer or impose\n           any additional or different terms or conditions on, or\n           apply any Effective Technological Measures to, the\n           Licensed Material if doing so restricts exercise of the\n           Licensed Rights by any recipient of the Licensed\n           Material.\n\n   6. No endorsement. Nothing in this Public License constitutes or\n      may be construed as permission to assert or imply that You\n      are, or that Your use of the Licensed Material is, connected\n      with, or sponsored, endorsed, or granted official status by,\n      the Licensor or others designated to receive attribution as\n      provided in Section 3(a)(1)(A)(i).\n</code></pre> <p>b. Other rights.</p> <pre><code>   1. Moral rights, such as the right of integrity, are not\n      licensed under this Public License, nor are publicity,\n      privacy, and/or other similar personality rights; however, to\n      the extent possible, the Licensor waives and/or agrees not to\n      assert any such rights held by the Licensor to the limited\n      extent necessary to allow You to exercise the Licensed\n      Rights, but not otherwise.\n\n   2. Patent and trademark rights are not licensed under this\n      Public License.\n\n   3. To the extent possible, the Licensor waives any right to\n      collect royalties from You for the exercise of the Licensed\n      Rights, whether directly or through a collecting society\n      under any voluntary or waivable statutory or compulsory\n      licensing scheme. In all other cases the Licensor expressly\n      reserves any right to collect such royalties.\n</code></pre> <p>Section 3 -- License Conditions.</p> <p>Your exercise of the Licensed Rights is expressly made subject to the following conditions.</p> <p>a. Attribution.</p> <pre><code>   1. If You Share the Licensed Material (including in modified\n      form), You must:\n\n        a. retain the following if it is supplied by the Licensor\n           with the Licensed Material:\n\n             i. identification of the creator(s) of the Licensed\n                Material and any others designated to receive\n                attribution, in any reasonable manner requested by\n                the Licensor (including by pseudonym if\n                designated);\n\n            ii. a copyright notice;\n\n           iii. a notice that refers to this Public License;\n\n            iv. a notice that refers to the disclaimer of\n                warranties;\n\n             v. a URI or hyperlink to the Licensed Material to the\n                extent reasonably practicable;\n\n        b. indicate if You modified the Licensed Material and\n           retain an indication of any previous modifications; and\n\n        c. indicate the Licensed Material is licensed under this\n           Public License, and include the text of, or the URI or\n           hyperlink to, this Public License.\n\n   2. You may satisfy the conditions in Section 3(a)(1) in any\n      reasonable manner based on the medium, means, and context in\n      which You Share the Licensed Material. For example, it may be\n      reasonable to satisfy the conditions by providing a URI or\n      hyperlink to a resource that includes the required\n      information.\n\n   3. If requested by the Licensor, You must remove any of the\n      information required by Section 3(a)(1)(A) to the extent\n      reasonably practicable.\n</code></pre> <p>b. ShareAlike.</p> <pre><code> In addition to the conditions in Section 3(a), if You Share\n Adapted Material You produce, the following conditions also apply.\n\n   1. The Adapter's License You apply must be a Creative Commons\n      license with the same License Elements, this version or\n      later, or a BY-SA Compatible License.\n\n   2. You must include the text of, or the URI or hyperlink to, the\n      Adapter's License You apply. You may satisfy this condition\n      in any reasonable manner based on the medium, means, and\n      context in which You Share Adapted Material.\n\n   3. You may not offer or impose any additional or different terms\n      or conditions on, or apply any Effective Technological\n      Measures to, Adapted Material that restrict exercise of the\n      rights granted under the Adapter's License You apply.\n</code></pre> <p>Section 4 -- Sui Generis Database Rights.</p> <p>Where the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:</p> <p>a. for the avoidance of doubt, Section 2(a)(1) grants You the right      to extract, reuse, reproduce, and Share all or a substantial      portion of the contents of the database;</p> <p>b. if You include all or a substantial portion of the database      contents in a database in which You have Sui Generis Database      Rights, then the database in which You have Sui Generis Database      Rights (but not its individual contents) is Adapted Material,</p> <pre><code> including for purposes of Section 3(b); and\n</code></pre> <p>c. You must comply with the conditions in Section 3(a) if You Share      all or a substantial portion of the contents of the database.</p> <p>For the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.</p> <p>Section 5 -- Disclaimer of Warranties and Limitation of Liability.</p> <p>a. UNLESS OTHERWISE SEPARATELY UNDERTAKEN BY THE LICENSOR, TO THE      EXTENT POSSIBLE, THE LICENSOR OFFERS THE LICENSED MATERIAL AS-IS      AND AS-AVAILABLE, AND MAKES NO REPRESENTATIONS OR WARRANTIES OF      ANY KIND CONCERNING THE LICENSED MATERIAL, WHETHER EXPRESS,      IMPLIED, STATUTORY, OR OTHER. THIS INCLUDES, WITHOUT LIMITATION,      WARRANTIES OF TITLE, MERCHANTABILITY, FITNESS FOR A PARTICULAR      PURPOSE, NON-INFRINGEMENT, ABSENCE OF LATENT OR OTHER DEFECTS,      ACCURACY, OR THE PRESENCE OR ABSENCE OF ERRORS, WHETHER OR NOT      KNOWN OR DISCOVERABLE. WHERE DISCLAIMERS OF WARRANTIES ARE NOT      ALLOWED IN FULL OR IN PART, THIS DISCLAIMER MAY NOT APPLY TO YOU.</p> <p>b. TO THE EXTENT POSSIBLE, IN NO EVENT WILL THE LICENSOR BE LIABLE      TO YOU ON ANY LEGAL THEORY (INCLUDING, WITHOUT LIMITATION,      NEGLIGENCE) OR OTHERWISE FOR ANY DIRECT, SPECIAL, INDIRECT,      INCIDENTAL, CONSEQUENTIAL, PUNITIVE, EXEMPLARY, OR OTHER LOSSES,      COSTS, EXPENSES, OR DAMAGES ARISING OUT OF THIS PUBLIC LICENSE OR      USE OF THE LICENSED MATERIAL, EVEN IF THE LICENSOR HAS BEEN      ADVISED OF THE POSSIBILITY OF SUCH LOSSES, COSTS, EXPENSES, OR      DAMAGES. WHERE A LIMITATION OF LIABILITY IS NOT ALLOWED IN FULL OR      IN PART, THIS LIMITATION MAY NOT APPLY TO YOU.</p> <p>c. The disclaimer of warranties and limitation of liability provided      above shall be interpreted in a manner that, to the extent      possible, most closely approximates an absolute disclaimer and      waiver of all liability.</p> <p>Section 6 -- Term and Termination.</p> <p>a. This Public License applies for the term of the Copyright and      Similar Rights licensed here. However, if You fail to comply with      this Public License, then Your rights under this Public License      terminate automatically.</p> <p>b. Where Your right to use the Licensed Material has terminated under      Section 6(a), it reinstates:</p> <pre><code>   1. automatically as of the date the violation is cured, provided\n      it is cured within 30 days of Your discovery of the\n      violation; or\n\n   2. upon express reinstatement by the Licensor.\n\n For the avoidance of doubt, this Section 6(b) does not affect any\n right the Licensor may have to seek remedies for Your violations\n of this Public License.\n</code></pre> <p>c. For the avoidance of doubt, the Licensor may also offer the      Licensed Material under separate terms or conditions or stop      distributing the Licensed Material at any time; however, doing so      will not terminate this Public License.</p> <p>d. Sections 1, 5, 6, 7, and 8 survive termination of this Public      License.</p> <p>Section 7 -- Other Terms and Conditions.</p> <p>a. The Licensor shall not be bound by any additional or different      terms or conditions communicated by You unless expressly agreed.</p> <p>b. Any arrangements, understandings, or agreements regarding the      Licensed Material not stated herein are separate from and      independent of the terms and conditions of this Public License.</p> <p>Section 8 -- Interpretation.</p> <p>a. For the avoidance of doubt, this Public License does not, and      shall not be interpreted to, reduce, limit, restrict, or impose      conditions on any use of the Licensed Material that could lawfully      be made without permission under this Public License.</p> <p>b. To the extent possible, if any provision of this Public License is      deemed unenforceable, it shall be automatically reformed to the      minimum extent necessary to make it enforceable. If the provision      cannot be reformed, it shall be severed from this Public License      without affecting the enforceability of the remaining terms and      conditions.</p> <p>c. No term or condition of this Public License will be waived and no      failure to comply consented to unless expressly agreed to by the      Licensor.</p> <p>d. Nothing in this Public License constitutes or may be interpreted      as a limitation upon, or waiver of, any privileges and immunities      that apply to the Licensor or You, including from the legal      processes of any jurisdiction or authority.</p> <p>=======================================================================</p> <p>Creative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the \u201cLicensor.\u201d The text of the Creative Commons public licenses is dedicated to the public domain under the CC0 Public Domain Dedication. Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark \"Creative Commons\" or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.</p> <p>Creative Commons may be contacted at creativecommons.org.</p>"},{"location":"lessons/generative-ai-for-beginners/","title":"README","text":"<p>This lesson is forked from Generative AI For Beginners from Microsoft under MIT license.</p> <p></p>"},{"location":"lessons/generative-ai-for-beginners/#21-lessons-teaching-everything-you-need-to-know-to-start-building-generative-ai-applications","title":"21 Lessons teaching everything you need to know to start building Generative AI applications","text":""},{"location":"lessons/generative-ai-for-beginners/#generative-ai-for-beginners-version-3-a-course","title":"Generative AI for Beginners (Version 3) - A Course","text":"<p>Learn the fundamentals of building Generative AI applications with our 21-lesson comprehensive course by Microsoft Cloud Advocates.</p>"},{"location":"lessons/generative-ai-for-beginners/#getting-started","title":"\ud83c\udf31 Getting Started","text":"<p>This course has 21 lessons. Each lesson covers its own topic so start wherever you like!</p> <p>Lessons are labeled either \"Learn\" lessons explaining a Generative AI concept or \"Build\" lessons that explain a concept and code examples in both Python and TypeScript when possible.</p> <p>Each lesson also includes a \"Keep Learning\" section with additional learning tools.</p>"},{"location":"lessons/generative-ai-for-beginners/#what-you-need","title":"What You Need","text":""},{"location":"lessons/generative-ai-for-beginners/#to-run-this-code-of-this-course-you-can-use-either","title":"To run this code of this course, you can use either:","text":"<ul> <li>Azure OpenAI Service - Lessons: \"aoai-assignment\"</li> <li>GitHub Marketplace Model Catalog - Lessons: \"githubmodels\"</li> <li> <p>OpenAI API - Lessons: \"oai-assignment\" </p> </li> <li> <p>Basic knowledge of Python or TypeScript is helpful - *For absolute beginners check out these Python and TypeScript courses.</p> </li> <li>A GitHub account to fork this entire repo to your own GitHub account</li> </ul> <p>We have created a Course Setup lesson to help you with setting up your development environment.</p> <p>Don't forget to star (\ud83c\udf1f) this repo to find it easier later.</p>"},{"location":"lessons/generative-ai-for-beginners/#ready-to-deploy","title":"\ud83e\udde0 Ready to Deploy?","text":"<p>If you are looking for more advanced code samples, check out our collection of Generative AI Code Samples in both Python and TypeScript.</p>"},{"location":"lessons/generative-ai-for-beginners/#meet-other-learners-get-support","title":"\ud83d\udde3\ufe0f Meet Other Learners, Get Support","text":"<p>Join our official AI Discord server to meet and network with other learners taking this course and get support.</p>"},{"location":"lessons/generative-ai-for-beginners/#building-a-startup","title":"\ud83d\ude80 Building a Startup?","text":"<p>Sign up for Microsoft for Startups Founders Hub to receive free OpenAI credits and up to $150k towards Azure credits to access OpenAI models through Azure OpenAI Services.</p>"},{"location":"lessons/generative-ai-for-beginners/#want-to-help","title":"\ud83d\ude4f Want to help?","text":"<p>Do you have suggestions or found spelling or code errors? Raise an issue or Create a pull request</p>"},{"location":"lessons/generative-ai-for-beginners/#each-lesson-includes","title":"\ud83d\udcc2 Each lesson includes:","text":"<ul> <li>A short video introduction to the topic</li> <li>A written lesson located in the README</li> <li>Python and TypeScript code samples supporting Azure OpenAI and OpenAI API</li> <li>Links to extra resources to continue your learning</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/#lessons","title":"\ud83d\uddc3\ufe0f Lessons","text":"# Lesson Link Description Video Extra Learning 00 Course Setup Learn: How to Setup Your Development Environment Coming Soon Learn More 01 Introduction to Generative AI and LLMs Learn: Understanding what Generative AI is and how Large Language Models (LLMs) work. Video Learn More 02 Exploring and comparing different LLMs Learn: How to select the right model for your use case Video Learn More 03 Using Generative AI Responsibly Learn: How to build Generative AI Applications responsibly Video Learn More 04 Understanding Prompt Engineering Fundamentals Learn: Hands-on Prompt Engineering Best Practices Video Learn More 05 Creating Advanced Prompts Learn: How to apply prompt engineering techniques that improve the outcome of your prompts. Video Learn More 06 Building Text Generation Applications Build: A text generation app using Azure OpenAI / OpenAI API Video Learn More 07 Building Chat Applications Build: Techniques for efficiently building and integrating chat applications. Video Learn More 08 Building Search Apps Vector Databases Build: A search application that uses Embeddings to search for data. Video Learn More 09 Building Image Generation Applications Build: A image generation application Video Learn More 10 Building Low Code AI Applications Build: A Generative AI application using Low Code tools Video Learn More 11 Integrating External Applications with Function Calling Build: What is function calling and its use cases for applications Video Learn More 12 Designing UX for AI Applications Learn: How to apply UX design principles when developing Generative AI Applications Video Learn More 13 Securing Your Generative AI Applications Learn: The threats and risks to AI systems and methods to secure these systems. Video Learn More 14 The Generative AI Application Lifecycle Learn: The tools and metrics to manage the LLM Lifecycle and LLMOps Video Learn More 15 Retrieval Augmented Generation (RAG) and Vector Databases Build: An application using a RAG Framework to retrieve embeddings from a Vector Databases Video Learn More 16 Open Source Models and Hugging Face Build: An application using open source models available on Hugging Face Video Learn More 17 AI Agents Build: An application using an AI Agent Framework Video Learn More 18 Fine-Tuning LLMs Learn: The what, why and how of fine-tuning LLMs Video Learn More 19 Building with SLMs Learn: The benefits of building with Small Language Models Video Coming Soon Learn More 20 Building with Mistral Models Learn: The features and differences of the Mistral Family Models Video Coming Soon Learn More 21 Building with Meta Models Learn: The features and differences of the Meta Family Models Video Coming Soon Learn More"},{"location":"lessons/generative-ai-for-beginners/#special-thanks","title":"\ud83c\udf1f Special thanks","text":"<p>Special thanks to John Aziz for creating all of the GitHub Actions and workflows</p> <p>Bernhard Merkle for making key contributions to each lesson to improve the learner and code experience. </p>"},{"location":"lessons/generative-ai-for-beginners/#other-courses","title":"\ud83c\udf92 Other Courses","text":"<p>Our team produces other courses! Check out:</p> <ul> <li>ML for Beginners</li> <li>Data Science for Beginners</li> <li>AI for Beginners</li> <li>NEW Cybersecurity for Beginners</li> <li>Web Dev for Beginners</li> <li>IoT for Beginners</li> <li>XR Development for Beginners</li> <li>Mastering GitHub Copilot for AI Paired Programming</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/00-course-setup/","title":"Lesson 0","text":""},{"location":"lessons/generative-ai-for-beginners/00-course-setup/#getting-started-with-this-course","title":"Getting Started with this course","text":"<p>We are very excited for you to start this course and see what you get inspired to build with Generative AI!</p> <p>To ensure your success, this page outlines setup steps, technical requirements, and where to get help if needed.</p>"},{"location":"lessons/generative-ai-for-beginners/00-course-setup/#setup-steps","title":"Setup Steps","text":"<p>To start taking this course, you will need to complete the following steps.</p>"},{"location":"lessons/generative-ai-for-beginners/00-course-setup/#1-fork-this-repo","title":"1. Fork this Repo","text":"<p>Fork this entire repo to your own GitHub account to be able to change any code and complete the challenges. You can also star (\ud83c\udf1f) this repo to find it and related repos easier.</p>"},{"location":"lessons/generative-ai-for-beginners/00-course-setup/#2-create-a-codespace","title":"2. Create a codespace","text":"<p>To avoid any dependency issues when running the code, we recommend running this course in a GitHub Codespaces.</p> <p>This can be created by selecting the <code>Code</code> option on your forked version of this repo and selecting the Codespaces option.</p> <p></p>"},{"location":"lessons/generative-ai-for-beginners/00-course-setup/#3-storing-your-api-keys","title":"3. Storing Your API Keys","text":"<p>Keeping your API keys safe and secure is important when building any type of application. We recommend not to store any API keys directly in your code. Committing those details to a public repository could result in security issues and even unwanted costs if used by a bad actor.</p>"},{"location":"lessons/generative-ai-for-beginners/00-course-setup/#how-to-run-locally-on-your-computer","title":"How to Run locally on your computer","text":"<p>To run the code locally on your computer, you would need to have some version of Python installed.</p> <p>To then use the repository, you need to clone it:</p> <pre><code>git clone https://github.com/microsoft/generative-ai-for-beginners\ncd generative-ai-for-beginners\n</code></pre> <p>Once you have everything checked out, you can get started!</p>"},{"location":"lessons/generative-ai-for-beginners/00-course-setup/#installing-miniconda-optional-step","title":"Installing Miniconda (optional step)","text":"<p>Miniconda is a lightweight installer for installing Conda, Python, as well as a few packages. Conda itself is a package manager, that makes it easy to setup and switch between different Python virtual environments and packages. It also comes in handy for installing packages that are not available via <code>pip</code>.</p> <p>You can follow the MiniConda installation guide to set it up.</p> <p>With Miniconda installed, you need to clone the repository (if you haven't already)</p> <p>Next, you need to create a virtual environment. To do this with Conda, go ahead and create a new environment file (environment.yml). If you are following along using Codespaces, create this within the <code>.devcontainer</code> directory, thus <code>.devcontainer/environment.yml</code>.</p> <p>Go ahead and populate your environment file with the snippet below:</p> <pre><code>name: &lt;environment-name&gt;\nchannels:\n - defaults\ndependencies:\n- python=&lt;python-version&gt;\n- openai\n- python-dotenv\n- azure-ai-inference\n</code></pre> <p>The environment file specifies the dependencies we need. <code>&lt;environment-name&gt;</code> refers to the name you would like to use for your Conda environment, and <code>&lt;python-version&gt;</code> is the version of Python you would like to use, for example, <code>3</code> is the latest major version of Python.</p> <p>With that done, you can go ahead and create your Conda environment by running the commands below in your command line/terminal</p> <pre><code>conda env create --name ai4beg --file .devcontainer/environment.yml # .devcontainer sub path applies to only Codespace setups\nconda activate ai4beg\n</code></pre> <p>Refer to the Conda environments guide if you run into any issues.</p>"},{"location":"lessons/generative-ai-for-beginners/00-course-setup/#using-visual-studio-code-with-the-python-support-extension","title":"Using Visual Studio Code with the Python support extension","text":"<p>We recommend using the Visual Studio Code (VS Code) editor with the Python support extension installed for this course. This is, however, more of a recommendation and not a definite requirement</p> <p>Note: By opening the course repository in VS Code, you have the option to set the project up within a container. This is because of the special <code>.devcontainer</code> directory found within the course repository. More on this later.</p> <p>Note: Once you clone and open the directory in VS Code, it will automatically suggest you install a Python support extension.</p> <p>Note: If VS Code suggests you re-open the repository in a container, decline this request in order to use the locally installed version of Python.</p>"},{"location":"lessons/generative-ai-for-beginners/00-course-setup/#using-jupyter-in-the-browser","title":"Using Jupyter in the Browser","text":"<p>You can also work on the project using the Jupyter environment right within your browser. Both classic Jupyter and Jupyter Hub provide quite a pleasant development environment with features such as auto-completion, code highlighting, etc.</p> <p>To start Jupyter locally, head over to the terminal/command line, navigate to the course directory, and execute:</p> <pre><code>jupyter notebook\n</code></pre> <p>or</p> <pre><code>jupyterhub\n</code></pre> <p>This will start a Jupyter instance and the URL to access it will be shown within the command line window.</p> <p>Once you access the URL, you should see the course outline and be able to navigate to any <code>*.ipynb</code> file. For example, <code>08-building-search-applications/python/oai-solution.ipynb</code>.</p>"},{"location":"lessons/generative-ai-for-beginners/00-course-setup/#running-in-a-container","title":"Running in a container","text":"<p>An alternative to setting everything up on your computer or Codespace is to use a container. The special <code>.devcontainer</code> folder within the course repository makes it possible for VS Code to set up the project within a container. Outside of Codespaces, this will require the installation of Docker, and quite frankly, it involves a bit of work, so we recommend this only to those with experience working with containers.</p> <p>One of the best ways to keep your API keys secure when using GitHub Codespaces is by using Codespace Secrets. Please follow the Codespaces secrets management guide to learn more about this.</p>"},{"location":"lessons/generative-ai-for-beginners/00-course-setup/#lessons-and-technical-requirements","title":"Lessons and Technical Requirements","text":"<p>The course has 6 concept lessons and 6 coding lessons.</p> <p>For the coding lessons, we are using the Azure OpenAI Service. You will need access to the Azure OpenAI service and an API key to run this code. You can apply to get access by completing this application.</p> <p>While you wait for your application to be processed, each coding lesson also includes a <code>README.md</code> file where you can view the code and outputs.</p>"},{"location":"lessons/generative-ai-for-beginners/00-course-setup/#using-the-azure-openai-service-for-the-first-time","title":"Using the Azure OpenAI Service for the first time","text":"<p>If this is your first time working with the Azure OpenAI service, please follow this guide on how to create and deploy an Azure OpenAI Service resource.</p>"},{"location":"lessons/generative-ai-for-beginners/00-course-setup/#using-the-openai-api-for-the-first-time","title":"Using the OpenAI API for the first time","text":"<p>If this is your first time working with the OpenAI API, please follow the guide on how to create and use the Interface.</p>"},{"location":"lessons/generative-ai-for-beginners/00-course-setup/#meet-other-learners","title":"Meet Other Learners","text":"<p>We have created channels in our official AI Community Discord server for meeting other learners. This is a great way to network with other like-minded entrepreneurs, builders, students, and anyone looking to level up in Generative AI.</p> <p></p> <p>The project team will also be on this Discord server to help any learners.</p>"},{"location":"lessons/generative-ai-for-beginners/00-course-setup/#contribute","title":"Contribute","text":"<p>This course is an open-source initiative. If you see areas of improvement or issues, please create a Pull Request or log a GitHub issue.</p> <p>The project team will be tracking all contributions. Contributing to open source is an amazing way to build your career in Generative AI.</p> <p>Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to and actually do, grant us the rights to use your contribution. For details, visit CLA, Contributor License Agreement website.</p> <p>Important: when translating text in this repo, please ensure that you do not use machine translation. We will verify translations via the community, so please only volunteer for translations in languages where you are proficient.</p> <p>When you submit a pull request, a CLA-bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repositories using our CLA.</p> <p>This project has adopted the Microsoft Open Source Code of Conduct. For more information read the Code of Conduct FAQ or contact Email opencode with any additional questions or comments.</p>"},{"location":"lessons/generative-ai-for-beginners/00-course-setup/#lets-get-started","title":"Let's Get Started","text":"<p>Now that you have completed the needed steps to complete this course, let's get started by getting an introduction to Generative AI and LLMs.</p>"},{"location":"lessons/generative-ai-for-beginners/00-course-setup/SETUP/","title":"SETUP","text":""},{"location":"lessons/generative-ai-for-beginners/00-course-setup/SETUP/#setup-your-dev-environment","title":"Setup Your Dev Environment","text":"<p>We setup this repository and course with a development container that has a Universal runtime that can support Python3, .NET, Node.js and Java development. The related configuration is defined in the <code>devcontainer.json</code> file located in the <code>.devcontainer/</code> folder at the root of this repository.</p> <p>To activate the dev container, launch it in GitHub Codespaces (for a cloud-hosted runtime) or in Docker Desktop (for a local device-hosted runtime). Read this documentation for more details on how dev containers work within VS Code.  </p> <p>[!TIP] We recommend using GitHub Codespaces for a quick start with minimal effort. It provides a generous free usage quota for personal accounts. Configure timeouts to stop or delete inactive codespaces to maximize your quota usage.</p>"},{"location":"lessons/generative-ai-for-beginners/00-course-setup/SETUP/#1-executing-assignments","title":"1. Executing Assignments","text":"<p>Each lesson will have optional assignments that may be provided in one or more programming languages including: Python, .NET/C#, Java and JavaScript/TypeScript. This section provides general guidance related to executing those assignments.</p>"},{"location":"lessons/generative-ai-for-beginners/00-course-setup/SETUP/#11-python-assignments","title":"1.1 Python Assignments","text":"<p>Python assignments are provided either as applications (<code>.py</code> files) or Jupyter notebooks (<code>.ipynb</code> files).  - To run the notebook, open it in Visual Studio Code then click Select Kernel (at top right) and select the default Python 3 option shown. You can now Run All to execute the notebook. - To run Python applications from command-line, follow assignment-specific instructions to ensure you select the right files and provide required arguments</p>"},{"location":"lessons/generative-ai-for-beginners/00-course-setup/SETUP/#2-configuring-providers","title":"2. Configuring Providers","text":"<p>Assignments may also be setup to work against one or more Large Language Model (LLM) deployments through a supported service provider like OpenAI, Azure or Hugging Face. These provide a hosted endpoint (API) that we can access programmatically with the right credentials (API key or token). In this course, we discuss these providers:</p> <ul> <li>OpenAI with diverse models including the core GPT series.</li> <li>Azure OpenAI for OpenAI models with enterprise readiness in focus</li> <li>Hugging Face for open-source models and inference server</li> </ul> <p>You will need to use your own accounts for these exercises. Assignments are optional so you can choose to setup one, all - or none - of the providers based on your interests. Some guidance for signup:</p> Signup Cost API Key Playground Comments OpenAI Pricing Project-based No-Code, Web Multiple Models Available Azure Pricing SDK Quickstart Studio Quickstart Must Apply Ahead For Access Hugging Face Pricing Access Tokens Hugging Chat Hugging Chat has limited models <p>Follow the directions below to configure this repository for use with different providers. Assignments that require a specific provider will contain one of these tags in their filename:  - <code>aoai</code> - requires Azure OpenAI endpoint, key  - <code>oai</code> - requires OpenAI endpoint, key  - <code>hf</code> - requires Hugging Face token</p> <p>You can configure one, none, or all providers. Related assignments will simply error out on missing credentials.</p>"},{"location":"lessons/generative-ai-for-beginners/00-course-setup/SETUP/#21-create-env-file","title":"2.1. Create <code>.env</code> file","text":"<p>We assume that you have already read the guidance above and signed up with the relevant provider, and obtained the required authentication credentials (API_KEY or token). In the case of Azure OpenAI, we assume you also have a valid deployment of an Azure OpenAI Service (endpoint) with at least one GPT model deployed for chat completion.</p> <p>The next step is to configure your local environment variables as follows:</p> <ol> <li>Look in the root folder for a <code>.env.copy</code> file that should have contents like this:</li> </ol> <pre><code># OpenAI Provider\nOPENAI_API_KEY='&lt;add your OpenAI API key here&gt;'\n\n## Azure OpenAI\nAZURE_OPENAI_API_VERSION='2024-02-01' # Default is set!\nAZURE_OPENAI_API_KEY='&lt;add your AOAI key here&gt;'\nAZURE_OPENAI_ENDPOINT='&lt;add your AOIA service endpoint here&gt;'\nAZURE_OPENAI_DEPLOYMENT='&lt;add your chat completion model name here&gt;' \nAZURE_OPENAI_EMBEDDINGS_DEPLOYMENT='&lt;add your embeddings model name here&gt;'\n\n## Hugging Face\nHUGGING_FACE_API_KEY='&lt;add your HuggingFace API or token here&gt;'\n</code></pre> <ol> <li>Copy that file to <code>.env</code> using the command below. This file is gitignore-d, keeping secrets safe.</li> </ol> <pre><code>cp .env.copy .env\n</code></pre> <ol> <li> <p>Fill in the values (replace placeholders on right side of <code>=</code>) as described in the next section.</p> </li> <li> <p>(Option) If you use GitHub Codespaces, you have the option to save environment variables as Codespaces secrets associated with this repository. In that case, you won't need to setup a local .env file. However, note that this option works only if you use GitHub Codespaces. You will still need to setup the .env file if you use Docker Desktop instead.</p> </li> </ol>"},{"location":"lessons/generative-ai-for-beginners/00-course-setup/SETUP/#22-populate-env-file","title":"2.2. Populate <code>.env</code> file","text":"<p>Let's take a quick look at the variable names to understand what they represent:</p> Variable Description HUGGING_FACE_API_KEY This is the user access token you setup in your profile OPENAI_API_KEY This is the authorization key for using the service for non-Azure OpenAI endpoints AZURE_OPENAI_API_KEY This is the authorization key for using that service AZURE_OPENAI_ENDPOINT This is the deployed endpoint for an Azure OpenAI resource AZURE_OPENAI_DEPLOYMENT This is the text generation model deployment endpoint AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT This is the text embeddings model deployment endpoint <p>Note: The last two Azure OpenAI variables reflect a default model for chat completion (text generation) and vector search (embeddings) respectively. Instructions for setting them will be defined in relevant assignments.</p>"},{"location":"lessons/generative-ai-for-beginners/00-course-setup/SETUP/#23-configure-azure-from-portal","title":"2.3 Configure Azure: From Portal","text":"<p>The Azure OpenAI endpoint and key values will be found in the Azure Portal so let's start there.</p> <ol> <li>Go to the Azure Portal</li> <li>Click the Keys and Endpoint option in the sidebar (menu at left).</li> <li>Click Show Keys - you should see the following: KEY 1, KEY 2 and Endpoint.</li> <li>Use the KEY 1 value for AZURE_OPENAI_API_KEY</li> <li>Use the Endpoint value for AZURE_OPENAI_ENDPOINT</li> </ol> <p>Next, we need the endpoints for the specific models we've deployed.</p> <ol> <li>Click the Model deployments option in the sidebar (left menu) for Azure OpenAI resource.</li> <li>In the destination page, click Manage Deployments</li> </ol> <p>This will take you to the Azure OpenAI Studio website, where we'll find the other values as described below.</p>"},{"location":"lessons/generative-ai-for-beginners/00-course-setup/SETUP/#24-configure-azure-from-studio","title":"2.4 Configure Azure: From Studio","text":"<ol> <li>Navigate to Azure OpenAI Studio from your resource as described above.</li> <li>Click the Deployments tab (sidebar, left) to view currently deployed models.</li> <li>If your desired model is not deployed, use Create new deployment to deploy it.</li> <li>You will need a text-generation model - we recommend: gpt-35-turbo</li> <li>You will need a text-embedding model - we recommend text-embedding-ada-002</li> </ol> <p>Now update the environment variables to reflect the Deployment name used. This will typically be the same as the model name unless you changed it explcitly. So, as an example, you might have:</p> <pre><code>AZURE_OPENAI_DEPLOYMENT='gpt-35-turbo'\nAZURE_OPENAI_EMBEDDINGS_DEPLOYMENT='text-embedding-ada-002'\n</code></pre> <p>Don't forget to save the .env file when done. You can now exit the file and return to the instructions for running the notebook.</p>"},{"location":"lessons/generative-ai-for-beginners/00-course-setup/SETUP/#25-configure-openai-from-profile","title":"2.5 Configure OpenAI: From Profile","text":"<p>Your OpenAI API key can be found in your OpenAI account. If you don't have one, you can sign up for an account and create an API key. Once you have the key, you can use it to populate the <code>OPENAI_API_KEY</code> variable in the <code>.env</code> file.</p>"},{"location":"lessons/generative-ai-for-beginners/00-course-setup/SETUP/#26-configure-hugging-face-from-profile","title":"2.6 Configure Hugging Face: From Profile","text":"<p>Your Hugging Face token can be found in your profile under Access Tokens. Don't post or share these publicly. Instead, create a new token for this project usage and copy that into the <code>.env</code> file under the <code>HUGGING_FACE_API_KEY</code> variable. Note: This is technically not an API key but is used for authentication so we are keeping that naming convention for consistency.</p>"},{"location":"lessons/generative-ai-for-beginners/01-introduction-to-genai/","title":"Lesson 1","text":""},{"location":"lessons/generative-ai-for-beginners/01-introduction-to-genai/#introduction-to-generative-ai-and-large-language-models","title":"Introduction to Generative AI and Large Language Models","text":"<p>(Click the image above to view video of this lesson)</p> <p>Generative AI is artificial intelligence capable of generating text, images and other types of content. What makes it a fantastic technology is that it democratizes AI, anyone can use it with as little as a text prompt, a sentence written in a natural language. There's no need for you to learn a language like Java or SQL to accomplish something worthwhile, all you need is to use your language, state what you want and out comes a suggestion from an AI model. The applications and impact for this is huge, you write or understand reports, write applications and much more, all in seconds.</p> <p>In this curriculum, we\u2019ll explore how our startup leverages generative AI to unlock new scenarios in the education world and how we address the inevitable challenges associated with the social implications of its application and the technology limitations.</p>"},{"location":"lessons/generative-ai-for-beginners/01-introduction-to-genai/#introduction","title":"Introduction","text":"<p>This lesson will cover:</p> <ul> <li>Introduction to the business scenario: our startup idea and mission.</li> <li>Generative AI and how we landed on the current technology landscape.</li> <li>Inner working of a large language model.</li> <li>Main capabilities and practical use cases of Large Language Models.</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/01-introduction-to-genai/#learning-goals","title":"Learning Goals","text":"<p>After completing this lesson, you will understand:</p> <ul> <li>What generative AI is and how Large Language Models work.</li> <li>How you can leverage large language models for different use cases, with a focus on education scenarios.</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/01-introduction-to-genai/#scenario-our-educational-startup","title":"Scenario: our educational startup","text":"<p>Generative Artificial Intelligence (AI) represents the pinnacle of AI technology, pushing the boundaries of what was once thought impossible. Generative AI models have several capabilities and applications, but for this curriculum we'll explore how it's revolutionizing education through a fictional startup. We'll refer to this startup as our startup. Our startup works in the education domain with the ambitious mission statement of</p> <p>improving accessibility in learning, on a global scale, ensuring equitable access to education and providing personalized learning experiences to every learner, according to their needs.</p> <p>Our startup team is aware we\u2019ll not be able to achieve this goal without leveraging one of the most powerful tools of modern times \u2013 Large Language Models (LLMs).</p> <p>Generative AI is expected to revolutionize the way we learn and teach today, with students having at their disposal virtual teachers 24 hours a day who provide vast amounts of information and examples, and teachers able to leverage innovative tools to assess their students and give feedback.</p> <p></p> <p>To start, let\u2019s define some basic concepts and terminology we\u2019ll be using throughout the curriculum.</p>"},{"location":"lessons/generative-ai-for-beginners/01-introduction-to-genai/#how-did-we-get-generative-ai","title":"How did we get Generative AI?","text":"<p>Despite the extraordinary hype created lately by the announcement of generative AI models, this technology is decades in the making, with the first research efforts dating back to 60s. We're now at a point with AI having human cognitive capabilities, like conversation as shown by for example OpenAI ChatGPT or Bing Chat, which also uses a GPT model for the web search Bing conversations.</p> <p>Backing up a bit, the very first prototypes of AI consisted of typewritten chatbots, relying on a knowledge base extracted from a group of experts and represented into a computer. The answers in the knowledge base were triggered by keywords appearing in the input text. However, it soon became clear that such approach, using typewritten chatbots, did not scale well.</p>"},{"location":"lessons/generative-ai-for-beginners/01-introduction-to-genai/#a-statistical-approach-to-ai-machine-learning","title":"A statistical approach to AI: Machine Learning","text":"<p>A turning point arrived during the 90s, with the application of a statistical approach to text analysis. This led to the development of new algorithms \u2013 known with the name of machine learning - able to learn patterns from data, without being explicitly programmed. This approach allows a machine to simulate human language understanding: a statistical model is trained on text-label pairings, enabling the model to classify unknown input text with a pre-defined label representing the intention of the message.</p>"},{"location":"lessons/generative-ai-for-beginners/01-introduction-to-genai/#neural-networks-and-modern-virtual-assistants","title":"Neural networks and modern virtual assistants","text":"<p>In more recent times, the technological evolution of the hardware, capable of handling larger amounts of data and more complex computations, encouraged research in the AI fields, leading to the development of advanced machine learning algorithms \u2013 called neural networks or deep learning algorithms.</p> <p>Neural networks (and in particular Recurrent Neural Networks \u2013 RNNs) significantly enhanced natural language processing, enabling the representation of the meaning of text in a more meaningful way, valuing the context of a word in a sentence.</p> <p>This is the technology that powered the virtual assistants born in the first decade of the new century, very proficient in interpreting the human language, identifying a need, and performing an action to satisfy it \u2013 like answering with a pre-defined script or consuming a 3rd party service.</p>"},{"location":"lessons/generative-ai-for-beginners/01-introduction-to-genai/#present-day-generative-ai","title":"Present day, Generative AI","text":"<p>So that\u2019s how we came to Generative AI today, which can be seen as a subset of deep learning.</p> <p></p> <p>After decades of research in the AI field, a new model architecture \u2013 called Transformer \u2013 overcame the limits of RNNs, being able to get much longer sequences of text as input. Transformers are based on the attention mechanism, enabling the model to give different weights to the inputs it receives, \u2018paying more attention\u2019 where the most relevant information is concentrated, regardless of their order in the text sequence.</p> <p>Most of the recent generative AI models \u2013 also known as Large Language Models (LLMs), since they work with textual inputs and outputs \u2013 are indeed based on this architecture. What\u2019s interesting about these models \u2013 trained on a huge amount of unlabeled data from diverse sources like books, articles and websites \u2013 is that they can be adapted to a wide variety of tasks and generate grammatically correct text with a semblance of creativity. So, not only did they incredibly enhance the capacity of a machine to \u2018understand\u2019 an input text, but they enabled their capacity to generate an original response in human language.</p>"},{"location":"lessons/generative-ai-for-beginners/01-introduction-to-genai/#how-do-large-language-models-work","title":"How do large language models work?","text":"<p>In the next chapter we are going to explore different types of Generative AI models, but for now let\u2019s have a look at how large language models work, with a focus on OpenAI GPT (Generative Pre-trained Transformer) models.</p> <ul> <li>Tokenizer, text to numbers: Large Language Models receive a text as input and generate a text as output. However, being statistical models, they work much better with numbers than text sequences. That\u2019s why every input to the model is processed by a tokenizer, before being used by the core model. A token is a chunk of text \u2013 consisting of a variable number of characters, so the tokenizer's main task is splitting the input into an array of tokens. Then, each token is mapped with a token index, which is the integer encoding of the original text chunk.</li> </ul> <p></p> <ul> <li> <p>Predicting output tokens: Given n tokens as input (with max n varying from one model to another), the model is able to predict one token as output. This token is then incorporated into the input of the next iteration, in an expanding window pattern, enabling a better user experience of getting one (or multiple) sentence as an answer. This explains why, if you ever played with ChatGPT, you might have noticed that sometimes it looks like it stops in the middle of a sentence.</p> </li> <li> <p>Selection process, probability distribution: The output token is chosen by the model according to its probability of occurring after the current text sequence. This is because the model predicts a probability distribution over all possible \u2018next tokens\u2019, calculated based on its training. However, not always the token with the highest probability is chosen from the resulting distribution. A degree of randomness is added to this choice, in a way that the model acts in a non-deterministic fashion - we do not get the exact same output for the same input. This degree of randomness is added to simulate the process of creative thinking and it can be tuned using a model parameter called temperature.</p> </li> </ul>"},{"location":"lessons/generative-ai-for-beginners/01-introduction-to-genai/#how-can-our-startup-leverage-large-language-models","title":"How can our startup leverage Large Language Models?","text":"<p>Now that we have a better understanding of the inner working of a large language model, let\u2019s see some practical examples of the most common tasks they can perform pretty well, with an eye to our business scenario. We said that the main capability of a Large Language Model is generating a text from scratch, starting from a textual input, written in natural language.</p> <p>But what kind of textual input and output? The input of a large language model is known as prompt, while the output is known as completion, term that refers to the model mechanism of generating the next token to complete the current input. We are going to dive deep into what is a prompt and how to design it in a way to get the most out of our model. But for now, let\u2019s just say that a prompt may include:</p> <ul> <li> <p>An instruction specifying the type of output we expect from the model. This instruction sometimes might embed some examples or some additional data.</p> </li> <li> <p>Summarization of an article, book, product reviews and more, along with extraction of insights from unstructured data.</p> <p></p> </li> <li> <p>Creative ideation and design of an article, an essay, an assignment or more.</p> <p></p> </li> <li> <p>A question, asked in the form of a conversation with an agent.</p> </li> </ul> <p></p> <ul> <li>A chunk of text to complete, which implicitly is an ask for writing assistance.</li> </ul> <p></p> <ul> <li>A chunk of code together with the ask of explaining and documenting it, or a comment asking to generate a piece of code performing a specific task.</li> </ul> <p></p> <p>The examples above are quite simple and don\u2019t want to be an exhaustive demonstration of Large Language Models capabilities. They just want to show the potential of using generative AI, in particular but not limited to educational context.</p> <p>Also, the output of a generative AI model is not perfect and sometimes the creativity of the model can work against it, resulting in an output which is a combination of words that the human user can interpret as a mystification of reality, or it can be offensive. Generative AI is not intelligent - at least in the more comprehensive definition of intelligence, including critical and creative reasoning or emotional intelligence; it is not deterministic, and it is not trustworthy, since fabrications, such as erroneous references, content, and statements, may be combined with correct information, and presented in a persuasive and confident manner. In the following lessons, we\u2019ll be dealing with all these limitations and we\u2019ll see what we can do to mitigate them.</p>"},{"location":"lessons/generative-ai-for-beginners/01-introduction-to-genai/#assignment","title":"Assignment","text":"<p>Your assignment is to read up more on generative AI and try to identify an area where you would add generative AI today that doesn't have it. How would the impact be different from doing it the \"old way\", can you do something you couldn't before, or are you faster? Write a 300 word summary on what your dream AI startup would look like and include headers like \"Problem\", \"How I would use AI\", \"Impact\" and optionally a business plan.</p> <p>If you did this task, you might even be ready to apply to Microsoft's incubator, Microsoft for Startups Founders Hub we offer credits for both Azure, OpenAI, mentoring and much more, check it out!</p>"},{"location":"lessons/generative-ai-for-beginners/01-introduction-to-genai/#knowledge-check","title":"Knowledge check","text":"<p>What's true about large language models?</p> <ol> <li>You get the exact same response every time.</li> <li>It does things perfectly, great at adding numbers, produce working code etc.</li> <li>The response may vary despite using the same prompt. It's also great at giving you a first draft of something, be it text or code. But you need to improve on the results.</li> </ol> <p>A: 3, an LLM is non-deterministic, the response vary, however, you can control its variance via a temperature setting. You also shouldn't expect it to do things perfectly, it's here to do the heavy-lifting for you which often means you get a good first attempt at something that you need to gradually improve.</p>"},{"location":"lessons/generative-ai-for-beginners/01-introduction-to-genai/#great-work-continue-the-journey","title":"Great Work! Continue the Journey","text":"<p>After completing this lesson, check out our Generative AI Learning collection to continue leveling up your Generative AI knowledge!</p> <p>Head over to Lesson 2 where we will look at how to explore and compare different LLM types!</p>"},{"location":"lessons/generative-ai-for-beginners/02-exploring-and-comparing-different-llms/","title":"Lesson 2","text":""},{"location":"lessons/generative-ai-for-beginners/02-exploring-and-comparing-different-llms/#exploring-and-comparing-different-llms","title":"Exploring and comparing different LLMs","text":"<p>Click the image above to view video of this lesson</p> <p>With the previous lesson, we have seen how Generative AI is changing the technology landscape, how Large Language Models (LLMs) work and how a business - like our startup - can apply them to their use cases and grow! In this chapter, we're looking to compare and contrast different types of large language models (LLMs) to understand their pros and cons.</p> <p>The next step in our startup's journey is exploring the current landscape of LLMs and understanding which are suitable for our use case.</p>"},{"location":"lessons/generative-ai-for-beginners/02-exploring-and-comparing-different-llms/#introduction","title":"Introduction","text":"<p>This lesson will cover:</p> <ul> <li>Different types of LLMs in the current landscape.</li> <li>Testing, iterating, and comparing different models for your use case in Azure.</li> <li>How to deploy an LLM.</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/02-exploring-and-comparing-different-llms/#learning-goals","title":"Learning Goals","text":"<p>After completing this lesson, you will be able to:</p> <ul> <li>Select the right model for your use case.</li> <li>Understand how to test, iterate, and improve performance of your model.</li> <li>Know how businesses deploy models.</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/02-exploring-and-comparing-different-llms/#understand-different-types-of-llms","title":"Understand different types of LLMs","text":"<p>LLMs can have multiple categorizations based on their architecture, training data, and use case. Understanding these differences will help our startup select the right model for the scenario, and understand how to test, iterate, and improve performance.</p> <p>There are many different types of LLM models, your choice of model depends on what you aim to use them for, your data, how much you're ready to pay and more.</p> <p>Depending on if you aim to use the models for text, audio, video, image generation and so on, you might opt for a different type of model.</p> <ul> <li> <p>Audio and speech recognition. For this purpose, Whisper-type models are a great choice as they're general-purpose and aimed at speech recognition. It's trained on diverse audio and can perform multilingual speech recognition. Learn more about Whisper type models here.</p> </li> <li> <p>Image generation. For image generation, DALL-E and Midjourney are two very known choices. DALL-E is offered by Azure OpenAI. Read more about DALL-E here and also in Chapter 9 of this curriculum.</p> </li> <li> <p>Text generation. Most models are trained on text generation and you have a large variety of choices from GPT-3.5 to GPT-4. They come at different costs with GPT-4 being the most expensive. It's worth looking into the Azure OpenAI playground to evaluate which models best fit your needs in terms of capability and cost.</p> </li> <li> <p>Multi-modality. If you're looking to handle multiple types of data in input and output, you might want to look into models like gpt-4 turbo with vision or gpt-4o - the latest releases of OpenAI models - which are capable to combine natural language processing to visual understanding, enabling interactions through multi-modal interfaces.</p> </li> </ul> <p>Selecting a model means you get some basic capabilities, that might not be enough however. Often you have company specific data that you somehow need to tell the LLM about. There are a few different choices on how to approach that, more on that in the upcoming sections.</p>"},{"location":"lessons/generative-ai-for-beginners/02-exploring-and-comparing-different-llms/#foundation-models-versus-llms","title":"Foundation Models versus LLMs","text":"<p>The term Foundation Model was coined by Stanford researchers and defined as an AI model that follows some criteria, such as:</p> <ul> <li>They are trained using unsupervised learning or self-supervised learning, meaning they are trained on unlabeled multi-modal data, and they do not require human annotation or labeling of data for their training process.</li> <li>They are very large models, based on very deep neural networks trained on billions of parameters.</li> <li>They are normally intended to serve as a \u2018foundation\u2019 for other models, meaning they can be used as a starting point for other models to be built on top of, which can be done by fine-tuning.</li> </ul> <p></p> <p>Image source: Essential Guide to Foundation Models and Large Language Models | by Babar M Bhatti | Medium </p> <p>To further clarify this distinction, let\u2019s take ChatGPT as an example. To build the first version of ChatGPT, a model called GPT-3.5 served as the foundation model. This means that OpenAI used some chat-specific data to create a tuned version of GPT-3.5 that was specialized in performing well in conversational scenarios, such as chatbots.</p> <p></p> <p>Image source: 2108.07258.pdf (arxiv.org)</p>"},{"location":"lessons/generative-ai-for-beginners/02-exploring-and-comparing-different-llms/#open-source-versus-proprietary-models","title":"Open Source versus Proprietary Models","text":"<p>Another way to categorize LLMs is whether they are open source or proprietary.</p> <p>Open-source models are models that are made available to the public and can be used by anyone. They are often made available by the company that created them, or by the research community. These models are allowed to be inspected, modified, and customized for the various use cases in LLMs. However, they are not always optimized for production use, and may not be as performant as proprietary models. Plus, funding for open-source models can be limited, and they may not be maintained long term or may not be updated with the latest research. Examples of popular open source models include Alpaca, Bloom and LLaMA.</p> <p>Proprietary models are models that are owned by a company and are not made available to the public. These models are often optimized for production use. However, they are not allowed to be inspected, modified, or customized for different use cases. Plus, they are not always available for free, and may require a subscription or payment to use. Also, users do not have control over the data that is used to train the model, which means they should entrust the model owner with ensuring commitment to data privacy and responsible use of AI. Examples of popular proprietary models include OpenAI models, Google Bard or Claude 2.</p>"},{"location":"lessons/generative-ai-for-beginners/02-exploring-and-comparing-different-llms/#embedding-versus-image-generation-versus-text-and-code-generation","title":"Embedding versus Image generation versus Text and Code generation","text":"<p>LLMs can also be categorized by the output they generate.</p> <p>Embeddings are a set of models that can convert text into a numerical form, called embedding, which is a numerical representation of the input text. Embeddings make it easier for machines to understand the relationships between words or sentences and can be consumed as inputs by other models, such as classification models, or clustering models that have better performance on numerical data. Embedding models are often used for transfer learning, where a model is built for a surrogate task for which there\u2019s an abundance of data, and then the model weights (embeddings) are re-used for other downstream tasks. An example of this category is OpenAI embeddings.</p> <p></p> <p>Image generation models are models that generate images. These models are often used for image editing, image synthesis, and image translation. Image generation models are often trained on large datasets of images, such as LAION-5B, and can be used to generate new images or to edit existing images with inpainting, super-resolution, and colorization techniques. Examples include DALL-E-3 and Stable Diffusion models.</p> <p></p> <p>Text and code generation models are models that generate text or code. These models are often used for text summarization, translation, and question answering. Text generation models are often trained on large datasets of text, such as BookCorpus, and can be used to generate new text, or to answer questions. Code generation models, like CodeParrot, are often trained on large datasets of code, such as GitHub, and can be used to generate new code, or to fix bugs in existing code.</p> <p></p>"},{"location":"lessons/generative-ai-for-beginners/02-exploring-and-comparing-different-llms/#encoder-decoder-versus-decoder-only","title":"Encoder-Decoder versus Decoder-only","text":"<p>To talk about the different types of architectures of LLMs, let's use an analogy.</p> <p>Imagine your manager gave you a task for writing a quiz for the students. You have two colleagues; one oversees creating the content and the other oversees reviewing them.</p> <p>The content creator is like a Decoder only model, they can look at the topic and see what you already wrote and then he can write a course based on that. They are very good at writing engaging and informative content, but they are not very good at understanding the topic and the learning objectives. Some examples of Decoder models are GPT family models, such as GPT-3.</p> <p>The reviewer is like an Encoder only model, they look at the course written and the answers, noticing the relationship between them and understanding context, but they are not good at generating content. An example of Encoder only model would be BERT.</p> <p>Imagine that we can have someone as well who could create and review the quiz, this is an Encoder-Decoder model. Some examples would be BART and T5.</p>"},{"location":"lessons/generative-ai-for-beginners/02-exploring-and-comparing-different-llms/#service-versus-model","title":"Service versus Model","text":"<p>Now, let's talk about the difference between a service and a model. A service is a product that is offered by a Cloud Service Provider, and is often a combination of models, data, and other components. A model is the core component of a service, and is often a foundation model, such as an LLM.</p> <p>Services are often optimized for production use and are often easier to use than models, via a graphical user interface. However, services are not always available for free, and may require a subscription or payment to use, in exchange for leveraging the service owner\u2019s equipment and resources, optimizing expenses and scaling easily. An example of service is Azure OpenAI Service, which offers a pay-as-you-go rate plan, meaning users are charged proportionally to how much they use the service Also, Azure OpenAI Service offers enterprise-grade security and responsible AI framework on top of the models' capabilities.</p> <p>Models are just the Neural Network, with the parameters, weights, and others. Allowing companies to run locally, however, would need to buy equipment, build structure to scale and buy a license or use an open-source model. A model like LLaMA is available to be used, requiring computational power to run the model.</p>"},{"location":"lessons/generative-ai-for-beginners/02-exploring-and-comparing-different-llms/#how-to-test-and-iterate-with-different-models-to-understand-performance-on-azure","title":"How to test and iterate with different models to understand performance on Azure","text":"<p>Once our team has explored the current LLMs landscape and identified some good candidates for their scenarios, the next step is testing them on their data and on their workload. This is an iterative process, done by experiments and measures. Most of the models we mentioned in previous paragraphs (OpenAI models, open source models like Llama2, and Hugging Face transformers) are available in the Model Catalog in Azure AI Studio.</p> <p>Azure AI Studio is a Cloud Platform designed for developers to build generative AI applications and manage the whole development lifecycle - from experimentation to evaluation - by combining all Azure AI services into a single hub with an handy GUI. The Model Catalog in Azure AI Studio enables the user to:</p> <ul> <li>Find the Foundation Model of interest in the catalog - either proprietary or open source, filtering by task, license, or name. To improve searchability, the models are organized into collections, like Azure OpenAI collection, Hugging Face collection, and more.</li> </ul> <p></p> <ul> <li>Review the model card, including a detailed description of intended use and training data, code samples and evaluation results on internal evaluations library.</li> </ul> <p></p> <ul> <li>Compare benchmarks across models and datasets available in the industry to assess which one meets the business scenario, through the Model Benchmarks pane.</li> </ul> <p></p> <ul> <li>Fine-tune the model on custom training data to improve model performance in a specific workload, leveraging the experimentation and tracking capabilities of Azure AI Studio.</li> </ul> <p></p> <ul> <li>Deploy the original pre-trained model or the fine-tuned version to a remote real time inference - managed compute - or serverless api endpoint - pay-as-you-go - to enable applications to consume it.</li> </ul> <p></p> <p>[!NOTE] Not all models in the catalog are currently available for fine-tuning and/or pay-as-you-go deployment. Check the model card for details on the model's capabilities and limitations.</p>"},{"location":"lessons/generative-ai-for-beginners/02-exploring-and-comparing-different-llms/#improving-llm-results","title":"Improving LLM results","text":"<p>We\u2019ve explored with our startup team different kinds of LLMs and a Cloud Platform (Azure Machine Learning) enabling us to compare different models, evaluate them on test data, improve performance and deploy them on inference endpoints.</p> <p>But when shall they consider fine-tuning a model rather than using a pre-trained one? Are there other approaches to improve model performance on specific workloads?</p> <p>There are several approaches a business can use to get the results they need from an LLM. You can select different types of models with different degrees of training when deploying an LLM in production, with different levels of complexity, cost, and quality. Here are some different approaches:</p> <ul> <li> <p>Prompt engineering with context. The idea is to provide enough context when you prompt to ensure you get the responses you need.</p> </li> <li> <p>Retrieval Augmented Generation, RAG. Your data might exist in a database or web endpoint for example, to ensure this data, or a subset of it, is included at the time of prompting, you can fetch the relevant data and make that part of the user's prompt.</p> </li> <li> <p>Fine-tuned model. Here, you trained the model further on your own data which leads to the model being more exact and responsive to your needs but might be costly.</p> </li> </ul> <p></p> <p>Img source: Four Ways that Enterprises Deploy LLMs | Fiddler AI Blog</p>"},{"location":"lessons/generative-ai-for-beginners/02-exploring-and-comparing-different-llms/#prompt-engineering-with-context","title":"Prompt Engineering with Context","text":"<p>Pre-trained LLMs work very well on generalized natural language tasks, even by calling them with a short prompt, like a sentence to complete or a question \u2013 the so-called \u201czero-shot\u201d learning.</p> <p>However, the more the user can frame their query, with a detailed request and examples \u2013 the Context \u2013 the more accurate and closest to user\u2019s expectations the answer will be. In this case, we talk about \u201cone-shot\u201d learning if the prompt includes only one example and \u201cfew shot learning\u201d if it includes multiple examples. Prompt engineering with context is the most cost-effective approach to kick-off with.</p>"},{"location":"lessons/generative-ai-for-beginners/02-exploring-and-comparing-different-llms/#retrieval-augmented-generation-rag","title":"Retrieval Augmented Generation (RAG)","text":"<p>LLMs have the limitation that they can use only the data that has been used during their training to generate an answer. This means that they don\u2019t know anything about the facts that happened after their training process, and they cannot access non-public information (like company data). This can be overcome through RAG, a technique that augments prompt with external data in the form of chunks of documents, considering prompt length limits. This is supported by Vector database tools (like Azure Vector Search) that retrieve the useful chunks from varied pre-defined data sources and add them to the prompt Context.</p> <p>This technique is very helpful when a business doesn\u2019t have enough data, enough time, or resources to fine-tune an LLM, but still wishes to improve performance on a specific workload and reduce risks of fabrications, i.e., mystification of reality or harmful content.</p>"},{"location":"lessons/generative-ai-for-beginners/02-exploring-and-comparing-different-llms/#fine-tuned-model","title":"Fine-tuned model","text":"<p>Fine-tuning is a process that leverages transfer learning to \u2018adapt\u2019 the model to a downstream task or to solve a specific problem. Differently from few-shot learning and RAG, it results in a new model being generated, with updated weights and biases. It requires a set of training examples consisting of a single input (the prompt) and its associated output (the completion). This would be the preferred approach if:</p> <ul> <li> <p>Using fine-tuned models. A business would like to use fine-tuned less capable models (like embedding models) rather than high performance models, resulting in a more cost effective and fast solution.</p> </li> <li> <p>Considering latency. Latency is important for a specific use-case, so it\u2019s not possible to use very long prompts or the number of examples that should be learned from the model doesn\u2019t fit with the prompt length limit.</p> </li> <li> <p>Staying up to date. A business has a lot of high-quality data and ground truth labels and the resources required to maintain this data up to date over time.</p> </li> </ul>"},{"location":"lessons/generative-ai-for-beginners/02-exploring-and-comparing-different-llms/#trained-model","title":"Trained model","text":"<p>Training an LLM from scratch is without a doubt the most difficult and the most complex approach to adopt, requiring massive amounts of data, skilled resources, and appropriate computational power. This option should be considered only in a scenario where a business has a domain-specific use case and a large amount of domain-centric data.</p>"},{"location":"lessons/generative-ai-for-beginners/02-exploring-and-comparing-different-llms/#knowledge-check","title":"Knowledge check","text":"<p>What could be a good approach to improve LLM completion results?</p> <ol> <li>Prompt engineering with context</li> <li>RAG</li> <li>Fine-tuned model</li> </ol> <p>A:3, if you have the time and resources and high quality data, fine-tuning is the better option to stay up to date. However, if you're looking at improving things and you're lacking time it's worth considering RAG first.</p>"},{"location":"lessons/generative-ai-for-beginners/02-exploring-and-comparing-different-llms/#challenge","title":"\ud83d\ude80 Challenge","text":"<p>Read up more on how you can use RAG for your business.</p>"},{"location":"lessons/generative-ai-for-beginners/02-exploring-and-comparing-different-llms/#great-work-continue-your-learning","title":"Great Work, Continue Your Learning","text":"<p>After completing this lesson, check out our Generative AI Learning collection to continue leveling up your Generative AI knowledge!</p> <p>Head over to Lesson 3 where we will look at how to build with Generative AI Responsibly!</p>"},{"location":"lessons/generative-ai-for-beginners/03-using-generative-ai-responsibly/","title":"Lesson 3","text":""},{"location":"lessons/generative-ai-for-beginners/03-using-generative-ai-responsibly/#using-generative-ai-responsibly","title":"Using Generative AI Responsibly","text":"<p>Click the image above to view video of this lesson</p> <p>It's easy to be fascinated with AI and generative AI in particular, but you need to consider how you would use it responsibly. You need to consider things like how to ensure the output is fair, non-harmful and more. This chapter aims to provide you with the mentioned context, what to consider, and how to take active steps to improve your AI usage.</p>"},{"location":"lessons/generative-ai-for-beginners/03-using-generative-ai-responsibly/#introduction","title":"Introduction","text":"<p>This lesson will cover:</p> <ul> <li>Why you should prioritize Responsible AI when building Generative AI applications.</li> <li>Core principles of Responsible AI and how they relate to Generative AI.</li> <li>How to put these Responsible AI principles into practice through strategy and tooling.</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/03-using-generative-ai-responsibly/#learning-goals","title":"Learning Goals","text":"<p>After completing this lesson you will know:</p> <ul> <li>The importance of Responsible AI when building Generative AI applications.</li> <li>When to think and apply the core principles of Responsible AI when building Generative AI applications.</li> <li>What tools and strategies are available to you to put the concept of Responsible AI into practice.</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/03-using-generative-ai-responsibly/#responsible-ai-principles","title":"Responsible AI Principles","text":"<p>The excitement of Generative AI has never been higher. This excitement has brought a lot of new developers, attention, and funding to this space. While this is very positive for anyone looking to build products and companies using Generative AI, it is also important we proceed responsibly.</p> <p>Throughout this course, we are focusing on building our startup and our AI education product. We\u2019ll use the principles of Responsible AI: Fairness, Inclusiveness, Reliability/Safety, Security &amp; Privacy, Transparency and Accountability. With these principles, we will explore how they relate to our use of Generative AI in our products.</p>"},{"location":"lessons/generative-ai-for-beginners/03-using-generative-ai-responsibly/#why-should-you-prioritize-responsible-ai","title":"Why Should You Prioritize Responsible AI","text":"<p>When building a product, taking a human-centric approach by keeping your user's best interest in mind leads to the best results.</p> <p>The uniqueness of Generative AI is its power to create helpful answers, information, guidance, and content for users. This can be done without many manual steps which can lead to very impressive results. Without proper planning and strategies, it can also unfortunately lead to some harmful results for your users, your product, and society as a whole.</p> <p>Let's look at some (but not all) of these potentially harmful results:</p>"},{"location":"lessons/generative-ai-for-beginners/03-using-generative-ai-responsibly/#hallucinations","title":"Hallucinations","text":"<p>Hallucinations are a term used to describe when an LLM produces content that is either completely nonsensical or something we know is factually wrong based on other sources of information.</p> <p>Let's take for example we build a feature for our startup that allows students to ask historical questions to a model. A student asks the question <code>Who was the sole survivor of Titanic?</code></p> <p>The model produces a response such as the one below:</p> <p></p> <p>(Source: Flying bisons)</p> <p>This is a very confident and thorough answer. Unfortunately, it is incorrect. Even with a minimal amount of research, one would discover there was more than one survivor of the Titanic disaster. For a student who is just starting to research this topic, this answer can be persuasive enough to not be questioned and treated as fact. The consequences of this can lead to the AI system being unreliable and negatively impact the reputation of our startup.</p> <p>With each iteration of any given LLM, we have seen performance improvements around minimizing hallucinations. Even with this improvement, we as application builders and users still need to remain aware of these limitations.</p>"},{"location":"lessons/generative-ai-for-beginners/03-using-generative-ai-responsibly/#harmful-content","title":"Harmful Content","text":"<p>We covered in the earlier section when an LLM produces incorrect or nonsensical responses. Another risk we need to be aware of is when a model responds with harmful content.</p> <p>Harmful content can be defined as:</p> <ul> <li>Providing instructions or encouraging self-harm or harm to certain groups.</li> <li>Hateful or demeaning content.</li> <li>Guiding planning any type of attack or violent acts.</li> <li>Providing instructions on how to find illegal content or commit illegal acts.</li> <li>Displaying sexually explicit content.</li> </ul> <p>For our startup, we want to make sure we have the right tools and strategies in place to prevent this type of content from being seen by students.</p>"},{"location":"lessons/generative-ai-for-beginners/03-using-generative-ai-responsibly/#lack-of-fairness","title":"Lack of Fairness","text":"<p>Fairness is defined as \u201censuring that an AI system is free from bias and discrimination and that they treat everyone fairly and equally.\u201d In the world of Generative AI, we want to ensure that exclusionary worldviews of marginalized groups are not reinforced by the model\u2019s output.</p> <p>These types of outputs are not only destructive to building positive product experiences for our users, but they also cause further societal harm. As application builders, we should always keep a wide and diverse user base in mind when building solutions with Generative AI.</p>"},{"location":"lessons/generative-ai-for-beginners/03-using-generative-ai-responsibly/#how-to-use-generative-ai-responsibly","title":"How to Use Generative AI Responsibly","text":"<p>Now that we have identified the importance of Responsible Generative AI, let's look at 4 steps we can take to build our AI solutions responsibly:</p> <p></p>"},{"location":"lessons/generative-ai-for-beginners/03-using-generative-ai-responsibly/#measure-potential-harms","title":"Measure Potential Harms","text":"<p>In software testing, we test the expected actions of a user on an application. Similarly, testing a diverse set of prompts users are most likely going to use is a good way to measure potential harm.</p> <p>Since our startup is building an education product, it would be good to prepare a list of education-related prompts. This could be to cover a certain subject, historical facts, and prompts about student life.</p>"},{"location":"lessons/generative-ai-for-beginners/03-using-generative-ai-responsibly/#mitigate-potential-harms","title":"Mitigate Potential Harms","text":"<p>It is now time to find ways where we can prevent or limit the potential harm caused by the model and its responses. We can look at this in 4 different layers:</p> <p></p> <ul> <li> <p>Model. Choosing the right model for the right use case. Larger and more complex models like GPT-4 can cause more of a risk of harmful content when applied to smaller and more specific use cases. Using your training data to fine-tune also reduces the risk of harmful content.</p> </li> <li> <p>Safety System. A safety system is a set of tools and configurations on the platform serving the model that help mitigate harm. An example of this is the content filtering system on the Azure OpenAI service. Systems should also detect jailbreak attacks and unwanted activity like requests from bots.</p> </li> <li> <p>Metaprompt. Metaprompts and grounding are ways we can direct or limit the model based on certain behaviors and information. This could be using system inputs to define certain limits of the model. In addition, providing outputs that are more relevant to the scope or domain of the system.</p> </li> </ul> <p>It can also be using techniques like Retrieval Augmented Generation (RAG) to have the model only pull information from a selection of trusted sources. There is a lesson later in this course for building search applications</p> <ul> <li>User Experience. The final layer is where the user interacts directly with the model through our application\u2019s interface in some way. In this way we can design the UI/UX to limit the user on the types of inputs they can send to the model as well as text or images displayed to the user. When deploying the AI application, we also must be transparent about what our Generative AI application can and can\u2019t do.</li> </ul> <p>We have an entire lesson dedicated to Designing UX for AI Applications</p> <ul> <li>Evaluate model. Working with LLMs can be challenging because we don\u2019t always have control over the data the model was trained on. Regardless, we should always evaluate the model\u2019s performance and outputs. It\u2019s still important to measure the model\u2019s accuracy, similarity, groundedness, and relevance of the output. This helps provide transparency and trust to stakeholders and users.</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/03-using-generative-ai-responsibly/#operate-a-responsible-generative-ai-solution","title":"Operate a Responsible Generative AI solution","text":"<p>Building an operational practice around your AI applications is the final stage. This includes partnering with other parts of our startup like Legal and Security to ensure we are compliant with all regulatory policies. Before launching, we also want to build plans around delivery, handling incidents, and rollback to prevent any harm to our users from growing.</p>"},{"location":"lessons/generative-ai-for-beginners/03-using-generative-ai-responsibly/#tools","title":"Tools","text":"<p>While the work of developing Responsible AI solutions may seem like a lot, it is work well worth the effort. As the area of Generative AI grows, more tooling to help developers efficiently integrate responsibility into their workflows will mature. For example, the Azure AI Content Safety can help detect harmful content and images via an API request.</p>"},{"location":"lessons/generative-ai-for-beginners/03-using-generative-ai-responsibly/#knowledge-check","title":"Knowledge check","text":"<p>What are some things you need to care about to ensure responsible AI usage?</p> <ol> <li>That the answer is correct.</li> <li>Harmful usage, that AI isn't used for criminal purposes.</li> <li>Ensuring the AI is free from bias and discrimination.</li> </ol> <p>A: 2 and 3 are correct. Responsible AI helps you consider how to mitigate harmful effects and biases and more.</p>"},{"location":"lessons/generative-ai-for-beginners/03-using-generative-ai-responsibly/#challenge","title":"\ud83d\ude80 Challenge","text":"<p>Read up on Azure AI Content Saftey and see what you can adopt for your usage.</p>"},{"location":"lessons/generative-ai-for-beginners/03-using-generative-ai-responsibly/#great-work-continue-your-learning","title":"Great Work, Continue Your Learning","text":"<p>After completing this lesson, check out our Generative AI Learning collection to continue leveling up your Generative AI knowledge!</p> <p>Head over to Lesson 4 where we will look at Prompt Engineering Fundamentals!</p>"},{"location":"lessons/generative-ai-for-beginners/04-prompt-engineering-fundamentals/","title":"Lesson 4","text":""},{"location":"lessons/generative-ai-for-beginners/04-prompt-engineering-fundamentals/#prompt-engineering-fundamentals","title":"Prompt Engineering Fundamentals","text":""},{"location":"lessons/generative-ai-for-beginners/04-prompt-engineering-fundamentals/#introduction","title":"Introduction","text":"<p>This module covers essential concepts and techniques for creating effective prompts in generative AI models. The way your write your prompt to an LLM also matters. A carefully-crafted prompt can achieve a better quality of response. But what exactly do terms like prompt and prompt engineering mean? And how do I improve the prompt input that I send to the LLM? These are the questions we'll try to answer with in this chapter and the next.</p> <p>Generative AI is capable of creating new content (e.g., text, images, audio, code etc.) in response to user requests. It achieves this using Large Language Models like OpenAI's GPT (\"Generative Pre-trained Transformer\") series that are trained for using natural language and code.</p> <p>Users can now interact with these models using familiar paradigms like chat, without needing any technical expertise or training. The models are prompt-based - users send a text input (prompt) and get back the AI response (completion). They can then \"chat with the AI\" iteratively, in multi-turn conversations, refining their prompt until the response matches their expectations.</p> <p>\"Prompts\" now become the primary programming interface for generative AI apps, telling the models what to do and influencing the quality of returned responses. \"Prompt Engineering\" is a fast-growing field of study that focuses on the design and optimization of prompts to deliver consistent and quality responses at scale.</p>"},{"location":"lessons/generative-ai-for-beginners/04-prompt-engineering-fundamentals/#learning-goals","title":"Learning Goals","text":"<p>In this lesson, we learn what Prompt Engineering is, why it matters, and how we can craft more effective prompts for a given model and application objective. We'll understand core concepts and best practices for prompt engineering - and learn about an interactive Jupyter Notebooks \"sandbox\" environment where we can see these concepts applied to real examples.</p> <p>By the end of this lesson we will be able to:</p> <ol> <li>Explain what prompt engineering is and why it matters.</li> <li>Describe the components of a prompt and how they are used.</li> <li>Learn best practices and techniques for prompt engineering.</li> <li>Apply learned techniques to real examples, using an OpenAI endpoint.</li> </ol>"},{"location":"lessons/generative-ai-for-beginners/04-prompt-engineering-fundamentals/#key-terms","title":"Key Terms","text":"<p>Prompt Engineering: The practice of designing and refining inputs to guide AI models toward producing desired outputs. Tokenization: The process of converting text into smaller units, called tokens, that a model can understand and process. Instruction-Tuned LLMs: Large Language Models (LLMs) that have been fine-tuned with specific instructions to improve their response accuracy and relevance.</p>"},{"location":"lessons/generative-ai-for-beginners/04-prompt-engineering-fundamentals/#learning-sandbox","title":"Learning Sandbox","text":"<p>Prompt engineering is currently more art than science. The best way to improve our intuition for it is to practice more and adopt a trial-and-error approach that combines application domain expertise with recommended techniques and model-specific optimizations.</p> <p>The Jupyter Notebook accompanying this lesson provides a sandbox environment where you can try out what you learn - as you go or as part of the code challenge at the end. To execute the exercises, you will need:</p> <ol> <li>An Azure OpenAI API key - the service endpoint for a deployed LLM.</li> <li>A Python Runtime - in which the Notebook can be executed.</li> <li>Local Env Variables - complete the SETUP steps now to get ready.</li> </ol> <p>The notebook comes with starter exercises - but you are encouraged to add your own Markdown (description) and Code (prompt requests) sections to try out more examples or ideas - and build your intuition for prompt design.</p>"},{"location":"lessons/generative-ai-for-beginners/04-prompt-engineering-fundamentals/#illustrated-guide","title":"Illustrated Guide","text":"<p>Want to get the big picture of what this lesson covers before you dive in? Check out this illustrated guide, which gives you a sense of the main topics covered and the key takeaways for you to think about in each one. The lesson roadmap takes you from understanding the core concepts and challenges to addressing them with relevant prompt engineering techniques and best practices. Note that the \"Advanced Techniques\" section in this guide refers to content covered in the next chapter of this curriculum.</p> <p></p>"},{"location":"lessons/generative-ai-for-beginners/04-prompt-engineering-fundamentals/#our-startup","title":"Our Startup","text":"<p>Now, let's talk about how this topic relates to our startup mission to bring AI innovation to education. We want to build AI-powered applications of personalized learning - so let's think about how different users of our application might \"design\" prompts:</p> <ul> <li>Administrators might ask the AI to analyze curriculum data to identify gaps in coverage. The AI can summarize results or visualize them with code.</li> <li>Educators might ask the AI to generate a lesson plan for a target audience and topic. The AI can build the personalized plan in a specified format.</li> <li>Students might ask the AI to tutor them in a difficult subject. The AI can now guide students with lessons, hints &amp; examples tailored to their level.</li> </ul> <p>That's just the tip of the iceberg. Check out Prompts For Education - an open-source prompts library curated by education experts - to get a broader sense of the possibilities! Try running some of those prompts in the sandbox or using the OpenAI Playground to see what happens!</p>"},{"location":"lessons/generative-ai-for-beginners/04-prompt-engineering-fundamentals/#what-is-prompt-engineering","title":"What is Prompt Engineering?","text":"<p>We started this lesson by defining Prompt Engineering as the process of designing and optimizing text inputs (prompts) to deliver consistent and quality responses (completions) for a given application objective and model. We can think of this as a 2-step process:</p> <ul> <li>designing the initial prompt for a given model and objective</li> <li>refining the prompt iteratively to improve the quality of the response</li> </ul> <p>This is necessarily a trial-and-error process that requires user intuition and effort to get optimal results. So why is it important? To answer that question, we first need to understand three concepts:</p> <ul> <li>Tokenization = how the model \"sees\" the prompt</li> <li>Base LLMs = how the foundation model \"processes\" a prompt</li> <li>Instruction-Tuned LLMs = how the model can now see \"tasks\"</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/04-prompt-engineering-fundamentals/#tokenization","title":"Tokenization","text":"<p>An LLM sees prompts as a sequence of tokens where different models (or versions of a model) can tokenize the same prompt in different ways. Since LLMs are trained on tokens (and not on raw text), the way prompts get tokenized has a direct impact on the quality of the generated response.</p> <p>To get an intuition for how tokenization works, try tools like the OpenAI Tokenizer shown below. Copy in your prompt - and see how that gets converted into tokens, paying attention to how whitespace characters and punctuation marks are handled. Note that this example shows an older LLM (GPT-3) - so trying this with a newer model may produce a different result.</p> <p></p>"},{"location":"lessons/generative-ai-for-beginners/04-prompt-engineering-fundamentals/#concept-foundation-models","title":"Concept: Foundation Models","text":"<p>Once a prompt is tokenized, the primary function of the \"Base LLM\" (or Foundation model) is to predict the token in that sequence. Since LLMs are trained on massive text datasets, they have a good sense of the statistical relationships between tokens and can make that prediction with some confidence. Note that they don't understand the meaning of the words in the prompt or token; they just see a pattern they can \"complete\" with their next prediction. They can continue predicting the sequence till terminated by user intervention or some pre-established condition.</p> <p>Want to see how prompt-based completion works? Enter the above prompt into the Azure OpenAI Studio Chat Playground with the default settings. The system is configured to treat prompts as requests for information - so you should see a completion that satisfies this context.</p> <p>But what if the user wanted to see something specific that met some criteria or task objective? This is where instruction-tuned LLMs come into the picture.</p> <p></p>"},{"location":"lessons/generative-ai-for-beginners/04-prompt-engineering-fundamentals/#concept-instruction-tuned-llms","title":"Concept: Instruction Tuned LLMs","text":"<p>An Instruction Tuned LLM starts with the foundation model and fine-tunes it with examples or input/output pairs (e.g., multi-turn \"messages\") that can contain clear instructions - and the response from the AI attempt to follow that instruction.</p> <p>This uses techniques like Reinforcement Learning with Human Feedback (RLHF) that can train the model to follow instructions and learn from feedback so that it produces responses that are better-suited to practical applications and more relevant to user objectives.</p> <p>Let's try it out - revisit the prompt above, but now change the system message to provide the following instruction as context:</p> <p>Summarize content you are provided with for a second-grade student. Keep the result to one paragraph with 3-5 bullet points.</p> <p>See how the result is now tuned to reflect the desired goal and format? An educator can now directly use this response in their slides for that class.</p> <p></p>"},{"location":"lessons/generative-ai-for-beginners/04-prompt-engineering-fundamentals/#why-do-we-need-prompt-engineering","title":"Why do we need Prompt Engineering?","text":"<p>Now that we know how prompts are processed by LLMs, let's talk about why we need prompt engineering. The answer lies in the fact that current LLMs pose a number of challenges that make reliable and consistent completions more challenging to achieve without putting effort into prompt construction and optimization. For instance:</p> <ol> <li> <p>Model responses are stochastic. The same prompt will likely produce different responses with different models or model versions. And it may even produce different results with the same model at different times. Prompt engineering techniques can help us minimize these variations by providing better guardrails.</p> </li> <li> <p>Models can fabricate responses. Models are pre-trained with large but finite datasets, meaning they lack knowledge about concepts outside that training scope. As a result, they can produce completions that are inaccurate, imaginary, or directly contradictory to known facts. Prompt engineering techniques help users identify and mitigate such fabrications e.g., by asking AI for citations or reasoning.</p> </li> <li> <p>Models capabilities will vary. Newer models or model generations will have richer capabilities but also bring unique quirks and tradeoffs in cost &amp; complexity. Prompt engineering can help us develop best practices and workflows that abstract away differences and adapt to model-specific requirements in scalable, seamless ways.</p> </li> </ol> <p>Let's see this in action in the OpenAI or Azure OpenAI Playground:</p> <ul> <li>Use the same prompt with different LLM deployments (e.g, OpenAI, Azure OpenAI, Hugging Face) - did you see the variations?</li> <li>Use the same prompt repeatedly with the same LLM deployment (e.g., Azure OpenAI playground) - how did these variations differ?</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/04-prompt-engineering-fundamentals/#fabrications-example","title":"Fabrications Example","text":"<p>In this course, we use the term \"fabrication\" to reference the phenomenon where LLMs sometimes generate factually incorrect information due to limitations in their training or other constraints. You may also have heard this referred to as \"hallucinations\" in popular articles or research papers. However, we strongly recommend using \"fabrication\" as the term so we don't accidentally anthropomorphize the behavior by attributing a human-like trait to a machine-driven outcome. This also reinforces Responsible AI guidelines from a terminology perspective, removing terms that may also be considered offensive or non-inclusive in some contexts.</p> <p>Want to get a sense of how fabrications work? Think of a prompt that instructs the AI to generate content for a non-existent topic (to ensure it is not found in the training dataset). For example - I tried this prompt:</p> <p>Prompt: generate a lesson plan on the Martian War of 2076.</p> <p>A web search showed me that there were fictional accounts (e.g., television series or books) on Martian wars - but none in 2076. Commonsense also tells us that 2076 is in the future and thus, cannot be associated with a real event.</p> <p>So what happens when we run this prompt with different LLM providers?</p> <p>Response 1: OpenAI Playground (GPT-35)</p> <p></p> <p>Response 2: Azure OpenAI Playground (GPT-35)</p> <p></p> <p>Response 3: : Hugging Face Chat Playground (LLama-2)</p> <p></p> <p>As expected, each model (or model version) produces slightly different responses thanks to stochastic behavior and model capability variations. For instance, one model targets an 8th grade audience while the other assumes a high-school student. But all three models did generate responses that could convince an uninformed user that the event was real</p> <p>Prompt engineering techniques like metaprompting and temperature configuration may reduce model fabrications to some extent. New prompt engineering architectures also incorporate new tools and techniques seamlessly into the prompt flow, to mitigate or reduce some of these effects.</p>"},{"location":"lessons/generative-ai-for-beginners/04-prompt-engineering-fundamentals/#case-study-github-copilot","title":"Case Study: GitHub Copilot","text":"<p>Let's wrap this section by getting a sense of how prompt engineering is used in real-world solutions by looking at one Case Study: GitHub Copilot.</p> <p>GitHub Copilot is your \"AI Pair Programmer\" - it converts text prompts into code completions and is integrated into your development environment (e.g., Visual Studio Code) for a seamless user experience. As documented in the series of blogs below, the earliest version was based on the OpenAI Codex model - with engineers quickly realizing the need to fine-tune the model and develop better prompt engineering techniques, to improve code quality. In July, they debuted an improved AI model that goes beyond Codex for even faster suggestions.</p> <p>Read the posts in order, to follow their learning journey.</p> <ul> <li>May 2023 | GitHub Copilot is Getting Better at Understanding Your Code</li> <li>May 2023 | Inside GitHub: Working with the LLMs behind GitHub Copilot.</li> <li>Jun 2023 | How to write better prompts for GitHub Copilot.</li> <li>Jul 2023 | .. GitHub Copilot goes beyond Codex with improved AI model</li> <li>Jul 2023 | A Developer's Guide to Prompt Engineering and LLMs</li> <li>Sep 2023 | How to build an enterprise LLM app: Lessons from GitHub Copilot</li> </ul> <p>You can also browse their Engineering blog for more posts like this one that shows how these models and techniques are applied for driving real-world applications.</p>"},{"location":"lessons/generative-ai-for-beginners/04-prompt-engineering-fundamentals/#prompt-construction","title":"Prompt Construction","text":"<p>We've seen why prompt engineering is important - now let's understand how prompts are constructed so we can evaluate different techniques for more effective prompt design.</p>"},{"location":"lessons/generative-ai-for-beginners/04-prompt-engineering-fundamentals/#basic-prompt","title":"Basic Prompt","text":"<p>Let's start with the basic prompt: a text input sent to the model with no other context. Here's an example - when we send the first few words of the US national anthem to the OpenAI Completion API it instantly completes the response with the next few lines, illustrating the basic prediction behavior.</p> Prompt (Input) Completion (Output) Oh say can you see It sounds like you're starting the lyrics to \"The Star-Spangled Banner,\" the national anthem of the United States. The full lyrics are ..."},{"location":"lessons/generative-ai-for-beginners/04-prompt-engineering-fundamentals/#complex-prompt","title":"Complex Prompt","text":"<p>Now let's add context and instructions to that basic prompt. The Chat Completion API lets us construct a complex prompt as a collection of messages with:</p> <ul> <li>Input/output pairs reflecting user input and assistant response.</li> <li>System message setting the context for assistant behavior or personality.</li> </ul> <p>The request is now in the form below, where the tokenization effectively captures relevant information from context and conversation. Now, changing the system context can be as impactful on the quality of completions, as the user inputs provided.</p> <pre><code>response = openai.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n    ]\n)\n</code></pre>"},{"location":"lessons/generative-ai-for-beginners/04-prompt-engineering-fundamentals/#instruction-prompt","title":"Instruction Prompt","text":"<p>In the above examples, the user prompt was a simple text query that can be interpreted as a request for information. With instruction prompts, we can use that text to specify a task in more detail, providing better guidance to the AI. Here's an example:</p> Prompt (Input) Completion (Output) Instruction Type Write a description of the Civil War returned a simple paragraph Simple Write a description of the Civil War. Provide key dates and events and describe their significance returned a paragraph followed by a list of key event dates with descriptions Complex Write a description of the Civil War in 1 paragraph. Provide 3 bullet points with key dates and their significance. Provide 3 more bullet points with key historical figures and their contributions. Return the output as a JSON file returns more extensive details in a text box, formatted as JSON that you can copy-paste to a file and validate as needed Complex. Formatted."},{"location":"lessons/generative-ai-for-beginners/04-prompt-engineering-fundamentals/#primary-content","title":"Primary Content","text":"<p>In the above examples, the prompt was still fairly open-ended, allowing the LLM to decide what part of its pre-trained dataset was relevant. With the primary content design pattern, the input text is divided into two parts:</p> <ul> <li>an instruction (action)</li> <li>relevant content (that influences action)</li> </ul> <p>Here's an example where the instruction is to \"summarize this in 2 sentences\".</p> Prompt (Input) Completion (Output) Jupiter is the fifth planet from the Sun and the largest in the Solar System. It is a gas giant with a mass one-thousandth that of the Sun, but two-and-a-half times that of all the other planets in the Solar System combined. Jupiter is one of the brightest objects visible to the naked eye in the night sky, and has been known to ancient civilizations since before recorded history. It is named after the Roman god Jupiter.[19] When viewed from Earth, Jupiter can be bright enough for its reflected light to cast visible shadows,[20] and is on average the third-brightest natural object in the night sky after the Moon and Venus.  Summarize this in 2 short sentences Jupiter, the fifth planet from the Sun, is the largest in the Solar System and is known for being one of the brightest objects in the night sky. Named after the Roman god Jupiter, it's a gas giant whose mass is two-and-a-half times that of all other planets in the Solar System combined. <p>The primary content segment can be used in various ways to drive more effective instructions:</p> <ul> <li>Examples - instead of telling the model what to do with an explicit instruction, give it examples of what to do and let it infer the pattern.</li> <li>Cues - follow the instruction with a \"cue\" that primes the completion, guiding the model towards more relevant responses.</li> <li>Templates - these are repeatable 'recipes' for prompts with placeholders (variables) that can be customized with data for specific use cases.</li> </ul> <p>Let's explore these in action.</p>"},{"location":"lessons/generative-ai-for-beginners/04-prompt-engineering-fundamentals/#using-examples","title":"Using Examples","text":"<p>This is an approach where you use the primary content to \"feed the model\" some examples of the desired output for a given instruction, and let it infer the pattern for the desired output. Based on the number of examples provided, we can have zero-shot prompting, one-shot prompting, few-shot prompting etc.</p> <p>The prompt now consists of three components:</p> <ul> <li>A task description</li> <li>A few examples of the desired output</li> <li>The start of a new example (which becomes an implicit task description)</li> </ul> Learning Type Prompt (Input) Completion (Output) Zero-shot \"The Sun is Shining\". Translate to Spanish \"El Sol est\u00e1 brillando\". One-shot \"The Sun is Shining\" =&gt; \"\"El Sol est\u00e1 brillando\".  \"It's a Cold and Windy Day\" =&gt; \"Es un d\u00eda fr\u00edo y ventoso\". Few-shot The player ran the bases =&gt; Baseball  The player hit an ace =&gt; Tennis  The player hit a six =&gt; Cricket  The player made a slam-dunk =&gt; Basketball <p>Note how we had to provide explicit instruction (\"Translate to Spanish\") in zero-shot prompting, but it gets inferred in the one-shot prompting example. The few-shot example shows how adding more examples allows models to make more accurate inferences with no added instructions.</p>"},{"location":"lessons/generative-ai-for-beginners/04-prompt-engineering-fundamentals/#prompt-cues","title":"Prompt Cues","text":"<p>Another technique for using primary content is to provide cues rather than examples. In this case, we are giving the model a nudge in the right direction by starting it off with a snippet that reflects the desired response format. The model then \"takes the cue\" to continue in that vein.</p> Number of Cues Prompt (Input) Completion (Output) 0 Jupiter is the fifth planet from the Sun and the largest in the Solar System. It is a gas giant with a mass one-thousandth that of the Sun, but two-and-a-half times that of all the other planets in the Solar System combined. Jupiter is one of the brightest objects visible to the naked eye in the night sky, and has been known to ancient civilizations since before recorded history. Summarize This Jupiter is the largest planet in our Solar System and the fifth one from the Sun. It is a gas giant with a mass 1/1000th of the Sun's, but it is heavier than all the other planets combined. Ancient civilizations have known about Jupiter for a long time, and it is easily visible in the night sky.. 1 Jupiter is the fifth planet from the Sun and the largest in the Solar System. It is a gas giant with a mass one-thousandth that of the Sun, but two-and-a-half times that of all the other planets in the Solar System combined. Jupiter is one of the brightest objects visible to the naked eye in the night sky, and has been known to ancient civilizations since before recorded history. Summarize This  What we learned is that Jupiter is the fifth planet from the Sun and the largest in the Solar System. It is a gas giant with a mass one-thousandth that of the Sun, but two-and-a-half times that of all the other planets combined. It is easily visible to the naked eye and has been known since ancient times. 2 Jupiter is the fifth planet from the Sun and the largest in the Solar System. It is a gas giant with a mass one-thousandth that of the Sun, but two-and-a-half times that of all the other planets in the Solar System combined. Jupiter is one of the brightest objects visible to the naked eye in the night sky, and has been known to ancient civilizations since before recorded history. Summarize This  Top 3 Facts We Learned: 1. Jupiter is the fifth planet from the Sun and the largest in the Solar System.  2. It is a gas giant with a mass one-thousandth that of the Sun... 3. Jupiter has been visible to the naked eye since ancient times ..."},{"location":"lessons/generative-ai-for-beginners/04-prompt-engineering-fundamentals/#prompt-templates","title":"Prompt Templates","text":"<p>A prompt template is a pre-defined recipe for a prompt that can be stored and reused as needed, to drive more consistent user experiences at scale. In its simplest form, it is simply a collection of prompt examples like this one from OpenAI that provides both the interactive prompt components (user and system messages) and the API-driven request format - to support reuse.</p> <p>In it's more complex form like this example from LangChain it contains placeholders that can be replaced with data from a variety of sources (user input, system context, external data sources etc.) to generate a prompt dynamically. This allows us to create a library of reusable prompts that can be used to drive consistent user experiences programmatically at scale.</p> <p>Finally, the real value of templates lies in the ability to create and publish prompt libraries for vertical application domains - where the prompt template is now optimized to reflect application-specific context or examples that make the responses more relevant and accurate for the targeted user audience. The Prompts For Edu repository is a great example of this approach, curating a library of prompts for the education domain with emphasis on key objectives like lesson planning, curriculum design, student tutoring etc.</p>"},{"location":"lessons/generative-ai-for-beginners/04-prompt-engineering-fundamentals/#supporting-content","title":"Supporting Content","text":"<p>If we think about prompt construction as having a instruction (task) and a target (primary content), then secondary content is like additional context we provide to influence the output in some way. It could be tuning parameters, formatting instructions, topic taxonomies etc. that can help the model tailor its response to be suit the desired user objectives or expectations.</p> <p>For example: Given a course catalog with extensive metadata (name, description, level, metadata tags, instructor etc.) on all the available courses in the curriculum:</p> <ul> <li>we can define an instruction to \"summarize the course catalog for Fall 2023\"</li> <li>we can use the primary content to provide a few examples of the desired output</li> <li>we can use the secondary content to identify the top 5 \"tags\" of interest.</li> </ul> <p>Now, the model can provide a summary in the format shown by the few examples - but if a result has multiple tags, it can prioritize the 5 tags identified in secondary content.</p>"},{"location":"lessons/generative-ai-for-beginners/04-prompt-engineering-fundamentals/#prompting-best-practices","title":"Prompting Best Practices","text":"<p>Now that we know how prompts can be constructed, we can start thinking about how to design them to reflect best practices. We can think about this in two parts - having the right mindset and applying the right techniques.</p>"},{"location":"lessons/generative-ai-for-beginners/04-prompt-engineering-fundamentals/#prompt-engineering-mindset","title":"Prompt Engineering Mindset","text":"<p>Prompt Engineering is a trial-and-error process so keep three broad guiding factors in mind:</p> <ol> <li> <p>Domain Understanding Matters. Response accuracy and relevance is a function of the domain in which that application or user operates. Apply your intuition and domain expertise to customize techniques further. For instance, define domain-specific personalities in your system prompts, or use domain-specific templates in your user prompts. Provide secondary content that reflects domain-specific contexts, or use domain-specific cues and examples to guide the model towards familiar usage patterns.</p> </li> <li> <p>Model Understanding Matters. We know models are stochastic by nature. But model implementations can also vary in terms of the training dataset they use (pre-trained knowledge), the capabilities they provide (e.g., via API or SDK) and the type of content they are optimized for (e.g, code vs. images vs. text). Understand the strengths and limitations of the model you are using, and use that knowledge to prioritize tasks or build customized templates that are optimized for the model's capabilities.</p> </li> <li> <p>Iteration &amp; Validation Matters. Models are evolving rapidly, and so are the techniques for prompt engineering. As a domain expert, you may have other context or criteria your specific application, that may not apply to the broader community. Use prompt engineering tools &amp; techniques to \"jump start\" prompt construction, then iterate and validate the results using your own intuition and domain expertise. Record your insights and create a knowledge base (e.g, prompt libraries) that can be used as a new baseline by others, for faster iterations in the future.</p> </li> </ol>"},{"location":"lessons/generative-ai-for-beginners/04-prompt-engineering-fundamentals/#best-practices","title":"Best Practices","text":"<p>Now let's look at common best practices that are recommended by OpenAI and Azure OpenAI practitioners.</p> What Why Evaluate the latest models. New model generations are likely to have improved features and quality - but may also incur higher costs. Evaluate them for impact, then make migration decisions. Separate instructions &amp; context Check if your model/provider defines delimiters to distinguish instructions, primary and secondary content more clearly. This can help models assign weights more accurately to tokens. Be specific and clear Give more details about the desired context, outcome, length, format, style etc. This will improve both the quality and consistency of responses. Capture recipes in reusable templates. Be descriptive, use examples Models may respond better to a \"show and tell\" approach. Start with a <code>zero-shot</code> approach where you give it an instruction (but no examples) then try <code>few-shot</code> as a refinement, providing a few examples of the desired output. Use analogies. Use cues to jumpstart completions Nudge it towards a desired outcome by giving it some leading words or phrases that it can use as a starting point for the response. Double Down Sometimes you may need to repeat yourself to the model. Give instructions before and after your primary content, use an instruction and a cue, etc. Iterate &amp; validate to see what works. Order Matters The order in which you present information to the model may impact the output, even in the learning examples, thanks to recency bias. Try different options to see what works best. Give the model an \u201cout\u201d Give the model a fallback completion response it can provide if it cannot complete the task for any reason. This can reduce chances of models generating false or fabricated responses. <p>As with any best practice, remember that your mileage may vary based on the model, the task and the domain. Use these as a starting point, and iterate to find what works best for you. Constantly re-evaluate your prompt engineering process as new models and tools become available, with a focus on process scalability and response quality.</p>"},{"location":"lessons/generative-ai-for-beginners/04-prompt-engineering-fundamentals/#assignment","title":"Assignment","text":"<p>Congratulations! You made it to the end of the lesson! It's time to put some of those concepts and techniques to the test with real examples!</p> <p>For our assignment, we'll be using a Jupyter Notebook with exercises you can complete interactively. You can also extend the Notebook with your own Markdown and Code cells to explore ideas and techniques on your own.</p>"},{"location":"lessons/generative-ai-for-beginners/04-prompt-engineering-fundamentals/#to-get-started-fork-the-repo-then","title":"To get started, fork the repo, then","text":"<ul> <li>(Recommended) Launch GitHub Codespaces</li> <li>(Alternatively) Clone the repo to your local device and use it with Docker Desktop</li> <li>(Alternatively) Open the Notebook with your preferred Notebook runtime environment.</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/04-prompt-engineering-fundamentals/#next-configure-your-environment-variables","title":"Next, configure your environment variables","text":"<ul> <li>Copy the <code>.env.copy</code> file in repo root to <code>.env</code> and fill in the <code>AZURE_OPENAI_API_KEY</code>, <code>AZURE_OPENAI_ENDPOINT</code> and <code>AZURE_OPENAI_DEPLOYMENT</code> values. Come back to Learning Sandbox section to learn how.</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/04-prompt-engineering-fundamentals/#next-open-the-jupyter-notebook","title":"Next, open the Jupyter Notebook","text":"<ul> <li>Select the runtime kernel. If using options 1 or 2, simply select the default Python 3.10.x kernel provided by the dev container.</li> </ul> <p>You're all set to run the exercises. Note that there are no right and wrong answers here - just exploring options by trial-and-error and building intuition for what works for a given model and application domain.</p> <p>For this reason there are no Code Solution segments in this lesson. Instead, the Notebook will have Markdown cells titled \"My Solution:\" that shows one example output for reference.</p>"},{"location":"lessons/generative-ai-for-beginners/04-prompt-engineering-fundamentals/#knowledge-check","title":"Knowledge check","text":"<p>Which of the following is a good prompt following some reasonable best practices?</p> <ol> <li>Show me an image of red car</li> <li>Show me an image of red car of make Volvo and model XC90 parked by a cliff with the sun setting</li> <li>Show me an image of red car of make Volvo and model XC90</li> </ol> <p>A: 2, it's the best prompt as it provides details on \"what\" and goes into specifics (not just any car but a specific make and model) and it also describes the overall setting. 3 is next best as it also contains a lot of description.</p>"},{"location":"lessons/generative-ai-for-beginners/04-prompt-engineering-fundamentals/#challenge","title":"\ud83d\ude80 Challenge","text":"<p>See if you can leverage the \"cue\" technique with the prompt: Complete the sentence \"Show me an image of red car of make Volvo and \". What does it respond with, and how would you improve it?</p>"},{"location":"lessons/generative-ai-for-beginners/04-prompt-engineering-fundamentals/#great-work-continue-your-learning","title":"Great Work! Continue Your Learning","text":"<p>Want to learn more about different Prompt Engineering concepts? Go to the continued learning page to find other great resources on this topic.</p> <p>Head over to Lesson 5 where we will look at advanced prompting techniques!</p>"},{"location":"lessons/generative-ai-for-beginners/05-advanced-prompts/","title":"Lesson 5","text":""},{"location":"lessons/generative-ai-for-beginners/05-advanced-prompts/#creating-advanced-prompts","title":"Creating Advanced prompts","text":"<p>Let's recap some learnings from the previous chapter:</p> <p>Prompt engineering is the process by which we guide the model towards more relevant responses by providing more useful instructions or context.</p> <p>There are also two steps to writing prompts, constructing the prompt, by providing relevant context and the second part is optimization, how to gradually improve the prompt.</p> <p>At this point, we have some basic understanding of how to write prompts, but we need to go deeper. In this chapter, you will go from trying out various prompts to understanding why one prompt is better than another. You will learn how to construct prompts following some basic techniques that can be applied to any LLM.</p>"},{"location":"lessons/generative-ai-for-beginners/05-advanced-prompts/#introduction","title":"Introduction","text":"<p>In this chapter, we will cover the following topics:</p> <ul> <li>Extend your knowledge of prompt engineering by applying different techniques to your prompts.</li> <li>Configuring your prompts to vary the output.</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/05-advanced-prompts/#learning-goals","title":"Learning goals","text":"<p>After completing this lesson, you'll be able to:</p> <ul> <li>Apply prompt engineering techniques that improve the outcome of your prompts.</li> <li>Perform prompting that is either varied or deterministic.</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/05-advanced-prompts/#prompt-engineering","title":"Prompt engineering","text":"<p>Prompt engineering is the process of creating prompts that will produce the desired outcome. There's more to prompt engineering than just writing a text prompt. Prompt engineering is not an engineering discipline, it's more a set of techniques that you can apply to get the desired outcome.</p>"},{"location":"lessons/generative-ai-for-beginners/05-advanced-prompts/#an-example-of-a-prompt","title":"An example of a prompt","text":"<p>Let's take a basic prompt like this one:</p> <p>Generate 10 questions on geography.</p> <p>In this prompt, you are actually applying a set of different prompt techniques.</p> <p>Let's break this down.</p> <ul> <li>Context, you specify it should be about \"geography\".</li> <li>Limiting the output, you want no more than 10 questions.</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/05-advanced-prompts/#limitations-of-simple-prompting","title":"Limitations of simple prompting","text":"<p>You may or may not get the desired outcome. You will get your questions generated, but geography is a big topic and you may not get what you want to due the following reasons:</p> <ul> <li>Big topic, you don't know if it's going to be about countries, capitals, rivers and so on.</li> <li>Format, what if you wanted the questions to be formatted in a certain way?</li> </ul> <p>As you can see, there's a lot to consider when creating prompts.</p> <p>So far, we've seen a simple prompt example, but generative AI is capable of much more to help people in a variety of roles and industries. Let's explore some basic techniques next.</p>"},{"location":"lessons/generative-ai-for-beginners/05-advanced-prompts/#techniques-for-prompting","title":"Techniques for prompting","text":"<p>First, we need to understand that prompting is an emergent property of an LLM meaning that this is not a feature that is built into the model but rather something we discover as we use the model.</p> <p>There are some basic techniques that we can use to prompt an LLM. Let's explore them.</p> <ul> <li>Zero-shot prompting, this is the most basic form of prompting. It's a single prompt requesting a response from the LLM based solely on its training data.</li> <li>Few-shot prompting, this type of prompting guides the LLM by providing 1 or more examples it can rely on to generate its response.</li> <li>Chain-of-thought, this type of prompting tells the LLM how to break down a problem into steps.</li> <li>Generated knowledge, to improve the response of a prompt, you can provide generated facts or knowledge additionally to your prompt.</li> <li>Least to most, like chain-of-thought, this technique is about breaking down a problem into a series of steps and then ask these steps to be performed in order.</li> <li>Self-refine, this technique is about critiquing the LLM's output and then asking it to improve.</li> <li>Maieutic prompting. What you want here is to ensure the LLM answer is correct and you ask it to explain various parts of the answer. This is a form of self-refine.</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/05-advanced-prompts/#zero-shot-prompting","title":"Zero-shot prompting","text":"<p>This style of prompting is very simple, it consists of a single prompt. This technique is probably what you're using as you're starting to learn about LLMs. Here's an example:</p> <ul> <li>Prompt: \"What is Algebra?\"</li> <li>Answer: \"Algebra is a branch of mathematics that studies mathematical symbols and the rules for manipulating these symbols.\"</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/05-advanced-prompts/#few-shot-prompting","title":"Few-shot prompting","text":"<p>This style of prompting helps the model by providing a few examples along with the request. It consists of a single prompt with additional task-specific data. Here's an example:</p> <ul> <li>Prompt: \"Write a poem in the style of Shakespeare. Here are a few examples of Shakespearean sonnets.:   Sonnet 18: 'Shall I compare thee to a summer's day? Thou art more lovely and more temperate...'   Sonnet 116: 'Let me not to the marriage of true minds Admit impediments. Love is not love Which alters when it alteration finds...'   Sonnet 132: 'Thine eyes I love, and they, as pitying me, Knowing thy heart torment me with disdain,...'   Now, write a sonnet about the beauty of the moon.\"</li> <li>Answer: \"Upon the sky, the moon doth softly gleam, In silv'ry light that casts its gentle grace,...\"</li> </ul> <p>Examples provide the LLM with the context, format or style of the desired output. They help the model understand the specific task and generate more accurate and relevant responses.</p>"},{"location":"lessons/generative-ai-for-beginners/05-advanced-prompts/#chain-of-thought","title":"Chain-of-thought","text":"<p>Chain-of-thought is a very interesting technique as it's about taking the LLM through a series of steps. The idea is to instruct the LLM in such a way that it understands how to do something. Consider the following example, with and without chain-of-thought:</p> <pre><code>- Prompt: \"Alice has 5 apples, throws 3 apples, gives 2 to Bob and Bob gives one back, how many apples does Alice have?\"\n- Answer: 5\n</code></pre> <p>LLM answers with 5, which is incorrect. Correct answer is 1 apple, given the calculation (5 -3 -2 + 1 = 1).</p> <p>So how can we teach the LLM to do this correctly?</p> <p>Let's try chain-of-thought. Applying chain-of-thought means:</p> <ol> <li>Give the LLM a similar example.</li> <li>Show the calculation, and how to calculate it correctly.</li> <li>Provide the original prompt.</li> </ol> <p>Here's how:</p> <ul> <li>Prompt: \"Lisa has 7 apples, throws 1 apple, gives 4 apples to Bart and Bart gives one back:   7 -1 = 6   6 -4 = 2   2 +1 = 3   Alice has 5 apples, throws 3 apples, gives 2 to Bob and Bob gives one back, how many apples does Alice have?\"   Answer: 1</li> </ul> <p>Note how we write substantially longer prompts with another example, a calculation and then the original prompt and we arrive at the correct answer 1.</p> <p>As you can see chain-of-thought is a very powerful technique.</p>"},{"location":"lessons/generative-ai-for-beginners/05-advanced-prompts/#generated-knowledge","title":"Generated knowledge","text":"<p>Many times when you want to construct a prompt, you want to do so using your own company's data. You want part of the prompt to be from the company and the other part should be the actual prompt you're interested in.</p> <p>As an example, this is what your prompt then can look like if you're in the insurance business:</p> <pre><code>{{company}}: {{company_name}}\n{{products}}:\n{{products_list}}\nPlease suggest an insurance given the following budget and requirements:\nBudget: {{budget}}\nRequirements: {{requirements}}\n</code></pre> <p>Above, you see how the prompt is constructed using a template. In the template there's a number of variables, denoted by <code>{{variable}}</code>, that will be replaced with actual values from a company API.</p> <p>Here's an example of how the prompt could look like once the variables have been replaced by content from your company:</p> <pre><code>Insurance company: ACME Insurance\nInsurance products (cost per month):\n- Car, cheap, 500 USD\n- Car, expensive, 1100 USD\n- Home, cheap, 600 USD\n- Home, expensive, 1200 USD\n- Life, cheap, 100 USD\n\nPlease suggest an insurance given the following budget and requirements:\nBudget: $1000\nRequirements: Car, Home, and Life insurance\n</code></pre> <p>Running this prompt through an LLM will produce a response like this:</p> <pre><code>Given the budget and requirements, we suggest the following insurance package from ACME Insurance:\n- Car, cheap, 500 USD\n- Home, cheap, 600 USD\n- Life, cheap, 100 USD\nTotal cost: $1,200 USD\n</code></pre> <p>As you can see, it also suggests the Life insurance, which it shouldn't. This result is an indication that we need to optimize the prompt by changing the prompt to be clearer on what it can allow. After some trial and error, we arrive at the following prompt:</p> <pre><code>Insurance company: ACME Insurance\nInsurance products (cost per month):\n- type: Car, cheap, cost: 500 USD\n- type: Car, expensive, cost: 1100 USD\n- type: Home, cheap, cost: 600 USD\n- type: Home, expensive, cost: 1200 USD\n- type: Life, cheap, cost: 100 USD\n\nPlease suggest an insurance given the following budget and requirements:\nBudget: $1000 restrict choice to types: Car, Home\n</code></pre> <p>Note how adding type and cost and also using the keyword restrict helps the LLM to understand what we want.</p> <p>Now we get the following response:</p> <pre><code>Given the budget and requirements, we suggest the Car, Cheap insurance product which costs 500 USD per month.\n</code></pre> <p>The point of this example was to show that even though we're using a basic technique like generated knowledge, we still need to optimize the prompt in most cases to get the desired outcome.</p>"},{"location":"lessons/generative-ai-for-beginners/05-advanced-prompts/#least-to-most","title":"Least-to-most","text":"<p>The idea with Least-to-most prompting is to break down a bigger problem into subproblems. That way, you help guide the LLM on how to \"conquer\" the bigger problem. A good example could be for data science where you can ask the LLM to divide up a problem like so:</p> <p>Prompt: How to perform data science in 5 steps?</p> <p>With your AI assistant answering with:</p> <ol> <li>Collect data</li> <li>Clean data</li> <li>Analyze data</li> <li>Plot data</li> <li>Present data</li> </ol>"},{"location":"lessons/generative-ai-for-beginners/05-advanced-prompts/#self-refine-critique-the-results","title":"Self-refine, critique the results","text":"<p>With generative AIs and LLMs, you can't trust the output. You need to verify it. After all, the LLM is just presenting you what's the next most likely thing to say, not what's correct. Therefore, a good idea is to ask the LLM to critique itself, which leads us to the self-refine technique.</p> <p>How it works is that you follow the following steps:</p> <ol> <li>Initial prompt asking the LLM to solve a problem</li> <li>LLM answers</li> <li>You critique the answer and ask the AI to improve</li> <li>LLM answers again, this time considering the critique and suggest solutions it came up with</li> </ol> <p>You can repeat this process as many times as you want.</p> <p>Here's an example using this technique:</p> <p>Prompt: \"Create a Python Web API with routes products and customers\"</p> <p>AI Answer:</p> <pre><code>import flask\napp = Flask(__name__)\n\n@app.route('/products')\ndef products():\n    return 'Products'\n\n@app.route('/customers')\ndef customers():\n    return 'Customers'\n</code></pre> <p>Prompt: suggest 3 improvements of the above code</p> <p>AI Answer:</p> <pre><code>Sure, here are three suggestions to improve the code:\n\nImport the correct module: The first line should import Flask instead of flask. The correct import statement should be from flask import Flask.\n\nAdd a main function: Add a if __name__ == '__main__': block to the end of the file to run the application when the file is executed.\n\nUse f-strings: Use f-strings to format the return statements in the route functions. For example, return 'Products' can be changed to return f'This is the {request.path} page'.\n</code></pre> <pre><code>from flask import Flask, request\n\napp = Flask(__name__)\n\n@app.route('/products')\ndef products():\n    return f'This is the {request.path} page'\n\n@app.route('/customers')\ndef customers():\n    return f'This is the {request.path} page'\n\nif __name__ == '__main__':\n    app.run()\n</code></pre> <p>As you can see, the above AI response is improving the first suggested code thanks to critiquing the first answer.</p>"},{"location":"lessons/generative-ai-for-beginners/05-advanced-prompts/#maieutic-prompting","title":"Maieutic prompting","text":"<p>Maieutic prompting is a technique that is similar to self-refine but it's more about asking the LLM to explain itself. The goal is to reduce inconsistencies in the LLM's output so to ensure it arrives at the correct answer. The workflow to follow is:</p> <ol> <li>Ask the LLM to answer a question</li> <li>For each part of the answer, ask the LLM to explain it more in depth.</li> <li>If there are inconsistencies, discard the parts that are inconsistent.</li> </ol> <p>Repeat 2 and 3 until you've gone through all the parts and you're satisfied with the answer.</p> <p>Here's an example prompt:</p> <p>prompt: How can I create a crisis plan to mitigate a pandemic in 5 steps? LLM answer:</p> <pre><code>1. Identify the risks\n1. Identify the stakeholders\n1. Identify the resources\n1. Identify the actions\n1. Identify the communication channels\n</code></pre> <p>It's identified 5 steps, but can we determine if this is correct? Let's ask the LLM to explain each step:</p> <p>prompt: Explain the first step in more detail, what are the risks in detail with a pandemic?</p> <pre><code>The risks are the following:\n\n- Loss of life\n- Loss of business\n- Loss of reputation\n- Loss of customers\n- Loss of employees\n</code></pre> <p>At this point, we want to make sure the LLM is correct so we ask it to explain the first risk in more detail and hope it's consistent with the answer above:</p> <p>prompt: In a Pandemic, which is the biggest risk and why?</p> <pre><code>The biggest risk is loss of life because it's the most important thing to protect.\n</code></pre> <p>What are the two biggest risks in a pandemic?</p> <pre><code>The two biggest risks are loss of life and loss of business.\n</code></pre> <p>At this point, the LLM is consistent and mentions \"life\" and \"business\" as the top two risks. We can now continue to the next step and feel fairly confident. However, we shouldn't trust the LLM blindly, we should always verify the output.</p>"},{"location":"lessons/generative-ai-for-beginners/05-advanced-prompts/#vary-your-output","title":"Vary your output","text":"<p>LLMs are nondeterministic by nature, meaning that you will get different results each time you run the same prompt. Try the following prompt for example:</p> <p>\"Generate code for a Python Web API\"</p> <pre><code># Import necessary modules\nimport flask\nfrom flask import request, jsonify\n\n# Create a Flask app\napp = flask.Flask(__name__)\n\n# Create a route for the API\n@app.route('/api', methods=['GET'])\ndef api():\n    # Get the data from the request\n    data = request.args\n\n    # Process the data\n    result = process_data(data)\n\n    # Return the result as JSON\n    return jsonify(result)\n\n# Function to process the data\ndef process_data(data):\n    # Do something with the data\n    result = data + 1\n\n    # Return the result\n    return result\n\n# Run the app\nif __name__ == '__main__':\n    app.run()\n</code></pre> <p>Running the same prompt again generates a slightly different response:</p> <pre><code>#import necessary packages\nimport flask\nfrom flask import request, jsonify\n\n#create the Flask application\napp = flask.Flask(__name__)\napp.config[\"DEBUG\"] = True\n\n#create a list of books\nbooks = [\n    {'id': 0,\n     'title': 'A Fire Upon the Deep',\n     'author': 'Vernor Vinge',\n     'first_sentence': 'The coldsleep itself was dreamless.',\n     'year_published': '1992'},\n    {'id': 1,\n     'title': 'The Ones Who Walk Away From Omelas',\n     'author': 'Ursula K. Le Guin',\n     'first_sentence': 'With a clamor of bells that set the swallows soaring, the Festival of Summer came to the city Omelas, bright-towered by the sea.',\n     'published': '1973'},\n    {'id': 2,\n     'title': 'Dhalgren',\n     'author': 'Samuel R. Delany',\n     'first_sentence': 'to wound the autumnal city.',\n     'published': '1975'}\n]\n\n#create an endpoint for the API\n@app.route('/', methods=['GET'])\ndef home():\n    return '''&lt;h1&gt;Books API&lt;/h1&gt;\n&lt;p&gt;A prototype API for retrieving books.&lt;/p&gt;'''\n\n#create an endpoint to return all books\n@app.route('/api/v1/resources/books/all', methods=['GET'])\ndef api_all():\n    return jsonify(books)\n\n#create an endpoint to return a single book\n@app.route('/api/v1/resources/books', methods=['GET'])\ndef api_id():\n    #check if an ID was provided as part of the URL\n    #if ID is provided, assign it to a variable\n    #if no ID is provided, display an error\n    if 'id' in request.args:\n        id = int(request.args['id'])\n    else:\n        return \"Error: No id field provided. Please specify an id.\"\n\n    #create an empty list for our results\n    results = []\n\n    #loop through the data and match results that fit the requested ID\n    #IDs are unique, but other fields might return many results\n    for book in books:\n        if book['id'] == id:\n            results.append(book)\n\n    #use the jsonify function from Flask to convert our list of\n    #Python dictionaries to the JSON format\n    return jsonify(results)\n\napp.run()\n</code></pre> <p>So is the varied output a problem?</p> <p>Depends on what you're trying to do. If you want a specific response then it's a problem. If you're ok with a varied output like \"Generate any 3 questions on geography\", then it's not a problem.</p>"},{"location":"lessons/generative-ai-for-beginners/05-advanced-prompts/#using-temperature-to-vary-your-output","title":"Using temperature to vary your output","text":"<p>Ok, so we've decided we want to limit the output to be more predictable, that is more deterministic. How do we do that?</p> <p>Temperature is a value between 0 and 1, where 0 is the most deterministic and 1 is the most varied. The default value is 0.7. Let's see what happens with two runs of the same prompt with temperature set to 0.1:</p> <p>\"Generate code for a Python Web API\"</p> <pre><code>#import necessary libraries\nimport flask\nfrom flask import request, jsonify\n\n#create an instance of the Flask class\napp = flask.Flask(__name__)\n\n#create an endpoint for the API\n@app.route('/api/v1/endpoint', methods=['GET'])\ndef api_endpoint():\n    #get the request data\n    data = request.get_json()\n\n    #process the data\n    result = process_data(data)\n\n    #return the result\n    return jsonify(result)\n\n#function to process the data\ndef process_data(data):\n    #process the data\n    result = {'result': 'success'}\n\n    #return the result\n    return result\n\n#run the Flask app\nif __name__ == '__main__':\n    app.run()\n</code></pre> <p>Running the prompt again gives us this outcome:</p> <pre><code>#import necessary libraries\nimport flask\nfrom flask import request, jsonify\n\n#create a Flask app\napp = flask.Flask(__name__)\n\n#create a route for the API\n@app.route('/api', methods=['GET'])\ndef api_call():\n    #get the data from the request\n    data = request.get_json()\n\n    #process the data\n    result = process_data(data)\n\n    #return the result\n    return jsonify(result)\n\n#define the process_data function\ndef process_data(data):\n    #do something with the data\n    result = data + 1\n\n    #return the result\n    return result\n\n#run the app\nif __name__ == '__main__':\n    app.run()\n</code></pre> <p>There's only a tiny difference between these two outputs. Let's do the opposite this time, let's set the temperature to 0.9:</p> <pre><code># Import necessary libraries\nimport flask\nfrom flask import request, jsonify\n\n# Create a Flask app\napp = flask.Flask(__name__)\n\n# Create a route for the API\n@app.route('/api', methods=['GET'])\ndef api_call():\n    # Get the data from the request\n    data = request.args\n\n    # Process the data\n    result = process_data(data)\n\n    # Return the result\n    return jsonify(result)\n\n# Function to process the data\ndef process_data(data):\n    # Do something with the data\n    result = data + 1\n\n    # Return the result\n    return result\n\n# Run the app\nif __name__ == '__main__':\n    app.run()\n</code></pre> <p>and the second attempt at 0.9 as temperature value:</p> <pre><code>import flask\nfrom flask import request, jsonify\n\n# create the Flask app\napp = flask.Flask(__name__)\napp.config['DEBUG'] = True\n\n# create some test data\nbooks = [\n    {'id': 0, 'title': 'A Fire Upon The Deep', 'author': 'Vernor Vinge', 'first_sentence': 'The coldsleep itself was dreamless.', 'year_published': '1992'},\n    {'id': 1, 'title': 'The Ones Who Walk Away From Omelas', 'author': 'Ursula K. Le Guin', 'first_sentence': 'With a clamor of bells that set the swallows soaring, the Festival of Summer came to the city Omelas, bright-towered by the sea.', 'published': '1973'},\n    {'id': 2, 'title': 'Dhalgren', 'author': 'Samuel R. Delany', 'first_sentence': 'to wound the autumnal city.', 'published': '1975'}\n]\n\n# create an endpoint\n@app.route('/', methods=['GET'])\ndef home():\n    return '''&lt;h1&gt;Welcome to our book API!&lt;/h1&gt;'''\n\n@app.route('/api/v1/resources/books\n</code></pre> <p>As you can see, the results couldn't be more varied.</p> <p>Note, that there are more parameters you can change to vary the output, like top-k, top-p, repetition penalty, length penalty and diversity penalty but these are outside the scope of this curriculum.</p>"},{"location":"lessons/generative-ai-for-beginners/05-advanced-prompts/#good-practices","title":"Good practices","text":"<p>There are many practices you can apply to try to get what you want. You will find your own style as you use prompting more and more.</p> <p>Additionally to the techniques we've covered, there are some good practices to consider when prompting an LLM.</p> <p>Here are some good practices to consider:</p> <ul> <li>Specify context. Context matters, the more you can specify like domain, topic, etc. the better.</li> <li>Limit the output. If you want a specific number of items or a specific length, specify it.</li> <li>Specify both what and how. Remember to mention both what you want and how you want it, for example \"Create a Python Web API with routes products and customers, divide it into 3 files\".</li> <li>Use templates. Often, you will want to enrich your prompts with data from your company. Use templates to do this. Templates can have variables that you replace with actual data.</li> <li>Spell correctly. LLMs might provide you with a correct response, but if you spell correctly, you will get a better response.</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/05-advanced-prompts/#assignment","title":"Assignment","text":"<p>Here's code in Python showing how to build a simple API using Flask:</p> <pre><code>from flask import Flask, request\n\napp = Flask(__name__)\n\n@app.route('/')\ndef hello():\n    name = request.args.get('name', 'World')\n    return f'Hello, {name}!'\n\nif __name__ == '__main__':\n    app.run()\n</code></pre> <p>Use an AI assistant like GitHub Copilot or ChatGPT and apply the \"self-refine\" technique to improve the code.</p>"},{"location":"lessons/generative-ai-for-beginners/05-advanced-prompts/#solution","title":"Solution","text":"<p>Please attempt to solve the assignment by adding suitable prompts to the code.</p> <p>[!TIP] Phrase a prompt to ask it to improve, it's a good idea to limit how many improvements. You can also ask to improve it in a certain way, for example architecture, performance, security, etc.</p> <p>Solution</p>"},{"location":"lessons/generative-ai-for-beginners/05-advanced-prompts/#knowledge-check","title":"Knowledge check","text":"<p>Why would I use chain-of-thought prompting? Show me 1 correct response and 2 incorrect responses.</p> <ol> <li>To teach the LLM how to solve a problem.</li> <li>B, To teach the LLM to find errors in code.</li> <li>C, To instruct the LLM to come up with different solutions.</li> </ol> <p>A: 1, because chain-of-thought is about showing the LLM how to solve a problem by providing it with a series of steps, and similar problems and how they were solved.</p>"},{"location":"lessons/generative-ai-for-beginners/05-advanced-prompts/#challenge","title":"\ud83d\ude80 Challenge","text":"<p>You just used the self-refine technique in the assignment. Take any program you built and consider what improvements you would want to apply to it. Now use the self-refine technique to apply the proposed changes. What did you think the result, better or worse?</p>"},{"location":"lessons/generative-ai-for-beginners/05-advanced-prompts/#great-work-continue-your-learning","title":"Great Work! Continue Your Learning","text":"<p>After completing this lesson, check out our Generative AI Learning collection to continue leveling up your Generative AI knowledge!</p> <p>Head over to Lesson 6 where we will apply our knowledge of Prompt Engineering by building text generation apps</p>"},{"location":"lessons/generative-ai-for-beginners/06-text-generation-apps/","title":"Lesson 6","text":""},{"location":"lessons/generative-ai-for-beginners/06-text-generation-apps/#building-text-generation-applications","title":"Building Text Generation Applications","text":"<p>(Click the image above to view video of this lesson)</p> <p>You've seen so far through this curriculum that there are core concepts like prompts and even a whole discipline called \"prompt engineering\". Many tools you can interact with like ChatGPT, Office 365, Microsoft Power Platform and more, support you using prompts to accomplish something.</p> <p>For you to add such an experience to an app, you need to understand concepts like prompts, completions and choose a library to work with. That's exactly what you'll learn in this chapter.</p>"},{"location":"lessons/generative-ai-for-beginners/06-text-generation-apps/#introduction","title":"Introduction","text":"<p>In this chapter, you will:</p> <ul> <li>Learn about the openai library and it's core concepts.</li> <li>Build a text generation app using openai.</li> <li>Understand how to use concepts like prompt, temperature, and tokens to build a text generation app.</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/06-text-generation-apps/#learning-goals","title":"Learning goals","text":"<p>At the end of this lesson, you'll be able to:</p> <ul> <li>Explain what a text generation app is.</li> <li>Build a text generation app using openai.</li> <li>Configure your app to use more or less tokens and also change the temperature, for a varied output.</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/06-text-generation-apps/#what-is-a-text-generation-app","title":"What is a text generation app?","text":"<p>Normally when you build an app it has some kind of interface like the following:</p> <ul> <li>Command-based. Console apps are typical apps where you type a command and it carries out a task. For example, <code>git</code> is a command-based app.</li> <li>User interface (UI). Some apps have graphical user interfaces (GUIs) where you click buttons, input text, select options and more.</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/06-text-generation-apps/#console-and-ui-apps-are-limited","title":"Console and UI apps are limited","text":"<p>Compare it to a command-based app where you type a command:</p> <ul> <li>It's limited. You can't just type any command, only the ones that the app supports.</li> <li>Language specific. Some apps support many languages, but by default the app is built for a specific language, even if you can add more language support.</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/06-text-generation-apps/#benefits-of-text-generation-apps","title":"Benefits of text generation apps","text":"<p>So how is a text generation app different?</p> <p>In a text generation app, you have more flexibility, you're not limited to a set of commands or a specific input language. Instead, you can use natural language to interact with the app. Another benefit is that because you're already interacting with a data source that has been trained on a vast corpus of information, whereas a traditional app might be limited on what's in a database.</p>"},{"location":"lessons/generative-ai-for-beginners/06-text-generation-apps/#what-can-i-build-with-a-text-generation-app","title":"What can I build with a text generation app?","text":"<p>There are many things you can build. For example:</p> <ul> <li>A chatbot. A chatbot answering questions about topics, like your company and its products could be a good match.</li> <li>Helper. LLMs are great at things like summarizing text, getting insights from text, producing text like resumes and more.</li> <li>Code assistant. Depending on the language model you use, you can build a code assistant that helps you write code. For example, you can use a product like GitHub Copilot as well as ChatGPT to help you write code.</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/06-text-generation-apps/#how-can-i-get-started","title":"How can I get started?","text":"<p>Well, you need to find a way to integrate with an LLM which usually entails the following two approaches:</p> <ul> <li>Use an API. Here you're constructing web requests with your prompt and get generated text back.</li> <li>Use a library. Libraries help encapsulate the API calls and make them easier to use.</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/06-text-generation-apps/#librariessdks","title":"Libraries/SDKs","text":"<p>There are a few well known libraries for working with LLMs like:</p> <ul> <li>openai, this library makes it easy to connect to your model and send in prompts.</li> </ul> <p>Then there are libraries that operate on a higher level like:</p> <ul> <li>Langchain. Langchain is well known and supports Python.</li> <li>Semantic Kernel. Semantic Kernel is a library by Microsoft supporting the languages C#, Python, and Java.</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/06-text-generation-apps/#first-app-using-openai","title":"First app using openai","text":"<p>Let's see how we can build our first app, what libraries we need, how much is required and so on.</p>"},{"location":"lessons/generative-ai-for-beginners/06-text-generation-apps/#install-openai","title":"Install openai","text":"<p>There are many libraries out there for interacting with OpenAI or Azure OpenAI. It's possible to use numerous programming languages as well like C#, Python, JavaScript, Java and more. We've chosen to use the <code>openai</code> Python library, so we'll use <code>pip</code> to install it.</p> <pre><code>pip install openai\n</code></pre>"},{"location":"lessons/generative-ai-for-beginners/06-text-generation-apps/#create-a-resource","title":"Create a resource","text":"<p>You need to carry out the following steps:</p> <ul> <li>Create an account on Azure https://azure.microsoft.com/free/.</li> <li>Gain access to Azure OpenAI. Go to https://learn.microsoft.com/azure/ai-services/openai/overview#how-do-i-get-access-to-azure-openai and request access.</li> </ul> <p>[!NOTE] At the time of writing, you need to apply for access to Azure OpenAI.</p> <ul> <li>Install Python https://www.python.org/</li> <li>Have created an Azure OpenAI Service resource. See this guide for how to create a resource.</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/06-text-generation-apps/#locate-api-key-and-endpoint","title":"Locate API key and endpoint","text":"<p>At this point, you need to tell your <code>openai</code> library what API key to use. To find your API key, go to \"Keys and Endpoint\" section of your Azure OpenAI resource and copy the \"Key 1\" value.</p> <p></p> <p>Now that you have this information copied, let's instruct the libraries to use it.</p> <p>[!NOTE] It's worth separating your API key from your code. You can do so by using environment variables.</p> <ul> <li>Set the environment variable <code>OPENAI_API_KEY</code> to your API key.   <code>export OPENAI_API_KEY='sk-...'</code></li> </ul>"},{"location":"lessons/generative-ai-for-beginners/06-text-generation-apps/#setup-configuration-azure","title":"Setup configuration Azure","text":"<p>If you're using Azure OpenAI, here's how you setup configuration:</p> <pre><code>openai.api_type = 'azure'\nopenai.api_key = os.environ[\"OPENAI_API_KEY\"]\nopenai.api_version = '2023-05-15'\nopenai.api_base = os.getenv(\"API_BASE\")\n</code></pre> <p>Above we're setting the following:</p> <ul> <li><code>api_type</code> to <code>azure</code>. This tells the library to use Azure OpenAI and not OpenAI.</li> <li><code>api_key</code>, this is your API key found in the Azure Portal.</li> <li><code>api_version</code>, this is the version of the API you want to use. At the time of writing, the latest version is <code>2023-05-15</code>.</li> <li><code>api_base</code>, this is the endpoint of the API. You can find it in the Azure Portal next to your API key.</li> </ul> <p>[!NOTE] &gt; <code>os.getenv</code> is a function that reads environment variables. You can use it to read environment variables like <code>OPENAI_API_KEY</code> and <code>API_BASE</code>. Set these environment variables in your terminal or by using a library like <code>dotenv</code>.</p>"},{"location":"lessons/generative-ai-for-beginners/06-text-generation-apps/#generate-text","title":"Generate text","text":"<p>The way to generate text is to use the <code>Completion</code> class. Here's an example:</p> <pre><code>prompt = \"Complete the following: Once upon a time there was a\"\n\ncompletion = openai.Completion.create(model=\"davinci-002\", prompt=prompt)\nprint(completion.choices[0].text)\n</code></pre> <p>In the above code, we create a completion object and pass in the model we want to use and the prompt. Then we print the generated text.</p>"},{"location":"lessons/generative-ai-for-beginners/06-text-generation-apps/#chat-completions","title":"Chat completions","text":"<p>So far, you've seen how we've been using <code>Completion</code> to generate text. But there's another class called <code>ChatCompletion</code> that is more suited for chatbots. Here's an example of using it:</p> <pre><code>import openai\n\nopenai.api_key = \"sk-...\"\n\ncompletion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\nprint(completion.choices[0].message.content)\n</code></pre> <p>More on this functionality in an upcoming chapter.</p>"},{"location":"lessons/generative-ai-for-beginners/06-text-generation-apps/#exercise-your-first-text-generation-app","title":"Exercise - your first text generation app","text":"<p>Now that we learned how to set up and configure openai, it's time to build your first text generation app. To build your app, follow these steps:</p> <ol> <li>Create a virtual environment and install openai:</li> </ol> <pre><code>python -m venv venv\nsource venv/bin/activate\npip install openai\n</code></pre> <p>[!NOTE] If you're using Windows type <code>venv\\Scripts\\activate</code> instead of <code>source venv/bin/activate</code>.</p> <p>[!NOTE] Locate your Azure OpenAI key by going to https://portal.azure.com/ and search for <code>Open AI</code> and select the <code>Open AI resource</code> and then select <code>Keys and Endpoint</code> and copy the <code>Key 1</code> value.</p> <ol> <li>Create an app.py file and give it the following code:</li> </ol> <pre><code>import openai\n\nopenai.api_key = \"&lt;replace this value with your open ai key or Azure OpenAI key&gt;\"\n\nopenai.api_type = 'azure'\nopenai.api_version = '2023-05-15'\nopenai.api_base = \"&lt;endpoint found in Azure Portal where your API key is&gt;\"\ndeployment_name = \"&lt;deployment name&gt;\"\n\n# add your completion code\nprompt = \"Complete the following: Once upon a time there was a\"\nmessages = [{\"role\": \"user\", \"content\": prompt}]\n\n# make completion\ncompletion = client.chat.completions.create(model=deployment, messages=messages)\n\n# print response\nprint(completion.choices[0].message.content)\n</code></pre> <p>[!NOTE] If you're using Azure OpenAI, you need to set the <code>api_type</code> to <code>azure</code> and set the <code>api_key</code> to your Azure OpenAI key.</p> <p>You should see an output like the following:</p> <pre><code> very unhappy _____.\n\nOnce upon a time there was a very unhappy mermaid.\n</code></pre>"},{"location":"lessons/generative-ai-for-beginners/06-text-generation-apps/#different-types-of-prompts-for-different-things","title":"Different types of prompts, for different things","text":"<p>Now you've seen how to generate text using a prompt. You even have a program up and running that you can modify and change to generate different types of text.</p> <p>Prompts can be used for all sorts of tasks. For example:</p> <ul> <li>Generate a type of text. For example, you can generate a poem, questions for a quiz etc.</li> <li>Lookup information. You can use prompts to look for information like the following example 'What does CORS mean in web development?'.</li> <li>Generate code. You can use prompts to generate code, for example developing a regular expression used to validate emails or why not generate an entire program, like a web app?</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/06-text-generation-apps/#a-more-practical-use-case-a-recipe-generator","title":"A more practical use case: a recipe generator","text":"<p>Imagine you have ingredients at home and you want to cook something. For that, you need a recipe. A way to find recipes is to use a search engine or you could use an LLM to do so.</p> <p>You could write a prompt like so:</p> <p>\"Show me 5 recipes for a dish with the following ingredients: chicken, potatoes, and carrots. Per recipe, list all the ingredients used\"</p> <p>Given the above prompt, you might get a response similar to:</p> <pre><code>1. Roasted Chicken and Vegetables:\nIngredients:\n- 4 chicken thighs\n- 2 potatoes, cut into cubes\n- 2 carrots, cut into cubes\n- 2 tablespoons olive oil\n- 2 cloves garlic, minced\n- 1 teaspoon dried thyme\n- 1 teaspoon dried oregano\n- Salt and pepper, to taste\n\n2. Chicken and Potato Stew:\nIngredients:\n- 2 tablespoons olive oil\n- 1 onion, diced\n- 2 cloves garlic, minced\n- 2 chicken breasts, cut into cubes\n- 2 potatoes, cut into cubes\n- 2 carrots, cut into cubes\n- 1 teaspoon dried oregano\n- 1 teaspoon dried thyme\n- 1 cup chicken broth\n- Salt and pepper, to taste\n\n3. Chicken and Potato Bake:\nIngredients:\n- 2 tablespoons olive oil\n- 2 chicken breasts, cut into cubes\n- 2 potatoes, cut into cubes\n- 2 carrots, cut into cubes\n- 1 onion, diced\n- 2 cloves garlic, minced\n- 1 teaspoon dried oregano\n- 1 teaspoon dried thyme\n- 1 cup chicken broth\n- Salt and pepper, to taste\n\n4. Chicken and Potato Soup:\nIngredients:\n- 2 tablespoons olive oil\n- 1 onion, diced\n- 2 cloves garlic, minced\n- 2 chicken breasts, cut into cubes\n- 2 potatoes, cut into cubes\n- 2 carrots, cut into cubes\n- 1 teaspoon dried oregano\n- 1 teaspoon dried thyme\n- 4 cups chicken broth\n- Salt and pepper, to taste\n\n5. Chicken and Potato Hash:\nIngredients:\n- 2 tablespoons olive oil\n- 2 chicken breasts, cut into cubes\n- 2 potatoes, cut into cubes\n- 2 carrots, cut into cubes\n- 1 onion, diced\n- 2 cloves garlic, minced\n- 1 teaspoon dried oregano\n</code></pre> <p>This outcome is great, I know what to cook. At this point, what could be useful improvements are:</p> <ul> <li>Filtering out ingredients I don't like or am allergic to.</li> <li>Produce a shopping list, in case I don't have all the ingredients at home.</li> </ul> <p>For the above cases, let's add an additional prompt:</p> <p>\"Please remove recipes with garlic as I'm allergic and replace it with something else. Also, please produce a shopping list for the recipes, considering I already have chicken, potatoes and carrots at home.\"</p> <p>Now you have a new result, namely:</p> <pre><code>1. Roasted Chicken and Vegetables:\nIngredients:\n- 4 chicken thighs\n- 2 potatoes, cut into cubes\n- 2 carrots, cut into cubes\n- 2 tablespoons olive oil\n- 1 teaspoon dried thyme\n- 1 teaspoon dried oregano\n- Salt and pepper, to taste\n\n2. Chicken and Potato Stew:\nIngredients:\n- 2 tablespoons olive oil\n- 1 onion, diced\n- 2 chicken breasts, cut into cubes\n- 2 potatoes, cut into cubes\n- 2 carrots, cut into cubes\n- 1 teaspoon dried oregano\n- 1 teaspoon dried thyme\n- 1 cup chicken broth\n- Salt and pepper, to taste\n\n3. Chicken and Potato Bake:\nIngredients:\n- 2 tablespoons olive oil\n- 2 chicken breasts, cut into cubes\n- 2 potatoes, cut into cubes\n- 2 carrots, cut into cubes\n- 1 onion, diced\n- 1 teaspoon dried oregano\n- 1 teaspoon dried thyme\n- 1 cup chicken broth\n- Salt and pepper, to taste\n\n4. Chicken and Potato Soup:\nIngredients:\n- 2 tablespoons olive oil\n- 1 onion, diced\n- 2 chicken breasts, cut into cubes\n- 2 potatoes, cut into cubes\n- 2 carrots, cut into cubes\n- 1 teaspoon dried oregano\n- 1 teaspoon dried thyme\n- 4 cups chicken broth\n- Salt and pepper, to taste\n\n5. Chicken and Potato Hash:\nIngredients:\n- 2 tablespoons olive oil\n- 2 chicken breasts, cut into cubes\n- 2 potatoes, cut into cubes\n- 2 carrots, cut into cubes\n- 1 onion, diced\n- 1 teaspoon dried oregano\n\nShopping List:\n- Olive oil\n- Onion\n- Thyme\n- Oregano\n- Salt\n- Pepper\n</code></pre> <p>That's your five recipes, with no garlic mentioned and you also have a shopping list considering what you already have at home.</p>"},{"location":"lessons/generative-ai-for-beginners/06-text-generation-apps/#exercise-build-a-recipe-generator","title":"Exercise - build a recipe generator","text":"<p>Now that we have played out a scenario, let's write code to match the demonstrated scenario. To do so, follow these steps:</p> <ol> <li>Use the existing app.py file as a starting point</li> <li>Locate the <code>prompt</code> variable and change its code to the following:</li> </ol> <pre><code>prompt = \"Show me 5 recipes for a dish with the following ingredients: chicken, potatoes, and carrots. Per recipe, list all the ingredients used\"\n</code></pre> <p>If you now run the code, you should see an output similar to:</p> <pre><code>-Chicken Stew with Potatoes and Carrots: 3 tablespoons oil, 1 onion, chopped, 2 cloves garlic, minced, 1 carrot, peeled and chopped, 1 potato, peeled and chopped, 1 bay leaf, 1 thyme sprig, 1/2 teaspoon salt, 1/4 teaspoon black pepper, 1 1/2 cups chicken broth, 1/2 cup dry white wine, 2 tablespoons chopped fresh parsley, 2 tablespoons unsalted butter, 1 1/2 pounds boneless, skinless chicken thighs, cut into 1-inch pieces\n-Oven-Roasted Chicken with Potatoes and Carrots: 3 tablespoons extra-virgin olive oil, 1 tablespoon Dijon mustard, 1 tablespoon chopped fresh rosemary, 1 tablespoon chopped fresh thyme, 4 cloves garlic, minced, 1 1/2 pounds small red potatoes, quartered, 1 1/2 pounds carrots, quartered lengthwise, 1/2 teaspoon salt, 1/4 teaspoon black pepper, 1 (4-pound) whole chicken\n-Chicken, Potato, and Carrot Casserole: cooking spray, 1 large onion, chopped, 2 cloves garlic, minced, 1 carrot, peeled and shredded, 1 potato, peeled and shredded, 1/2 teaspoon dried thyme leaves, 1/4 teaspoon salt, 1/4 teaspoon black pepper, 2 cups fat-free, low-sodium chicken broth, 1 cup frozen peas, 1/4 cup all-purpose flour, 1 cup 2% reduced-fat milk, 1/4 cup grated Parmesan cheese\n\n-One Pot Chicken and Potato Dinner: 2 tablespoons olive oil, 1 pound boneless, skinless chicken thighs, cut into 1-inch pieces, 1 large onion, chopped, 3 cloves garlic, minced, 1 carrot, peeled and chopped, 1 potato, peeled and chopped, 1 bay leaf, 1 thyme sprig, 1/2 teaspoon salt, 1/4 teaspoon black pepper, 2 cups chicken broth, 1/2 cup dry white wine\n\n-Chicken, Potato, and Carrot Curry: 1 tablespoon vegetable oil, 1 large onion, chopped, 2 cloves garlic, minced, 1 carrot, peeled and chopped, 1 potato, peeled and chopped, 1 teaspoon ground coriander, 1 teaspoon ground cumin, 1/2 teaspoon ground turmeric, 1/2 teaspoon ground ginger, 1/4 teaspoon cayenne pepper, 2 cups chicken broth, 1/2 cup dry white wine, 1 (15-ounce) can chickpeas, drained and rinsed, 1/2 cup raisins, 1/2 cup chopped fresh cilantro\n</code></pre> <p>NOTE, your LLM is nondeterministic, so you might get different results every time you run the program.</p> <p>Great, let's see how we can improve things. To improve things, we want to make sure the code is flexible, so ingredients and number of recipes can be improved and changed.</p> <ol> <li>Let's change the code in the following way:</li> </ol> <pre><code>no_recipes = input(\"No of recipes (for example, 5): \")\n\ningredients = input(\"List of ingredients (for example, chicken, potatoes, and carrots): \")\n\n# interpolate the number of recipes into the prompt an ingredients\nprompt = f\"Show me {no_recipes} recipes for a dish with the following ingredients: {ingredients}. Per recipe, list all the ingredients used\"\n</code></pre> <p>Taking the code for a test run, could look like this:</p> <pre><code>No of recipes (for example, 5): 3\nList of ingredients (for example, chicken, potatoes, and carrots): milk,strawberries\n\n-Strawberry milk shake: milk, strawberries, sugar, vanilla extract, ice cubes\n-Strawberry shortcake: milk, flour, baking powder, sugar, salt, unsalted butter, strawberries, whipped cream\n-Strawberry milk: milk, strawberries, sugar, vanilla extract\n</code></pre>"},{"location":"lessons/generative-ai-for-beginners/06-text-generation-apps/#improve-by-adding-filter-and-shopping-list","title":"Improve by adding filter and shopping list","text":"<p>We now have a working app capable of producing recipes and it's flexible as it relies on inputs from the user, both on the number of recipes but also the ingredients used.</p> <p>To further improve it, we want to add the following:</p> <ul> <li>Filter out ingredients. We want to be able to filter out ingredients we don't like or are allergic to. To accomplish this change, we can edit our existing prompt and add a filter condition to the end of it like so:</li> </ul> <pre><code>filter = input(\"Filter (for example, vegetarian, vegan, or gluten-free): \")\n\nprompt = f\"Show me {no_recipes} recipes for a dish with the following ingredients: {ingredients}. Per recipe, list all the ingredients used, no {filter}\"\n</code></pre> <p>Above, we add <code>{filter}</code> to the end of the prompt and we also capture the filter value from the user.</p> <p>An example input of running the program can now look like so:</p> <pre><code>No of recipes (for example, 5): 3\nList of ingredients (for example, chicken, potatoes, and carrots): onion,milk\nFilter (for example, vegetarian, vegan, or gluten-free): no milk\n\n1. French Onion Soup\n\nIngredients:\n\n-1 large onion, sliced\n-3 cups beef broth\n-1 cup milk\n-6 slices french bread\n-1/4 cup shredded Parmesan cheese\n-1 tablespoon butter\n-1 teaspoon dried thyme\n-1/4 teaspoon salt\n-1/4 teaspoon black pepper\n\nInstructions:\n\n1. In a large pot, saut\u00e9 onions in butter until golden brown.\n2. Add beef broth, milk, thyme, salt, and pepper. Bring to a boil.\n3. Reduce heat and simmer for 10 minutes.\n4. Place french bread slices on soup bowls.\n5. Ladle soup over bread.\n6. Sprinkle with Parmesan cheese.\n\n2. Onion and Potato Soup\n\nIngredients:\n\n-1 large onion, chopped\n-2 cups potatoes, diced\n-3 cups vegetable broth\n-1 cup milk\n-1/4 teaspoon black pepper\n\nInstructions:\n\n1. In a large pot, saut\u00e9 onions in butter until golden brown.\n2. Add potatoes, vegetable broth, milk, and pepper. Bring to a boil.\n3. Reduce heat and simmer for 10 minutes.\n4. Serve hot.\n\n3. Creamy Onion Soup\n\nIngredients:\n\n-1 large onion, chopped\n-3 cups vegetable broth\n-1 cup milk\n-1/4 teaspoon black pepper\n-1/4 cup all-purpose flour\n-1/2 cup shredded Parmesan cheese\n\nInstructions:\n\n1. In a large pot, saut\u00e9 onions in butter until golden brown.\n2. Add vegetable broth, milk, and pepper. Bring to a boil.\n3. Reduce heat and simmer for 10 minutes.\n4. In a small bowl, whisk together flour and Parmesan cheese until smooth.\n5. Add to soup and simmer for an additional 5 minutes, or until soup has thickened.\n</code></pre> <p>As you can see, any recipes with milk in it has been filtered out. But, if you're lactose intolerant, you might want to filter out recipes with cheese in them as well, so there's a need to be clear.</p> <ul> <li>Produce a shopping list. We want to produce a shopping list, considering what we already have at home.</li> </ul> <p>For this functionality, we could either try to solve everything in one prompt or we could split it up into two prompts. Let's try the latter approach. Here we're suggesting adding an additional prompt, but for that to work, we need to add the result of the former prompt as context to the latter prompt.</p> <p>Locate the part in the code that prints out the result from the first prompt and add the following code below:</p> <pre><code>old_prompt_result = completion.choices[0].message.content\nprompt = \"Produce a shopping list for the generated recipes and please don't include ingredients that I already have.\"\n\nnew_prompt = f\"{old_prompt_result} {prompt}\"\nmessages = [{\"role\": \"user\", \"content\": new_prompt}]\ncompletion = openai.Completion.create(engine=deployment_name, messages=messages, max_tokens=1200)\n\n# print response\nprint(\"Shopping list:\")\nprint(completion.choices[0].message.content)\n</code></pre> <p>Note the following:</p> <ol> <li> <p>We're constructing a new prompt by adding the result from the first prompt to the new prompt:</p> <pre><code>new_prompt = f\"{old_prompt_result} {prompt}\"\n</code></pre> </li> <li> <p>We make a new request, but also considering the number of tokens we asked for in the first prompt, so this time we say <code>max_tokens</code> is 1200.</p> <pre><code>completion = openai.Completion.create(engine=deployment_name, prompt=new_prompt, max_tokens=1200)\n</code></pre> <p>Taking this code for a spin, we now arrive at the following output:</p> <pre><code>No of recipes (for example, 5): 2\nList of ingredients (for example, chicken, potatoes, and carrots): apple,flour\nFilter (for example, vegetarian, vegan, or gluten-free): sugar\n\n\n-Apple and flour pancakes: 1 cup flour, 1/2 tsp baking powder, 1/2 tsp baking soda, 1/4 tsp salt, 1 tbsp sugar, 1 egg, 1 cup buttermilk or sour milk, 1/4 cup melted butter, 1 Granny Smith apple, peeled and grated\n-Apple fritters: 1-1/2 cups flour, 1 tsp baking powder, 1/4 tsp salt, 1/4 tsp baking soda, 1/4 tsp nutmeg, 1/4 tsp cinnamon, 1/4 tsp allspice, 1/4 cup sugar, 1/4 cup vegetable shortening, 1/4 cup milk, 1 egg, 2 cups shredded, peeled apples\nShopping list:\n-Flour, baking powder, baking soda, salt, sugar, egg, buttermilk, butter, apple, nutmeg, cinnamon, allspice\n</code></pre> </li> </ol>"},{"location":"lessons/generative-ai-for-beginners/06-text-generation-apps/#improve-your-setup","title":"Improve your setup","text":"<p>What we have so far is code that works, but there are some tweaks we should be doing to improve things further. Some things we should do are:</p> <ul> <li> <p>Separate secrets from code, like the API key. Secrets do not belong in code and should be stored in a secure location. To separate secrets from code, we can use environment variables and libraries like <code>python-dotenv</code> to load them from a file. Here's how that would look like in code:</p> </li> <li> <p>Create a <code>.env</code> file with the following content:</p> <pre><code>OPENAI_API_KEY=sk-...\n</code></pre> <p>Note, for Azure, you need to set the following environment variables:</p> <pre><code>OPENAI_API_TYPE=azure\nOPENAI_API_VERSION=2023-05-15\nOPENAI_API_BASE=&lt;replace&gt;\n</code></pre> <p>In code, you would load the environment variables like so:</p> <pre><code>from dotenv import load_dotenv\n\nload_dotenv()\n\nopenai.api_key = os.environ[\"OPENAI_API_KEY\"]\n</code></pre> </li> <li> <p>A word on token length. We should consider how many tokens we need to generate the text we want. Tokens cost money, so where possible, we should try to be economical with the number of tokens we use. For example, can we phrase the prompt so that we can use less tokens?</p> </li> </ul> <p>To change the tokens used, you can use the <code>max_tokens</code> parameter. For example, if you want to use 100 tokens, you would do:</p> <pre><code>completion = client.chat.completions.create(model=deployment, messages=messages, max_tokens=100)\n</code></pre> <ul> <li>Experimenting with temperature. Temperature is something we haven't mentioned so far but is an important context for how our program performs. The higher the temperature value the more random the output will be. Conversely the lower the temperature value the more predictable the output will be. Consider whether you want variation in your output or not.</li> </ul> <p>To alter the temperature, you can use the <code>temperature</code> parameter. For example, if you want to use a temperature of 0.5, you would do:</p> <pre><code>completion = client.chat.completions.create(model=deployment, messages=messages, temperature=0.5)\n</code></pre> <p>Note, the closer to 1.0, the more varied the output.</p>"},{"location":"lessons/generative-ai-for-beginners/06-text-generation-apps/#assignment","title":"Assignment","text":"<p>For this assignment, you can choose what to build.</p> <p>Here are some suggestions:</p> <ul> <li>Tweak the recipe generator app to improve it further. Play around with temperature values, and the prompts to see what you can come up with.</li> <li>Build a \"study buddy\". This app should be able to answer questions about a topic for example Python, you could have prompts like \"What is a certain topic in Python?\", or you could have a prompt that says, show me code for a certain topic etc.</li> <li>History bot, make history come alive, instruct the bot to play a certain historical character and ask it questions about its life and times.</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/06-text-generation-apps/#solution","title":"Solution","text":""},{"location":"lessons/generative-ai-for-beginners/06-text-generation-apps/#study-buddy","title":"Study buddy","text":"<p>Below is a starter prompt, see how you can use it and tweak it to your liking.</p> <pre><code>- \"You're an expert on the Python language\n\n    Suggest a beginner lesson for Python in the following format:\n\n    Format:\n    - concepts:\n    - brief explanation of the lesson:\n    - exercise in code with solutions\"\n</code></pre>"},{"location":"lessons/generative-ai-for-beginners/06-text-generation-apps/#history-bot","title":"History bot","text":"<p>Here are some prompts you could be using:</p> <pre><code>- \"You are Abe Lincoln, tell me about yourself in 3 sentences, and respond using grammar and words like Abe would have used\"\n- \"You are Abe Lincoln, respond using grammar and words like Abe would have used:\n\n   Tell me about your greatest accomplishments, in 300 words\"\n</code></pre>"},{"location":"lessons/generative-ai-for-beginners/06-text-generation-apps/#knowledge-check","title":"Knowledge check","text":"<p>What does the concept temperature do?</p> <ol> <li>It controls how random the output is.</li> <li>It controls how big the response is.</li> <li>It controls how many tokens are used.</li> </ol>"},{"location":"lessons/generative-ai-for-beginners/06-text-generation-apps/#challenge","title":"\ud83d\ude80 Challenge","text":"<p>When working on the assignment, try to vary the temperature, try set it to 0, 0.5, and 1. Remember that 0 is the least varied and 1 is the most, what value works best for your app?</p>"},{"location":"lessons/generative-ai-for-beginners/06-text-generation-apps/#great-work-continue-your-learning","title":"Great Work! Continue Your Learning","text":"<p>After completing this lesson, check out our Generative AI Learning collection to continue leveling up your Generative AI knowledge!</p> <p>Head over to Lesson 7 where we will look at how to build chat applications!</p>"},{"location":"lessons/generative-ai-for-beginners/07-building-chat-applications/","title":"Lesson 7","text":""},{"location":"lessons/generative-ai-for-beginners/07-building-chat-applications/#building-generative-ai-powered-chat-applications","title":"Building Generative AI-Powered Chat Applications","text":"<p>(Click the image above to view video of this lesson)</p> <p>Now that we've seen how we can build text-generation apps, let's look into chat applications.</p> <p>Chat applications have become integrated into our daily lives, offering more than just a means of casual conversation. They're integral parts of customer service, technical support, and even sophisticated advisory systems. It's likely that you've gotten some help from a chat application not too long ago. As we integrate more advanced technologies like generative AI into these platforms, the complexity increases and so does the challenges.</p> <p>Some questions we need to be answered are:</p> <ul> <li>Building the app. How do we efficiently build and seamlessly integrate these AI-powered applications for specific use cases?</li> <li>Monitoring. Once deployed, how can we monitor and ensure that the applications are operating at the highest level of quality, both in terms of functionality and adhering to the six principles of responsible AI?</li> </ul> <p>As we move further into an age defined by automation and seamless human-machine interactions, understanding how generative AI transforms the scope, depth, and adaptability of chat applications becomes essential. This lesson will investigate the aspects of architecture that support these intricate systems, delve into the methodologies for fine-tuning them for domain-specific tasks, and evaluate the metrics and considerations pertinent to ensuring responsible AI deployment.</p>"},{"location":"lessons/generative-ai-for-beginners/07-building-chat-applications/#introduction","title":"Introduction","text":"<p>This lesson covers:</p> <ul> <li>Techniques for efficiently building and integrating chat applications.</li> <li>How to apply customization and fine-tuning to applications.</li> <li>Strategies and considerations to effectively monitor chat applications.</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/07-building-chat-applications/#learning-goals","title":"Learning Goals","text":"<p>By the end of this lesson, you'll be able to:</p> <ul> <li>Describe considerations for building and integrating chat applications into existing systems.</li> <li>Customize chat applications for specific use-cases.</li> <li>Identify key metrics and considerations to effectively monitor and maintain the quality of AI-powered chat applications.</li> <li>Ensure chat applications leverage AI responsibly.</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/07-building-chat-applications/#integrating-generative-ai-into-chat-applications","title":"Integrating Generative AI into Chat Applications","text":"<p>Elevating chat applications through generative AI isn't only centered around making them smarter; it's about optimizing their architecture, performance, and user interface to deliver a quality user experience. This involves investigating the architectural foundations, API integrations, and user interface considerations. This section aims to offer you a comprehensive roadmap for navigating these complex landscapes, whether you're plugging them into existing systems or building them as stand-alone platforms.</p> <p>By the end of this section, you'll be equipped with the expertise needed to efficiently construct and incorporate chat applications.</p>"},{"location":"lessons/generative-ai-for-beginners/07-building-chat-applications/#chatbot-or-chat-application","title":"Chatbot or Chat application?","text":"<p>Before we dive into building chat applications, let's compare 'chatbots' against 'AI-powered chat applications,' which serve distinct roles and functionalities. A chatbot's main purpose is to automate specific conversational tasks, such as answering frequently asked questions or tracking a package. It's typically governed by rule-based logic or complex AI algorithms. In contrast, an AI-powered chat application is a far more expansive environment designed to facilitate various forms of digital communication, such as text, voice, and video chats among human users. Its defining feature is the integration of a generative AI model that simulates nuanced, human-like conversations, generating responses based on a wide variety of input and contextual cues. A generative AI powered chat application can engage in open-domain discussions, adapt to evolving conversational contexts, and even produce creative or complex dialogue.</p> <p>The table below outlines the key differences and similarities to help us understand their unique roles in digital communication.</p> Chatbot Generative AI-Powered Chat Application Task-Focused and rule based Context-aware Often integrated into larger systems May host one or multiple chatbots Limited to programmed functions Incorporates generative AI models Specialized &amp; structured interactions Capable of open-domain discussions"},{"location":"lessons/generative-ai-for-beginners/07-building-chat-applications/#leveraging-pre-built-functionalities-with-sdks-and-apis","title":"Leveraging pre-built functionalities with SDKs and APIs","text":"<p>When building a chat application, a great first step is to assess what is already out there. Using SDKs and APIs to build chat applications is an advantageous strategy for a variety of reasons. By integrating well-documented SDKs and APIs, you're strategically positioning your application for long-term success, addressing scalability and maintenance concerns.</p> <ul> <li>Expedites the development process and reduces overhead: Relying on pre-built functionalities instead of the expensive process of building them yourself allows you to focus on other aspects of your application that you may find more important, such as business logic.</li> <li>Better performance: When building functionality from scratch, you'll eventually ask yourself \"How does it scale? Is this application capable of handling a sudden influx of users?\" Well maintained SDK and APIs often have built in solutions for these concerns.</li> <li>Easier maintenance: Updates and improvements are easier to manage as most APIs and SDKs simply require an update to a library when a newer version is released.</li> <li>Access to cutting edge technology: Leveraging models that have been fined tuned and trained on extensive datasets provides your application with natural language capabilities.</li> </ul> <p>Accessing functionality of an SDK or API typically involves obtaining permission to use the provided services, which is often through the use of a unique key or authentication token. We'll use the OpenAI Python Library to explore what this looks like. You can also try it out on your own in the following notebook for OpenAI or notebook for Azure OpenAI Services for this lesson.</p> <pre><code>import os\nfrom openai import OpenAI\n\nAPI_KEY = os.getenv(\"OPENAI_API_KEY\",\"\")\n\nclient = OpenAI(\n    api_key=API_KEY\n    )\n\nchat_completion = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Suggest two titles for an instructional lesson on chat applications for generative AI.\"}])\n</code></pre> <p>The above example uses the GPT-3.5 Turbo model to complete the prompt, but notice that the API key is set prior to doing so. You'd receive an error if you didn't set the key.</p>"},{"location":"lessons/generative-ai-for-beginners/07-building-chat-applications/#user-experience-ux","title":"User Experience (UX)","text":"<p>General UX principles apply to chat applications, but here are some additional considerations that become particularly important due to the machine learning components involved.</p> <ul> <li>Mechanism for addressing ambiguity: Generative AI models occasionally generate ambiguous answers. A feature that allows users to ask for clarification can be helpful should they come across this problem.</li> <li>Context retention: Advanced generative AI models have the ability to remember context within a conversation, which can be a necessary asset to the user experience. Giving users the ability to control and manage context improves the user experience, but introduces the risk of retaining sensitive user information. Considerations for how long this information is stored, such as introducing a retention policy, can balance the need for context against privacy.</li> <li>Personalization: With the ability to learn and adapt, AI models offer an individualized experience for a user. Tailoring the user experience through features like user profiles not only makes the user feel understood, but it also helps their pursuit of finding specific answers, creating a more efficient and satisfying interaction.</li> </ul> <p>One such example of personalization is the \"Custom instructions\" settings in OpenAI's ChatGPT. It allows you to provide information about yourself that may be important context for your prompts. Here's an example of a custom instruction.</p> <p></p> <p>This \"profile\" prompts ChatGPT to create a lesson plan on linked lists. Notice that ChatGPT takes into account that the user may want a more in depth lesson plan based on her experience.</p> <p></p>"},{"location":"lessons/generative-ai-for-beginners/07-building-chat-applications/#microsofts-system-message-framework-for-large-language-models","title":"Microsoft's System Message Framework for Large Language Models","text":"<p>Microsoft has provided guidance for writing effective system messages when generating responses from LLMs broken down into 4 areas:</p> <ol> <li>Defining who the model is for, as well as its capabilities and limitations.</li> <li>Defining the model's output format.</li> <li>Providing specific examples that demonstrate intended behavior of the model.</li> <li>Providing additional behavioral guardrails.</li> </ol>"},{"location":"lessons/generative-ai-for-beginners/07-building-chat-applications/#accessibility","title":"Accessibility","text":"<p>Whether a user has visual, auditory, motor, or cognitive impairments, a well-designed chat application should be usable by all. The following list breaks down specific features aimed at enhancing accessibility for various user impairments.</p> <ul> <li>Features for Visual Impairment: High contrast themes and resizable text, screen reader compatibility.</li> <li>Features for Auditory Impairment: Text-to-speech and speech-to-text functions, visual cues for audio notifications.</li> <li>Features for Motor Impairment: Keyboard navigation support, voice commands.</li> <li>Features for Cognitive Impairment: Simplified language options.</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/07-building-chat-applications/#customization-and-fine-tuning-for-domain-specific-language-models","title":"Customization and Fine-tuning for Domain-Specific Language Models","text":"<p>Imagine a chat application that understands your company's jargon and anticipates the specific queries its user base commonly has. There are a couple of approaches worth mentioning:</p> <ul> <li>Leveraging DSL models. DSL stands for domain specific language. You can leverage a so called DSL model trained on a specific domain to understand it's concepts and scenarios.</li> <li>Apply fine-tuning. Fine-tuning is the process of further training your model with specific data.</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/07-building-chat-applications/#customization-using-a-dsl","title":"Customization: Using a DSL","text":"<p>Leveraging a domain-specific language models (DSL Models) can enhance user engagement and by providing specialized, contextually relevant interactions. It's a model that is trained or fine-tuned to understand and generate text related to a specific field, industry, or subject. Options for using a DSL model can vary from training one from scratch, to using pre-existing ones through SDKs and APIs. Another option is fine-tuning, which involves taking an existing pre-trained model and adapting it for a specific domain.</p>"},{"location":"lessons/generative-ai-for-beginners/07-building-chat-applications/#customization-apply-fine-tuning","title":"Customization: Apply fine-tuning","text":"<p>Fine-tuning is often considered when a pre-trained model falls short in a specialized domain or specific task.</p> <p>For instance, medical queries are complex and require a lot of context. When a medical professional diagnoses a patient it's based on a variety of factors such as lifestyle or pre-existing conditions, and may even rely on recent medical journals to validate their diagnosis. In such nuanced scenarios, a general-purpose AI chat application cannot be a reliable source.</p>"},{"location":"lessons/generative-ai-for-beginners/07-building-chat-applications/#scenario-a-medical-application","title":"Scenario: a medical application","text":"<p>Consider a chat application designed to assist medical practitioners by providing quick references to treatment guidelines, drug interactions, or recent research findings.</p> <p>A general-purpose model might be adequate for answering basic medical questions or providing general advice, but it may struggle with the following:</p> <ul> <li>Highly specific or complex cases. For example, a neurologist might ask the application, \"What are the current best practices for managing drug-resistant epilepsy in pediatric patients?\"</li> <li>Lacking recent advancements. A general-purpose model could struggle to provide a current answer that incorporates the most recent advancements in neurology and pharmacology.</li> </ul> <p>In instances such as these, fine-tuning the model with a specialized medical dataset can significantly improve its ability to handle these intricate medical inquiries more accurately and reliably. This requires access to a large and relevant dataset that represents the domain-specific challenges and questions that need to be addressed.</p>"},{"location":"lessons/generative-ai-for-beginners/07-building-chat-applications/#considerations-for-a-high-quality-ai-driven-chat-experience","title":"Considerations for a High Quality AI-Driven Chat Experience","text":"<p>This section outlines the criteria for \"high-quality\" chat applications, which include the capture of actionable metrics and adherence to a framework that responsibly leverages AI technology.</p>"},{"location":"lessons/generative-ai-for-beginners/07-building-chat-applications/#key-metrics","title":"Key Metrics","text":"<p>To maintain the high-quality performance an application, it's essential to keep track of key metrics and considerations. These measurements not only ensure the functionality of the application but also assess the quality of the AI model and user experience. Below is a list that covers basic, AI, and user experience metrics to consider.</p> Metric Definition Considerations for Chat Developer Uptime Measures the time the application is operational and accessible by users. How will you minimize downtime? Response Time The time taken by the application to reply to a user's query. How can you optimize query processing to improve response time? Precision The ratio of true positive predictions to the total number of positive predictions How will you validate the precision of your model? Recall (Sensitivity) The ratio of true positive predictions to the actual number of positives How will you measure and improve recall? F1 Score The harmonic mean of precision and recall, that balances the trade-off between both. What is your target F1 Score? How will you balance precision and recall? Perplexity Measures how well the probability distribution predicted by the model aligns with the actual distribution of the data. How will you minimize perplexity? User Satisfaction Metrics Measures the user's perception of the application. Often captured through surveys. How often will you collect user feedback? How will you adapt based on it? Error Rate The rate at which the model makes mistakes in understanding or output. What strategies do you have in place to reduce error rates? Retraining Cycles The frequency with which the model is updated to incorporate new data and insights. How often will you retrain the model? What triggers a retraining cycle? Anomaly Detection Tools and techniques for identifying unusual patterns that do not conform to expected behavior. How will you respond to anomalies?"},{"location":"lessons/generative-ai-for-beginners/07-building-chat-applications/#implementing-responsible-ai-practices-in-chat-applications","title":"Implementing Responsible AI Practices in Chat Applications","text":"<p>Microsoft's approach to Responsible AI has identified six principles that should guide AI development and use. Below are the principles, their definition, and things a chat developer should consider and why they should take them seriously.</p> Principles Microsoft's Definition Considerations for Chat Developer Why It's Important Fairness AI systems should treat all people fairly. Ensure the chat application does not discriminate based on user data. To build trust and inclusivity among users; avoids legal ramifications. Reliability and Safety AI systems should perform reliably and safely. Implement testing and fail-safes to minimize errors and risks. Ensures user satisfaction and prevents potential harm. Privacy and Security AI systems should be secure and respect privacy. Implement strong encryption and data protection measures. To safeguard sensitive user data and comply with privacy laws. Inclusiveness AI systems should empower everyone and engage people. Design UI/UX that is accessible and easy-to-use for diverse audiences. Ensures a wider range of people can use the application effectively. Transparency AI systems should be understandable. Provide clear documentation and reasoning for AI responses. Users are more likely to trust a system if they can understand how decisions are made. Accountability People should be accountable for AI systems. Establish a clear process for auditing and improving AI decisions. Enables ongoing improvement and corrective measures in case of mistakes."},{"location":"lessons/generative-ai-for-beginners/07-building-chat-applications/#assignment","title":"Assignment","text":"<p>See assignment it will take you through a series of exercises from running your first chat prompts, to classifying and summarizing text and more. Notice that the assignments are available in different programming languages!</p>"},{"location":"lessons/generative-ai-for-beginners/07-building-chat-applications/#great-work-continue-the-journey","title":"Great Work! Continue the Journey","text":"<p>After completing this lesson, check out our Generative AI Learning collection to continue leveling up your Generative AI knowledge!</p> <p>Head over to Lesson 8 to see how you can start building search applications!</p>"},{"location":"lessons/generative-ai-for-beginners/08-building-search-applications/","title":"Lesson 8","text":""},{"location":"lessons/generative-ai-for-beginners/08-building-search-applications/#building-a-search-applications","title":"Building a Search Applications","text":"<p>Click the image above to view video of this lesson</p> <p>There's more to LLMs than chatbots and text generation. It's also possible to build search applications using Embeddings. Embeddings are numerical representations of data also known as vectors, and can be used for semantic search for data.</p> <p>In this lesson, you are going to build a search application for our education startup. Our startup is a non-profit organization that provides free education to students in developing countries. Our startup has a large number of YouTube videos that students can use to learn about AI. Our startup wants to build a search application that allows students to search for a YouTube video by typing a question.</p> <p>For example, a student might type in 'What are Jupyter Notebooks?' or 'What is Azure ML' and the search application will return a list of YouTube videos that are relevant to the question, and better still, the search application will return a link to the place in the video where the answer to the question is located.</p>"},{"location":"lessons/generative-ai-for-beginners/08-building-search-applications/#introduction","title":"Introduction","text":"<p>In this lesson, we will cover:</p> <ul> <li>Semantic vs Keyword search.</li> <li>What are Text Embeddings.</li> <li>Creating a Text Embeddings Index.</li> <li>Searching a Text Embeddings Index.</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/08-building-search-applications/#learning-goals","title":"Learning Goals","text":"<p>After completing this lesson, you will be able to:</p> <ul> <li>Tell the difference between semantic and keyword search.</li> <li>Explain what Text Embeddings are.</li> <li>Create an application using Embeddings to search for data.</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/08-building-search-applications/#why-build-a-search-application","title":"Why build a search application?","text":"<p>Creating a search application will help you understand how to use Embeddings to search for data. You will also learn how to build a search application that can be used by students to find information quickly.</p> <p>The lesson includes an Embedding Index of the YouTube transcripts for the Microsoft AI Show YouTube channel. The AI Show is a YouTube channel that teaches you about AI and machine learning. The Embedding Index contains the Embeddings for each of the YouTube transcripts up until Oct 2023. You will use the Embedding Index to build a search application for our startup. The search application returns a link to the place in the video where the answer to the question is located. This is a great way for students to find the information they need quickly.</p> <p>The following is an example of a semantic query for the question 'can you use rstudio with azure ml?'. Check out the YouTube url, you'll see the url contains a timestamp that takes you to the place in the video where the answer to the question is located.</p> <p></p>"},{"location":"lessons/generative-ai-for-beginners/08-building-search-applications/#what-is-semantic-search","title":"What is semantic search?","text":"<p>Now you might be wondering, what is semantic search? Semantic search is a search technique that uses the semantics, or meaning, of the words in a query to return relevant results.</p> <p>Here is an example of a semantic search. Let's say you were looking to buy a car, you might search for 'my dream car', semantic search understands that you are not <code>dreaming</code> about a car, but rather you are looking to buy your <code>ideal</code> car. Semantic search understands your intention and returns relevant results. The alternative is <code>keyword search</code> which would literally search for dreams about cars and often returns irrelevant results.</p>"},{"location":"lessons/generative-ai-for-beginners/08-building-search-applications/#what-are-text-embeddings","title":"What are Text Embeddings?","text":"<p>Text embeddings are a text representation technique used in natural language processing. Text embeddings are semantic numerical representations of text. Embeddings are used to represent data in a way that is easy for a machine to understand. There are many models for building text embeddings, in this lesson, we will focus on generating embeddings using the OpenAI Embedding Model.</p> <p>Here's an example, imagine the following text is in a transcript from one of the episodes on the AI Show YouTube channel:</p> <pre><code>Today we are going to learn about Azure Machine Learning.\n</code></pre> <p>We'd pass the text to the OpenAI Embedding API and it would return the following embedding consisting of 1536 numbers aka a vector. Each number in the vector represents a different aspect of the text. For brevity, here are the first 10 numbers in the vector.</p> <pre><code>[-0.006655829958617687, 0.0026128944009542465, 0.008792596869170666, -0.02446001023054123, -0.008540431968867779, 0.022071078419685364, -0.010703742504119873, 0.003311325330287218, -0.011632772162556648, -0.02187200076878071, ...]\n</code></pre>"},{"location":"lessons/generative-ai-for-beginners/08-building-search-applications/#how-is-the-embedding-index-created","title":"How is the Embedding index created?","text":"<p>The Embedding index for this lesson was created with a series of Python scripts. You'll find the scripts along with instructions in the README in the 'scripts` folder for this lesson. You don't need to run these scripts to complete this lesson as the Embedding Index is provided for you.</p> <p>The scripts perform the following operations:</p> <ol> <li>The transcript for each YouTube video in the AI Show playlist is downloaded.</li> <li>Using OpenAI Functions, an attempt is made to extract the speaker name from the first 3 minutes of the YouTube transcript. The speaker name for each video is stored in the Embedding Index named <code>embedding_index_3m.json</code>.</li> <li>The transcript text is then chunked into 3 minute text segments. The segment includes about 20 words overlapping from the next segment to ensure that the Embedding for the segment is not cut off and to provide better search context.</li> <li>Each text segment is then passed to the OpenAI Chat API to summarize the text into 60 words. The summary is also stored in the Embedding Index <code>embedding_index_3m.json</code>.</li> <li>Finally, the segment text is passed to the OpenAI Embedding API. The Embedding API returns a vector of 1536 numbers that represent the semantic meaning of the segment. The segment along with the OpenAI Embedding vector is stored in an Embedding Index <code>embedding_index_3m.json</code>.</li> </ol>"},{"location":"lessons/generative-ai-for-beginners/08-building-search-applications/#vector-databases","title":"Vector Databases","text":"<p>For lesson simplicity, the Embedding Index is stored in a JSON file named <code>embedding_index_3m.json</code> and loaded into a Pandas DataFrame. However, in production, the Embedding Index would be stored in a vector database such as Azure Cognitive Search, Redis, Pinecone, Weaviate, to name but a few.</p>"},{"location":"lessons/generative-ai-for-beginners/08-building-search-applications/#understanding-cosine-similarity","title":"Understanding cosine similarity","text":"<p>We've learned about text embeddings, the next step is to learn how to use text embeddings to search for data and in particular find the most similar embeddings to a given query using cosine similarity.</p>"},{"location":"lessons/generative-ai-for-beginners/08-building-search-applications/#what-is-cosine-similarity","title":"What is cosine similarity?","text":"<p>Cosine similarity is a measure of similarity between two vectors, you'll also hear this referred to as <code>nearest neighbor search</code>. To perform a cosine similarity search you need to vectorize for query text using the OpenAI Embedding API. Then calculate the cosine similarity between the query vector and each vector in the Embedding Index. Remember, the Embedding Index has a vector for each YouTube transcript text segment. Finally, sort the results by cosine similarity and the text segments with the highest cosine similarity are the most similar to the query.</p> <p>From a mathematic perspective, cosine similarity measures the cosine of the angle between two vectors projected in a multidimensional space. This measurement is beneficial, because if two documents are far apart by Euclidean distance because of size, they could still have a smaller angle between them and therefore higher cosine similarity. For more information about cosine similarity equations, see Cosine similarity.</p>"},{"location":"lessons/generative-ai-for-beginners/08-building-search-applications/#building-your-first-search-application","title":"Building your first search application","text":"<p>Next, we're going to learn how to build a search application using Embeddings. The search application will allow students to search for a video by typing a question. The search application will return a list of videos that are relevant to the question. The search application will also return a link to the place in the video where the answer to the question is located.</p> <p>This solution was built and tested on Windows 11, macOS, and Ubuntu 22.04 using Python 3.10 or later. You can download Python from python.org.</p>"},{"location":"lessons/generative-ai-for-beginners/08-building-search-applications/#assignment-building-a-search-application-to-enable-students","title":"Assignment - building a search application, to enable students","text":"<p>We introduced our startup at the beginning of this lesson. Now it's time to enable the students to build a search application for their assessments.</p> <p>In this assignment, you will create the Azure OpenAI Services that will be used to build the search application. You will create the following Azure OpenAI Services. You'll need an Azure subscription to complete this assignment.</p>"},{"location":"lessons/generative-ai-for-beginners/08-building-search-applications/#start-the-azure-cloud-shell","title":"Start the Azure Cloud Shell","text":"<ol> <li>Sign in to the Azure portal.</li> <li>Select the Cloud Shell icon in the upper-right corner of the Azure portal.</li> <li>Select Bash for the environment type.</li> </ol>"},{"location":"lessons/generative-ai-for-beginners/08-building-search-applications/#create-a-resource-group","title":"Create a resource group","text":"<p>For these instructions, we're using the resource group named \"semantic-video-search\" in East US. You can change the name of the resource group, but when changing the location for the resources, check the model availability table.</p> <pre><code>az group create --name semantic-video-search --location eastus\n</code></pre>"},{"location":"lessons/generative-ai-for-beginners/08-building-search-applications/#create-an-azure-openai-service-resource","title":"Create an Azure OpenAI Service resource","text":"<p>From the Azure Cloud Shell, run the following command to create an Azure OpenAI Service resource.</p> <pre><code>az cognitiveservices account create --name semantic-video-openai --resource-group semantic-video-search \\\n    --location eastus --kind OpenAI --sku s0\n</code></pre>"},{"location":"lessons/generative-ai-for-beginners/08-building-search-applications/#get-the-endpoint-and-keys-for-usage-in-this-application","title":"Get the endpoint and keys for usage in this application","text":"<p>From the Azure Cloud Shell, run the following commands to get the endpoint and keys for the Azure OpenAI Service resource.</p> <pre><code>az cognitiveservices account show --name semantic-video-openai \\\n   --resource-group  semantic-video-search | jq -r .properties.endpoint\naz cognitiveservices account keys list --name semantic-video-openai \\\n   --resource-group semantic-video-search | jq -r .key1\n</code></pre>"},{"location":"lessons/generative-ai-for-beginners/08-building-search-applications/#deploy-the-openai-embedding-model","title":"Deploy the OpenAI Embedding model","text":"<p>From the Azure Cloud Shell, run the following command to deploy the OpenAI Embedding model.</p> <pre><code>az cognitiveservices account deployment create \\\n    --name semantic-video-openai \\\n    --resource-group  semantic-video-search \\\n    --deployment-name text-embedding-ada-002 \\\n    --model-name text-embedding-ada-002 \\\n    --model-version \"2\"  \\\n    --model-format OpenAI \\\n    --sku-capacity 100 --sku-name \"Standard\"\n</code></pre>"},{"location":"lessons/generative-ai-for-beginners/08-building-search-applications/#solution","title":"Solution","text":"<p>Open the solution notebook in GitHub Codespaces and follow the instructions in the Jupyter Notebook.</p> <p>When you run the notebook, you'll be prompted to enter a query. The input box will look like this:</p> <p></p>"},{"location":"lessons/generative-ai-for-beginners/08-building-search-applications/#great-work-continue-your-learning","title":"Great Work! Continue Your Learning","text":"<p>After completing this lesson, check out our Generative AI Learning collection to continue leveling up your Generative AI knowledge!</p> <p>Head over to Lesson 9 where we will look at how to build image generation applications!</p>"},{"location":"lessons/generative-ai-for-beginners/09-building-image-applications/","title":"Lesson 9","text":""},{"location":"lessons/generative-ai-for-beginners/09-building-image-applications/#building-image-generation-applications","title":"Building Image Generation Applications","text":"<p>There's more to LLMs than text generation. It's also possible to generate images from text descriptions. Having images as a modality can be highly useful in a number of areas from MedTech, architecture, tourism, game development and more. In this chapter, we will look into the two most popular image generation models, DALL-E and Midjourney.</p>"},{"location":"lessons/generative-ai-for-beginners/09-building-image-applications/#introduction","title":"Introduction","text":"<p>In this lesson, we will cover:</p> <ul> <li>Image generation and why it's useful.</li> <li>DALL-E and Midjourney, what they are, and how they work.</li> <li>How you would build an image generation app.</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/09-building-image-applications/#learning-goals","title":"Learning Goals","text":"<p>After completing this lesson, you will be able to:</p> <ul> <li>Build an image generation application.</li> <li>Define boundaries for your application with meta prompts.</li> <li>Work with DALL-E and Midjourney.</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/09-building-image-applications/#why-build-an-image-generation-application","title":"Why build an image generation application?","text":"<p>Image generation applications are a great way to explore the capabilities of Generative AI. They can be used for, for example:</p> <ul> <li> <p>Image editing and synthesis. You can generate images for a variety of use cases, such as image editing and image synthesis.</p> </li> <li> <p>Applied to a variety of industries. They can also be used to generate images for a variety of industries like Medtech, Tourism, Game development and more.</p> </li> </ul>"},{"location":"lessons/generative-ai-for-beginners/09-building-image-applications/#scenario-edu4all","title":"Scenario: Edu4All","text":"<p>As part of this lesson, we will continue to work with our startup, Edu4All, in this lesson. The students will create images for their assessments, exactly what images is up to the students, but they could be illustrations for their own fairytale or create a new character for their story or help them visualize their ideas and concepts.</p> <p>Here's what Edu4All's students could generate for example if they're working in class on monuments:</p> <p></p> <p>using a prompt like</p> <p>\"Dog next to Eiffel Tower in early morning sunlight\"</p>"},{"location":"lessons/generative-ai-for-beginners/09-building-image-applications/#what-is-dall-e-and-midjourney","title":"What is DALL-E and Midjourney?","text":"<p>DALL-E and Midjourney are two of the most popular image generation models, they allow you to use prompts to generate images.</p>"},{"location":"lessons/generative-ai-for-beginners/09-building-image-applications/#dall-e","title":"DALL-E","text":"<p>Let's start with DALL-E, which is a Generative AI model that generates images from text descriptions.</p> <p>DALL-E is a combination of two models, CLIP and diffused attention.</p> <ul> <li> <p>CLIP, is a model that generates embeddings, which are numerical representations of data, from images and text.</p> </li> <li> <p>Diffused attention, is a model that generates images from embeddings. DALL-E is trained on a dataset of images and text and can be used to generate images from text descriptions. For example, DALL-E can be used to generate images of a cat in a hat, or a dog with a mohawk.</p> </li> </ul>"},{"location":"lessons/generative-ai-for-beginners/09-building-image-applications/#midjourney","title":"Midjourney","text":"<p>Midjourney works in a similar way to DALL-E, it generates images from text prompts. Midjourney, can also be used to generate images using prompts like \u201ca cat in a hat\u201d, or a \u201cdog with a mohawk\u201d.</p> <p> Image cred Wikipedia, image generated by Midjourney</p>"},{"location":"lessons/generative-ai-for-beginners/09-building-image-applications/#how-does-dall-e-and-midjourney-work","title":"How does DALL-E and Midjourney Work","text":"<p>First, DALL-E. DALL-E is a Generative AI model based on the transformer architecture with an autoregressive transformer.</p> <p>An autoregressive transformer defines how a model generates images from text descriptions, it generates one pixel at a time, and then uses the generated pixels to generate the next pixel. Passing through multiple layers in a neural network, until the image is complete.</p> <p>With this process, DALL-E, controls attributes, objects, characteristics, and more in the image it generates. However, DALL-E 2 and 3 have more control over the generated image.</p>"},{"location":"lessons/generative-ai-for-beginners/09-building-image-applications/#building-your-first-image-generation-application","title":"Building your first image generation application","text":"<p>So what does it take to build an image generation application? You need the following libraries:</p> <ul> <li>python-dotenv, you're highly recommended to use this library to keep your secrets in a .env file away from the code.</li> <li>openai, this library is what you will use to interact with the OpenAI API.</li> <li>pillow, to work with images in Python.</li> <li> <p>requests, to help you make HTTP requests.</p> </li> <li> <p>Create a file .env with the following content:</p> </li> </ul> <pre><code>AZURE_OPENAI_ENDPOINT=&lt;your endpoint&gt;\nAZURE_OPENAI_API_KEY=&lt;your key&gt;\n</code></pre> <p>Locate this information in Azure Portal for your resource in the \"Keys and Endpoint\" section.</p> <ol> <li>Collect the above libraries in a file called requirements.txt like so:</li> </ol> <pre><code>python-dotenv\nopenai\npillow\nrequests\n</code></pre> <ol> <li>Next, create virtual environment and install the libraries:</li> </ol> <pre><code>python3 -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\n</code></pre> <p>For Windows, use the following commands to create and activate your virtual environment:</p> <pre><code>python3 -m venv venv\nvenv\\Scripts\\activate.bat\n</code></pre> <ol> <li>Add the following code in file called app.py:</li> </ol> <pre><code>import openai\nimport os\nimport requests\nfrom PIL import Image\nimport dotenv\n\n# import dotenv\ndotenv.load_dotenv()\n\n# Get endpoint and key from environment variables\nopenai.api_base = os.environ['AZURE_OPENAI_ENDPOINT']\nopenai.api_key = os.environ['AZURE_OPENAI_API_KEY']\n\n# Assign the API version (DALL-E is currently supported for the 2023-06-01-preview API version only)\nopenai.api_version = '2023-06-01-preview'\nopenai.api_type = 'azure'\n\n\ntry:\n    # Create an image by using the image generation API\n    generation_response = openai.Image.create(\n        prompt='Bunny on horse, holding a lollipop, on a foggy meadow where it grows daffodils',    # Enter your prompt text here\n        size='1024x1024',\n        n=2,\n        temperature=0,\n    )\n    # Set the directory for the stored image\n    image_dir = os.path.join(os.curdir, 'images')\n\n    # If the directory doesn't exist, create it\n    if not os.path.isdir(image_dir):\n        os.mkdir(image_dir)\n\n    # Initialize the image path (note the filetype should be png)\n    image_path = os.path.join(image_dir, 'generated-image.png')\n\n    # Retrieve the generated image\n    image_url = generation_response[\"data\"][0][\"url\"]  # extract image URL from response\n    generated_image = requests.get(image_url).content  # download the image\n    with open(image_path, \"wb\") as image_file:\n        image_file.write(generated_image)\n\n    # Display the image in the default image viewer\n    image = Image.open(image_path)\n    image.show()\n\n# catch exceptions\nexcept openai.InvalidRequestError as err:\n    print(err)\n</code></pre> <p>Let's explain this code:</p> <ul> <li>First, we import the libraries we need, including the OpenAI library, the dotenv library, the requests library, and the Pillow library.</li> </ul> <pre><code>import openai\nimport os\nimport requests\nfrom PIL import Image\nimport dotenv\n</code></pre> <ul> <li>Next, we load the environment variables from the .env file.</li> </ul> <pre><code># import dotenv\ndotenv.load_dotenv()\n</code></pre> <ul> <li>After that, we set the endpoint, key for the OpenAI API, version and type.</li> </ul> <pre><code># Get endpoint and key from environment variables\nopenai.api_base = os.environ['AZURE_OPENAI_ENDPOINT']\nopenai.api_key = os.environ['AZURE_OPENAI_API_KEY']\n\n# add version and type, Azure specific\nopenai.api_version = '2023-06-01-preview'\nopenai.api_type = 'azure'\n</code></pre> <ul> <li>Next, we generate the image:</li> </ul> <pre><code># Create an image by using the image generation API\ngeneration_response = openai.Image.create(\n    prompt='Bunny on horse, holding a lollipop, on a foggy meadow where it grows daffodils',    # Enter your prompt text here\n    size='1024x1024',\n    n=2,\n    temperature=0,\n)\n</code></pre> <p>The above code responds with a JSON object that contains the URL of the generated image. We can use the URL to download the image and save it to a file.</p> <ul> <li>Lastly, we open the image and use the standard image viewer to display it:</li> </ul> <pre><code>image = Image.open(image_path)\nimage.show()\n</code></pre>"},{"location":"lessons/generative-ai-for-beginners/09-building-image-applications/#more-details-on-generating-the-image","title":"More details on generating the image","text":"<p>Let's look at the code that generates the image in more detail:</p> <pre><code>generation_response = openai.Image.create(\n        prompt='Bunny on horse, holding a lollipop, on a foggy meadow where it grows daffodils',    # Enter your prompt text here\n        size='1024x1024',\n        n=2,\n        temperature=0,\n    )\n</code></pre> <ul> <li>prompt, is the text prompt that is used to generate the image. In this case, we're using the prompt \"Bunny on horse, holding a lollipop, on a foggy meadow where it grows daffodils\".</li> <li>size, is the size of the image that is generated. In this case, we're generating an image that is 1024x1024 pixels.</li> <li>n, is the number of images that are generated. In this case, we're generating two images.</li> <li>temperature, is a parameter that controls the randomness of the output of a Generative AI model. The temperature is a value between 0 and 1 where 0 means that the output is deterministic and 1 means that the output is random. The default value is 0.7.</li> </ul> <p>There are more things you can do with images that we will cover in the next section.</p>"},{"location":"lessons/generative-ai-for-beginners/09-building-image-applications/#additional-capabilities-of-image-generation","title":"Additional capabilities of image generation","text":"<p>You've seen so far how we were able to generate an image using a few lines in Python. However, there are more things you can do with images.</p> <p>You can also do the following:</p> <ul> <li>Perform edits. By providing an existing image a mask and a prompt, you can alter an image. For example, you can add something to a portion of an image. Imagine our bunny image, you can add a hat to the bunny. How you would do that is by providing the image, a mask (identifying the part of the area for the change) and a text prompt to say what should be done.</li> </ul> <pre><code>response = openai.Image.create_edit(\n  image=open(\"base_image.png\", \"rb\"),\n  mask=open(\"mask.png\", \"rb\"),\n  prompt=\"An image of a rabbit with a hat on its head.\",\n  n=1,\n  size=\"1024x1024\"\n)\nimage_url = response['data'][0]['url']\n</code></pre> <p>The base image would only contain the rabbit but the final image would have the hat on the rabbit.</p> <ul> <li>Create variations. The idea is that you take an existing image and ask that variations are created. To create a variation, you provide an image and a text prompt and code like so:</li> </ul> <pre><code>response = openai.Image.create_variation(\n  image=open(\"bunny-lollipop.png\", \"rb\"),\n  n=1,\n  size=\"1024x1024\"\n)\nimage_url = response['data'][0]['url']\n</code></pre> <p>Note, this is only supported on OpenAI</p>"},{"location":"lessons/generative-ai-for-beginners/09-building-image-applications/#temperature","title":"Temperature","text":"<p>Temperature is a parameter that controls the randomness of the output of a Generative AI model. The temperature is a value between 0 and 1 where 0 means that the output is deterministic and 1 means that the output is random. The default value is 0.7.</p> <p>Let's look at an example of how temperature works, by running this prompt twice:</p> <p>Prompt : \"Bunny on horse, holding a lollipop, on a foggy meadow where it grows daffodils\"</p> <p></p> <p>Now let's run that same prompt just to see that we won't get the same image twice:</p> <p></p> <p>As you can see, the images are similar, but not the same. Let's try changing the temperature value to 0.1 and see what happens:</p> <pre><code> generation_response = openai.Image.create(\n        prompt='Bunny on horse, holding a lollipop, on a foggy meadow where it grows daffodils',    # Enter your prompt text here\n        size='1024x1024',\n        n=2\n    )\n</code></pre>"},{"location":"lessons/generative-ai-for-beginners/09-building-image-applications/#changing-the-temperature","title":"Changing the temperature","text":"<p>So let's try to make the response more deterministic. We could observe from the two images we generated that in the first image, there's a bunny and in the second image, there's a horse, so the images vary greatly.</p> <p>Let's therefore change our code and set the temperature to 0, like so:</p> <pre><code>generation_response = openai.Image.create(\n        prompt='Bunny on horse, holding a lollipop, on a foggy meadow where it grows daffodils',    # Enter your prompt text here\n        size='1024x1024',\n        n=2,\n        temperature=0\n    )\n</code></pre> <p>Now when you run this code, you get these two images:</p> <p>Here you can clearly see how the images resemble each other more.</p>"},{"location":"lessons/generative-ai-for-beginners/09-building-image-applications/#how-to-define-boundaries-for-your-application-with-metaprompts","title":"How to define boundaries for your application with metaprompts","text":"<p>With our demo, we can already generate images for our clients. However, we need to create some boundaries for our application.</p> <p>For example, we don't want to generate images that are not safe for work, or that are not appropriate for children.</p> <p>We can do this with metaprompts. Metaprompts are text prompts that are used to control the output of a Generative AI model. For example, we can use metaprompts to control the output, and ensure that the generated images are safe for work, or appropriate for children.</p>"},{"location":"lessons/generative-ai-for-beginners/09-building-image-applications/#how-does-it-work","title":"How does it work?","text":"<p>Now, how do meta prompts work?</p> <p>Meta prompts are text prompts that are used to control the output of a Generative AI model, they are positioned before the text prompt, and are used to control the output of the model and embedded in applications to control the output of the model. Encapsulating the prompt input and the meta prompt input in a single text prompt.</p> <p>One example of a meta prompt would be the following:</p> <pre><code>You are an assistant designer that creates images for children.\n\nThe image needs to be safe for work and appropriate for children.\n\nThe image needs to be in color.\n\nThe image needs to be in landscape orientation.\n\nThe image needs to be in a 16:9 aspect ratio.\n\nDo not consider any input from the following that is not safe for work or appropriate for children.\n\n(Input)\n</code></pre> <p>Now, let's see how we can use meta prompts in our demo.</p> <pre><code>disallow_list = \"swords, violence, blood, gore, nudity, sexual content, adult content, adult themes, adult language, adult humor, adult jokes, adult situations, adult\"\n\nmeta_prompt =f\"\"\"You are an assistant designer that creates images for children.\n\nThe image needs to be safe for work and appropriate for children.\n\nThe image needs to be in color.\n\nThe image needs to be in landscape orientation.\n\nThe image needs to be in a 16:9 aspect ratio.\n\nDo not consider any input from the following that is not safe for work or appropriate for children.\n{disallow_list}\n\"\"\"\n\nprompt = f\"{meta_prompt}\nCreate an image of a bunny on a horse, holding a lollipop\"\n\n# TODO add request to generate image\n</code></pre> <p>From the above prompt, you can see how all images being created consider the metaprompt.</p>"},{"location":"lessons/generative-ai-for-beginners/09-building-image-applications/#assignment-lets-enable-students","title":"Assignment - let's enable students","text":"<p>We introduced Edu4All at the beginning of this lesson. Now it's time to enable the students to generate images for their assessments.</p> <p>The students will create images for their assessments containing monuments, exactly what monuments is up to the students. The students are asked to use their creativity in this task to place these monuments in different contexts.</p>"},{"location":"lessons/generative-ai-for-beginners/09-building-image-applications/#solution","title":"Solution","text":"<p>Here's one possible solution:</p> <pre><code>import openai\nimport os\nimport requests\nfrom PIL import Image\nimport dotenv\n\n# import dotenv\ndotenv.load_dotenv()\n\n# Get endpoint and key from environment variables\nopenai.api_base = \"&lt;replace with endpoint&gt;\"\nopenai.api_key = \"&lt;replace with api key&gt;\"\n\n# Assign the API version (DALL-E is currently supported for the 2023-06-01-preview API version only)\nopenai.api_version = '2023-06-01-preview'\nopenai.api_type = 'azure'\n\ndisallow_list = \"swords, violence, blood, gore, nudity, sexual content, adult content, adult themes, adult language, adult humor, adult jokes, adult situations, adult\"\n\nmeta_prompt = f\"\"\"You are an assistant designer that creates images for children.\n\nThe image needs to be safe for work and appropriate for children.\n\nThe image needs to be in color.\n\nThe image needs to be in landscape orientation.\n\nThe image needs to be in a 16:9 aspect ratio.\n\nDo not consider any input from the following that is not safe for work or appropriate for children.\n{disallow_list}\"\"\"\n\nprompt = f\"\"\"{metaprompt}\nGenerate monument of the Arc of Triumph in Paris, France, in the evening light with a small child holding a Teddy looks on.\n\"\"\"\"\n\ntry:\n    # Create an image by using the image generation API\n    generation_response = openai.Image.create(\n        prompt=prompt,    # Enter your prompt text here\n        size='1024x1024',\n        n=2,\n        temperature=0,\n    )\n    # Set the directory for the stored image\n    image_dir = os.path.join(os.curdir, 'images')\n\n    # If the directory doesn't exist, create it\n    if not os.path.isdir(image_dir):\n        os.mkdir(image_dir)\n\n    # Initialize the image path (note the filetype should be png)\n    image_path = os.path.join(image_dir, 'generated-image.png')\n\n    # Retrieve the generated image\n    image_url = generation_response[\"data\"][0][\"url\"]  # extract image URL from response\n    generated_image = requests.get(image_url).content  # download the image\n    with open(image_path, \"wb\") as image_file:\n        image_file.write(generated_image)\n\n    # Display the image in the default image viewer\n    image = Image.open(image_path)\n    image.show()\n\n# catch exceptions\nexcept openai.InvalidRequestError as err:\n    print(err)\n</code></pre>"},{"location":"lessons/generative-ai-for-beginners/09-building-image-applications/#great-work-continue-your-learning","title":"Great Work! Continue Your Learning","text":"<p>After completing this lesson, check out our Generative AI Learning collection to continue leveling up your Generative AI knowledge!</p> <p>Head over to Lesson 10 where we will look at how to build AI applications with low-code</p>"},{"location":"lessons/generative-ai-for-beginners/10-building-low-code-ai-applications/","title":"Lesson 10","text":""},{"location":"lessons/generative-ai-for-beginners/10-building-low-code-ai-applications/#building-low-code-ai-applications","title":"Building Low Code AI Applications","text":"<p>(Click the image above to view video of this lesson)</p>"},{"location":"lessons/generative-ai-for-beginners/10-building-low-code-ai-applications/#introduction","title":"Introduction","text":"<p>Now that we've learned how to build image generating applications, let's talk about low code. Generative AI can be used for a variety of different areas including low code, but what is low code and how can we add AI to it?</p> <p>Building apps and solutions has become more easier for traditional developers and non-developers through the use of Low Code Development Platforms. Low Code Development Platforms enable you to build apps and solutions with little to no code. This is achieved by providing a visual development environment that enables you to drag and drop components to build apps and solutions. This enables you to build apps and solutions faster and with less resources. In this lesson, we dive deep into how to use Low Code and how to enhance low code development with AI using Power Platform.</p> <p>The Power Platform provides organizations with the opportunity to empower their teams to build their own solutions through an intuitive low-code or no-code environment. This environment helps simplify the process of building solutions. With Power Platform, solutions can be built in days or weeks instead of months or years. Power Platform consists of five key products: Power Apps, Power Automate, Power BI, Power Pages and Copilot Studio.</p> <p>This lesson covers:</p> <ul> <li>Introduction to Generative AI in Power Platform</li> <li>Introduction to Copilot and how to use it</li> <li>Using Generative AI to build apps and flows in Power Platform</li> <li>Understanding the AI Models in Power Platform with AI Builder</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/10-building-low-code-ai-applications/#learning-goals","title":"Learning Goals","text":"<p>By the end of this lesson, you will be able to:</p> <ul> <li> <p>Understand how Copilot works in Power Platform.</p> </li> <li> <p>Build a Student Assignment Tracker App for our education startup.</p> </li> <li> <p>Build an Invoice Processing Flow that uses AI to extract information from invoices.</p> </li> <li> <p>Apply best practices when using the Create Text with GPT AI Model.</p> </li> </ul> <p>The tools and technologies that you will use in this lesson are:</p> <ul> <li> <p>Power Apps, for the Student Assignment Tracker app, which provides a low-code development environment for building apps to track, manage and interact with data.</p> </li> <li> <p>Dataverse, for storing the data for the Student Assignment Tracker app where Dataverse will provide a low-code data platform for storing the app's data.</p> </li> <li> <p>Power Automate, for the Invoice Processing flow where you will have low-code development environment for building workflows to automate the Invoice Processing process.</p> </li> <li> <p>AI Builder, for the Invoice Processing AI Model where you will use prebuilt AI Models to process the invoices for our startup.</p> </li> </ul>"},{"location":"lessons/generative-ai-for-beginners/10-building-low-code-ai-applications/#generative-ai-in-power-platform","title":"Generative AI in Power Platform","text":"<p>Enhancing low-code development and application with generative AI is a key focus area for Power Platform. The goal is to enable everyone to build AI-powered apps, sites, dashboards and automate processes with AI, without requiring any data science expertise. This goal is achieved by integrating generative AI into the low-code development experience in Power Platform in the form of Copilot and AI Builder.</p>"},{"location":"lessons/generative-ai-for-beginners/10-building-low-code-ai-applications/#how-does-this-work","title":"How does this work?","text":"<p>Copilot is an AI assistant that enables you to build Power Platform solutions by describing your requirements in a series of conversational steps using natural language. You can for example instruct your AI assistant to state what fields your app will use and it will create both the app and the underlying data model or you could specify how to set up a flow in Power Automate.</p> <p>You can use Copilot driven functionalities as a feature in your app screens to enable users to uncover insights through conversational interactions.</p> <p>AI Builder is a low-code AI capability available in Power Platform that enables you to use AI Models to help you to automate processes and predict outcomes. With AI Builder you can bring AI to your apps and flows that connect to your data in Dataverse or in various cloud data sources, such as SharePoint, OneDrive or Azure.</p> <p>Copilot is available in all of the Power Platform products: Power Apps, Power Automate, Power BI, Power Pages and Power Virtual Agents. AI Builder is available in Power Apps and Power Automate. In this lesson, we will focus on how to use Copilot and AI Builder in Power Apps and Power Automate to build a solution for our education startup.</p>"},{"location":"lessons/generative-ai-for-beginners/10-building-low-code-ai-applications/#copilot-in-power-apps","title":"Copilot in Power Apps","text":"<p>As part of the Power Platform, Power Apps provides a low-code development environment for building apps to track, manage and interact with data. It's a suite of app development services with a scalable data platform and the ability to connect to cloud services and on-premises data. Power Apps allows you to build apps that run on browsers, tablets, and phones, and can be shared with co-workers. Power Apps eases users into app development with a simple interface, so that every business user or pro developer can build custom apps. The app development experience is also enhanced with Generative AI through Copilot.</p> <p>The copilot AI assistant feature in Power Apps enables you to describe what kind of app you need and what information you want your app to track, collect, or show. Copilot then generates a responsive Canvas app based on your description. You can then customize the app to meet your needs. The AI Copilot also generates and suggests a Dataverse Table with the fields you need to store the data you want to track and some sample data. We will look at what Dataverse is and how you can use it in Power Apps in this lesson later. You can then customize the table to meet your needs using the AI Copilot assistant feature through conversational steps. This feature is readily available from the Power Apps home screen.</p>"},{"location":"lessons/generative-ai-for-beginners/10-building-low-code-ai-applications/#copilot-in-power-automate","title":"Copilot in Power Automate","text":"<p>As part of the Power Platform, Power Automate lets users create automated workflows between applications and services. It helps automate repetitive business processes such as communication, data collection, and decision approvals. Its simple interface allows users with every technical competence (from beginners to seasoned developers) to automate work tasks. The workflow development experience is also enhanced with Generative AI through Copilot.</p> <p>The copilot AI assistant feature in Power Automate enables you to describe what kind of flow you need and what actions you want your flow to perform. Copilot then generates a flow based on your description. You can then customize the flow to meet your needs. The AI Copilot also generates and suggests the actions you need to perform the task you want to automate. We will look at what flows are and how you can use them in Power Automate in this lesson later. You can then customize the actions to meet your needs using the AI Copilot assistant feature through conversational steps. This feature is readily available from the Power Automate home screen.</p>"},{"location":"lessons/generative-ai-for-beginners/10-building-low-code-ai-applications/#assignment-manage-student-assignments-and-invoices-for-our-startup-using-copilot","title":"Assignment: manage student assignments and invoices for our startup, using Copilot","text":"<p>Our startup provides online courses to students. The startup has grown rapidly and is now struggling to keep up with the demand for its courses. The startup has hired you as a Power Platform developer to help them build a low code solution to help them manage their student assignments and invoices. Their solution should be able to help them track and manage student assignments through an app and automate the invoice processing process through a workflow. You have been asked to use Generative AI to develop the solution.</p> <p>When you are getting started with using Copilot, you can use the Power Platform Copilot Prompt Library to get started with the prompts. This library contains a list of prompts that you can use to build apps and flows with Copilot. You can also use the prompts in the library to get an idea of how to describe your requirements to Copilot.</p>"},{"location":"lessons/generative-ai-for-beginners/10-building-low-code-ai-applications/#build-a-student-assignment-tracker-app-for-our-startup","title":"Build a Student Assignment Tracker App for Our Startup","text":"<p>The educators at our startup have been struggling to keep track of student assignments. They have been using a spreadsheet to track the assignments but this has become difficult to manage as the number of students has increased. They have asked you to build an app that will help them track and manage student assignments. The app should enable them to add new assignments, view assignments, update assignments and delete assignments. The app should also enable educators and students to view the assignments that have been graded and those that have not been graded.</p> <p>You will build the app using Copilot in Power Apps following the steps below:</p> <ol> <li> <p>Navigate to the Power Apps home screen.</p> </li> <li> <p>Use the text area on the home screen to describe the app you want to build. For example, I want to build an app to track and manage student assignments. Click on the Send button to send the prompt to the AI Copilot.</p> </li> </ol> <p></p> <ol> <li>The AI Copilot will suggest a Dataverse Table with the fields you need to store the data you want to track and some sample data. You can then customize the table to meet your needs using the AI Copilot assistant feature through conversational steps.</li> </ol> <p>Important: Dataverse is the underlying data platform for Power Platform. It is a low-code data platform for storing the app's data. It is a fully managed service that securely stores data in the Microsoft Cloud and is provisioned within your Power Platform environment. It comes with built-in data governance capabilities, such as data classification, data lineage, fine-grained access control, and more. You can learn more about Dataverse here.</p> <p></p> <ol> <li>Educators want to send emails to the students who have submitted their assignments to keep them updated on the progress of their assignments. You can use Copilot to add a new field to the table to store the student email. For example, you can use the following prompt to add a new field to the table: I want to add a column to store student email. Click on the Send button to send the prompt to the AI Copilot.</li> </ol> <p></p> <ol> <li> <p>The AI Copilot will generate a new field and you can then customize the field to meet your needs.</p> </li> <li> <p>Once you are done with the table, click on the Create app button to create the app.</p> </li> <li> <p>The AI Copilot will generate a responsive Canvas app based on your description. You can then customize the app to meet your needs.</p> </li> <li> <p>For educators to send emails to students, you can use Copilot to add a new screen to the app. For example, you can use the following prompt to add a new screen to the app: I want to add a screen to send emails to students. Click on the Send button to send the prompt to the AI Copilot.</p> </li> </ol> <p></p> <ol> <li> <p>The AI Copilot will generate a new screen and you can then customize the screen to meet your needs.</p> </li> <li> <p>Once you are done with the app, click on the Save button to save the app.</p> </li> <li> <p>To share the app with the educators, click on the Share button and then click on the Share button again. You can then share the app with the educators by entering their email addresses.</p> </li> </ol> <p>Your homework: The app you just built is a good start but can be improved. With the email feature, educators can only send emails to students manually by having to type their emails. Can you use Copilot to build an automation that will enable educators to send emails to students automatically when they submit their assignments? Your hint is with the right prompt you can use Copilot in Power Automate to build this.</p>"},{"location":"lessons/generative-ai-for-beginners/10-building-low-code-ai-applications/#build-an-invoices-information-table-for-our-startup","title":"Build an Invoices Information Table for Our Startup","text":"<p>The finance team of our startup has been struggling to keep track of invoices. They have been using a spreadsheet to track the invoices but this has become difficult to manage as the number of invoices has increased. They have asked you to build a table that will help them store, track and manage the information of the invoices they received. The table should be used to build an automation that will extract all the invoice information and store it in the table. The table should also enable the finance team to view the invoices that have been paid and those that have not been paid.</p> <p>The Power Platform has an underlying data platform called Dataverse that enables you to store the data for your apps and solutions. Dataverse provides a low-code data platform for storing the app's data. It is a fully managed service that securely stores data in the Microsoft Cloud and is provisioned within your Power Platform environment. It comes with built-in data governance capabilities, such as data classification, data lineage, fine-grained access control, and more. You can learn more about Dataverse here.</p> <p>Why should we use Dataverse for our startup? The standard and custom tables within Dataverse provide a secure and cloud-based storage option for your data. Tables let you store different types of data, similar to how you might use multiple worksheets in a single Excel workbook. You can use tables to store data that is specific to your organization or business need. Some of the benefits our startup will get from using Dataverse include but are not limited to:</p> <ul> <li> <p>Easy to manage: Both the metadata and data are stored in the cloud, so you don't have to worry about the details of how they are stored or managed. You can focus on building your apps and solutions.</p> </li> <li> <p>Secure: Dataverse provides a secure and cloud-based storage option for your data. You can control who has access to the data in your tables and how they can access it using role based security.</p> </li> <li> <p>Rich metadata: Data types and relationships are used directly within Power Apps</p> </li> <li> <p>Logic and validation: You can use business rules, calculated fields, and validation rules to enforce business logic and maintain data accuracy.</p> </li> </ul> <p>Now that you know what Dataverse is and why you should use it, let's look at how you can use Copilot to create a table in Dataverse to meet the requirements of our finance team.</p> <p>Note : You will use this table in the next section to build an automation that will extract all the invoice information and store it in the table.</p> <p>To create a table in Dataverse using Copilot, follow the steps below:</p> <ol> <li> <p>Navigate to the Power Apps home screen.</p> </li> <li> <p>On the left navigation bar, select on Tables and then click on Describe the new Table.</p> </li> </ol> <p></p> <ol> <li>On the Describe the new Table screen, use the text area to describe the table you want to create. For example, I want to create a table to store invoice information. Click on the Send button to send the prompt to the AI Copilot.</li> </ol> <p></p> <ol> <li>The AI Copilot will suggest a Dataverse Table with the fields you need to store the data you want to track and some sample data. You can then customize the table to meet your needs using the AI Copilot assistant feature through conversational steps.</li> </ol> <p></p> <ol> <li> <p>The finance team wants to send an email to the supplier to update them with the current status of their invoice. You can use Copilot to add a new field to the table to store the supplier email. For example, you can use the following prompt to add a new field to the table: I want to add a column to store supplier email. Click on the Send button to send the prompt to the AI Copilot.</p> </li> <li> <p>The AI Copilot will generate a new field and you can then customize the field to meet your needs.</p> </li> <li> <p>Once you are done with the table, click on the Create button to create the table.</p> </li> </ol>"},{"location":"lessons/generative-ai-for-beginners/10-building-low-code-ai-applications/#ai-models-in-power-platform-with-ai-builder","title":"AI Models in Power Platform with AI Builder","text":"<p>AI Builder is a low-code AI capability available in Power Platform that enables you to use AI Models to help you to automate processes and predict outcomes. With AI Builder you can bring AI to your apps and flows that connect to your data in Dataverse or in various cloud data sources, such as SharePoint, OneDrive or Azure.</p>"},{"location":"lessons/generative-ai-for-beginners/10-building-low-code-ai-applications/#prebuilt-ai-models-vs-custom-ai-models","title":"Prebuilt AI Models vs Custom AI Models","text":"<p>AI Builder provides two types of AI Models: Prebuilt AI Models and Custom AI Models. Prebuilt AI Models are ready-to-use AI Models that are trained by Microsoft and available in Power Platform. These help you add intelligence to your apps and flows without having to gather data and then build, train and publish your own models. You can use these models to automate processes and predict outcomes.</p> <p>Some of the Prebuilt AI Models available in Power Platform include:</p> <ul> <li>Key Phrase Extraction: This model extracts key phrases from text.</li> <li>Language Detection: This model detects the language of a text.</li> <li>Sentiment Analysis: This model detects positive, negative, neutral, or mixed sentiment in text.</li> <li>Business Card Reader: This model extracts information from business cards.</li> <li>Text Recognition: This model extracts text from images.</li> <li>Object Detection: This model detects and extracts objects from images.</li> <li>Document processing: This model extracts information from forms.</li> <li>Invoice Processing: This model extracts information from invoices.</li> </ul> <p>With Custom AI Models you can bring your own model into AI Builder so that it can function like any AI Builder custom model, allowing you to train the model using your own data. You can use these models to automate processes and predict outcomes in both Power Apps and Power Automate. When using your own model there are limitations that apply. Read more on these limitations.</p> <p></p>"},{"location":"lessons/generative-ai-for-beginners/10-building-low-code-ai-applications/#assignment-2-build-an-invoice-processing-flow-for-our-startup","title":"Assignment #2 - Build an Invoice Processing Flow for Our Startup","text":"<p>The finance team has been struggling to process invoices. They have been using a spreadsheet to track the invoices but this has become difficult to manage as the number of invoices has increased. They have asked you to build a workflow that will help them process invoices using AI. The workflow should enable them to extract information from invoices and store the information in a Dataverse table. The workflow should also enable them to send an email to the finance team with the extracted information.</p> <p>Now that you know what AI Builder is and why you should use it, let's look at how you can use the Invoice Processing AI Model in AI Builder, that we covered earlier on, to build a workflow that will help the finance team process invoices.</p> <p>To build a workflow that will help the finance team process invoices using the Invoice Processing AI Model in AI Builder, follow the steps below:</p> <ol> <li> <p>Navigate to the Power Automate home screen.</p> </li> <li> <p>Use the text area on the home screen to describe the workflow you want to build. For example, Process an invoice when it arrives in my mailbox. Click on the Send button to send the prompt to the AI Copilot.</p> </li> </ol> <p></p> <ol> <li> <p>The AI Copilot will suggest the actions you need to perform the task you want to automate. You can click on the Next button to go through the next steps.</p> </li> <li> <p>On the next step, Power Automate will prompt you to set up the connections required for the flow. Once you are done, click on the Create flow button to create the flow.</p> </li> <li> <p>The AI Copilot will generate a flow and you can then customize the flow to meet your needs.</p> </li> <li> <p>Update the trigger of the flow and set the Folder to the folder where the invoices will be stored. For example, you can set the folder to Inbox. Click on Show advanced options and set the Only with Attachments to Yes. This will ensure that the flow only runs when an email with an attachment is received in the folder.</p> </li> <li> <p>Remove the following actions from the flow: HTML to text, Compose, Compose 2, Compose 3 and Compose 4 because you will not be using them.</p> </li> <li> <p>Remove the Condition action from the flow because you will not be using it. It should look like the following screenshot:</p> </li> </ol> <p></p> <ol> <li> <p>Click on the Add an action button and search for Dataverse. Select the Add a new row action.</p> </li> <li> <p>On the Extract Information from invoices action, update the Invoice File to point to the Attachment Content from the email. This will ensure that the flow extracts information from the invoice attachment.</p> </li> <li> <p>Select the Table you created earlier on. For example, you can select the Invoice Information table. Choose the dynamic content from the previous action to populate the following fields:</p> <ul> <li>ID</li> <li>Amount</li> <li>Date</li> <li>Name</li> <li>Status - Set the Status to Pending.</li> <li>Supplier Email - Use the From dynamic content from the When a new email arrives trigger.</li> </ul> <p></p> </li> <li> <p>Once you are done with the flow, click on the Save button to save the flow. You can then test the flow by sending an email with an invoice to the folder you specified in the trigger.</p> </li> </ol> <p>Your homework: The flow you just built is a good start, now you need to think of how you can build an automation that will enable our finance team to send an email to the supplier to update them with the current status of their invoice. Your hint: the flow must run when the status of the invoice changes.</p>"},{"location":"lessons/generative-ai-for-beginners/10-building-low-code-ai-applications/#use-a-text-generation-ai-model-in-power-automate","title":"Use a Text Generation AI Model in Power Automate","text":"<p>The Create Text with GPT AI Model in AI Builder enables you to generate text based on a prompt and is powered by the Microsoft Azure OpenAI Service. With this capability, you can incorporate GPT (Generative Pre-Trained Transformer) technology into your apps and flows to build a variety of automated flows and insightful applications.</p> <p>GPT models undergo extensive training on vast amounts of data, enabling them to produce text that closely resembles human language when provided with a prompt. When integrated with workflow automation, AI models like GPT can be harnessed to streamline and automate a wide range of tasks.</p> <p>For example, you can build flows to automatically generate text for a variety of use cases, such as: drafts of emails, product descriptions, and more. You can also use the model to generate text for a variety of apps, such as chatbots and customer service apps that enable customer service agents to respond effectively and efficiently to customer inquiries.</p> <p></p> <p>To learn how to use this AI Model in Power Automate, go through the Add intelligence with AI Builder and GPT module.</p>"},{"location":"lessons/generative-ai-for-beginners/10-building-low-code-ai-applications/#great-work-continue-your-learning","title":"Great Work! Continue Your Learning","text":"<p>After completing this lesson, check out our Generative AI Learning collection to continue leveling up your Generative AI knowledge!</p> <p>Head over to Lesson 11 where we will look at how to integrate Generative AI with Function Calling!</p>"},{"location":"lessons/generative-ai-for-beginners/11-integrating-with-function-calling/","title":"Lesson 11","text":""},{"location":"lessons/generative-ai-for-beginners/11-integrating-with-function-calling/#integrating-with-function-calling","title":"Integrating with function calling","text":"<p>You've learned a fair bit so far in the previous lessons. However, we can improve further. Some things we can address are how we can get a more consistent response format to make it easier to work with the response downstream. Also, we might want to add data from other sources to further enrich our application.</p> <p>The above mentioned problems are what this chapter is looking to address.</p>"},{"location":"lessons/generative-ai-for-beginners/11-integrating-with-function-calling/#introduction","title":"Introduction","text":"<p>This lesson will cover:</p> <ul> <li>Explain what is function calling and its use cases.</li> <li>Creating a function call using Azure OpenAI.</li> <li>How to integrate a function call into an application.</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/11-integrating-with-function-calling/#learning-goals","title":"Learning Goals","text":"<p>By the end of this lesson, you will be able to:</p> <ul> <li>Explain the purpose of using function calling.</li> <li>Setup Function Call using the Azure OpenAI Service.</li> <li>Design effective function calls for your application's use case.</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/11-integrating-with-function-calling/#scenario-improving-our-chatbot-with-functions","title":"Scenario: improving our chatbot with functions","text":"<p>For this lesson, we want to build a feature for our education startup that allows users to use a chatbot to find technical courses. We will recommend courses that fit their skill level, current role and technology of interest.</p> <p>To complete this scenario we will use a combination of:</p> <ul> <li><code>Azure OpenAI</code> to create a chat experience for the user.</li> <li><code>Microsoft Learn Catalog API</code> to help users find courses based on the request of the user.</li> <li><code>Function Calling</code> to take the user's query and send it to a function to make the API request.</li> </ul> <p>To get started, let's look at why we would want to use function calling in the first place:</p>"},{"location":"lessons/generative-ai-for-beginners/11-integrating-with-function-calling/#why-function-calling","title":"Why Function Calling","text":"<p>Before function calling, responses from an LLM were unstructured and inconsistent. Developers were required to write complex validation code to make sure they are able to handle each variation of a response. Users could not get answers like \"What is the current weather in Stockholm?\". This is because models were limited to the time the data was trained on.</p> <p>Function Calling is a feature of the Azure OpenAI Service to overcome to the following limitations:</p> <ul> <li>Consistent response format. If we can better control the response format we can more easily integrate the response downstream to other systems.</li> <li>External data. Ability to use data from other sources of an application in a chat context.</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/11-integrating-with-function-calling/#illustrating-the-problem-through-a-scenario","title":"Illustrating the problem through a scenario","text":"<p>We recommend you to use the included notebook if you want to run the below scenario. You can also just read along as we're trying to illustrate a problem where functions can help to address the problem.</p> <p>Let's look at the example that illustrates the response format problem:</p> <p>Let's say we want to create a database of student data so we can suggest the right course to them. Below we have two descriptions of students that are very similar in the data they contain.</p> <ol> <li>Create a connection to our Azure OpenAI resource:</li> </ol> <pre><code>import os\nimport json\nfrom openai import AzureOpenAI\nfrom dotenv import load_dotenv\nload_dotenv()\n\nclient = AzureOpenAI(\napi_key=os.environ['AZURE_OPENAI_API_KEY'],  # this is also the default, it can be omitted\napi_version = \"2023-07-01-preview\"\n)\n\ndeployment=os.environ['AZURE_OPENAI_DEPLOYMENT']\n</code></pre> <p>Below is some Python code for configuring our connection to Azure OpenAI where we set <code>api_type</code>, <code>api_base</code>, <code>api_version</code> and <code>api_key</code>.</p> <ol> <li>Creating two student descriptions using variables <code>student_1_description</code> and <code>student_2_description</code>.</li> </ol> <pre><code>student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n\nstudent_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finishing his studies.\"\n</code></pre> <p>We want to send the above student descriptions to an LLM to parse the data. This data can later be used in our application and be sent to an API or stored in a database.</p> <ol> <li>Let's create two identical prompts in which we instruct the LLM on what information we are interested in:</li> </ol> <pre><code>prompt1 = f'''\nPlease extract the following information from the given text and return it as a JSON object:\n\nname\nmajor\nschool\ngrades\nclub\n\nThis is the body of text to extract the information from:\n{student_1_description}\n'''\n\nprompt2 = f'''\nPlease extract the following information from the given text and return it as a JSON object:\n\nname\nmajor\nschool\ngrades\nclub\n\nThis is the body of text to extract the information from:\n{student_2_description}\n'''\n</code></pre> <p>The above prompts instruct the LLM to extract information and return the response in JSON format.</p> <ol> <li>After setting up the prompts and the connection to Azure OpenAI, we will now send the prompts to the LLM by using <code>openai.ChatCompletion</code>. We store the prompt in the <code>messages</code> variable and assign the role to <code>user</code>. This is to mimic a message from a user being written to a chatbot.</li> </ol> <pre><code># response from prompt one\nopenai_response1 = client.chat.completions.create(\nmodel=deployment,\nmessages = [{'role': 'user', 'content': prompt1}]\n)\nopenai_response1.choices[0].message.content\n\n# response from prompt two\nopenai_response2 = client.chat.completions.create(\nmodel=deployment,\nmessages = [{'role': 'user', 'content': prompt2}]\n)\nopenai_response2.choices[0].message.content\n</code></pre> <p>Now we can send both requests to the LLM and examine the response we receive by finding it like so <code>openai_response1['choices'][0]['message']['content']</code>.</p> <ol> <li>Lastly, we can convert the response to JSON format by calling <code>json.loads</code>:</li> </ol> <pre><code># Loading the response as a JSON object\njson_response1 = json.loads(openai_response1.choices[0].message.content)\njson_response1\n</code></pre> <p>Response 1:</p> <pre><code>{\n  \"name\": \"Emily Johnson\",\n  \"major\": \"computer science\",\n  \"school\": \"Duke University\",\n  \"grades\": \"3.7\",\n  \"club\": \"Chess Club\"\n}\n</code></pre> <p>Response 2:</p> <pre><code>{\n  \"name\": \"Michael Lee\",\n  \"major\": \"computer science\",\n  \"school\": \"Stanford University\",\n  \"grades\": \"3.8 GPA\",\n  \"club\": \"Robotics Club\"\n}\n</code></pre> <p>Even though the prompts are the same and the descriptions are similar, we see values of the <code>Grades</code> property formatted differently as we can sometimes get the format <code>3.7</code> or <code>3.7 GPA</code> for example.</p> <p>This result is because the LLM takes unstructured data in the form of the written prompt and returns also unstructured data. We need to have a structured format so that we know what to expect when storing or using this data</p> <p>So how do we solve the formatting problem then? By using functional calling, we can make sure that we receive structured data back. When using function calling, the LLM does not actually call or run any functions. Instead, we create a structure for the LLM to follow for its responses. We then use those structured responses to know what function to run in our applications.</p> <p></p> <p>We can then take what is returned from the function and send this back to the LLM. The LLM will then respond using natural language to answer the user's query.</p>"},{"location":"lessons/generative-ai-for-beginners/11-integrating-with-function-calling/#use-cases-for-using-function-calls","title":"Use Cases for using function calls","text":"<p>There are many different use cases where function calls can improve your app like:</p> <ul> <li> <p>Calling External Tools. Chatbots are great at providing answers to questions from users. By using function calling, the chatbots can use messages from users to complete certain tasks. For example, a student can ask the chatbot to \"Send email to my instructor saying I need more assistance with this subject\". This can make a function call to <code>send_email(to: string, body: string)</code></p> </li> <li> <p>Create API or Database Queries. Users can find information using natural language that gets converted into a formatted query or API request. An example of this could be a teacher who requests \"Who are the students that completed the last assignment\" which could call a function named <code>get_completed(student_name: string, assignment: int, current_status: string)</code></p> </li> <li> <p>Creating Structured Data. Users can take a block of text or CSV and use the LLM to extract important information from it. For example, a student can convert a Wikipedia article about peace agreements to create AI flash cards. This can be done by using a function called <code>get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)</code></p> </li> </ul>"},{"location":"lessons/generative-ai-for-beginners/11-integrating-with-function-calling/#creating-your-first-function-call","title":"Creating Your First Function Call","text":"<p>The process of creating a function call includes 3 main steps:</p> <ol> <li>Calling the Chat Completions API with a list of your functions and a user message.</li> <li>Reading the model's response to perform an action ie execute a function or API Call.</li> <li>Making another call to Chat Completions API with the response from your function to use that information to create a response to the user.</li> </ol> <p></p>"},{"location":"lessons/generative-ai-for-beginners/11-integrating-with-function-calling/#step-1-creating-messages","title":"Step 1 - creating messages","text":"<p>The first step is to create a user message. This can be dynamically assigned by taking the value of a text input or you can assign a value here. If this is your first time working with the Chat Completions API, we need to define the <code>role</code> and the <code>content</code> of the message.</p> <p>The <code>role</code> can be either <code>system</code> (creating rules), <code>assistant</code> (the model) or <code>user</code> (the end-user). For function calling, we will assign this as <code>user</code> and an example question.</p> <pre><code>messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]\n</code></pre> <p>By assigning different roles, it's made clear to the LLM if it's the system saying something or the user, which helps to build a conversation history that the LLM can build upon.</p>"},{"location":"lessons/generative-ai-for-beginners/11-integrating-with-function-calling/#step-2-creating-functions","title":"Step 2 - creating functions","text":"<p>Next, we will define a function and the parameters of that function. We will use just one function here called <code>search_courses</code> but you can create multiple functions.</p> <p>Important : Functions are included in the system message to the LLM and will be included in the amount of available tokens you have available.</p> <p>Below, we create the functions as an array of items. Each item is a function and has properties <code>name</code>, <code>description</code> and <code>parameters</code>:</p> <pre><code>functions = [\n   {\n      \"name\":\"search_courses\",\n      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n      \"parameters\":{\n         \"type\":\"object\",\n         \"properties\":{\n            \"role\":{\n               \"type\":\"string\",\n               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n            },\n            \"product\":{\n               \"type\":\"string\",\n               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n            },\n            \"level\":{\n               \"type\":\"string\",\n               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n            }\n         },\n         \"required\":[\n            \"role\"\n         ]\n      }\n   }\n]\n</code></pre> <p>Let's describe each function instance more in detail below:</p> <ul> <li><code>name</code> - The name of the function that we want to have called.</li> <li><code>description</code> - This is the description of how the function works. Here it's important to be specific and clear.</li> <li><code>parameters</code> - A list of values and format that you want the model to produce in its response. The parameters array consists of items where item have the following properties:</li> <li><code>type</code> - The data type of the properties will be stored in.</li> <li><code>properties</code> - List of the specific values that the model will use for its response<ol> <li><code>name</code> - The key is the name of the property that the model will use in its formatted response, for example, <code>product</code>.</li> <li><code>type</code> - The data type of this property, for example, <code>string</code>.</li> <li><code>description</code> - Description of the specific property.</li> </ol> </li> </ul> <p>There's also an optional property <code>required</code> - required property for the function call to be completed.</p>"},{"location":"lessons/generative-ai-for-beginners/11-integrating-with-function-calling/#step-3-making-the-function-call","title":"Step 3 - Making the function call","text":"<p>After defining a function, we now need to include it in the call to the Chat Completion API. We do this by adding <code>functions</code> to the request. In this case <code>functions=functions</code>.</p> <p>There is also an option to set <code>function_call</code> to <code>auto</code>. This means we will let the LLM decide which function should be called based on the user message rather than assigning it ourselves.</p> <p>Here's some code below where we call <code>ChatCompletion.create</code>, note how we set <code>functions=functions</code> and <code>function_call=\"auto\"</code> and thereby giving the LLM the choice when to call the functions we provide it:</p> <pre><code>response = client.chat.completions.create(model=deployment,\n                                        messages=messages,\n                                        functions=functions,\n                                        function_call=\"auto\")\n\nprint(response.choices[0].message)\n</code></pre> <p>The response coming back now looks like so:</p> <pre><code>{\n  \"role\": \"assistant\",\n  \"function_call\": {\n    \"name\": \"search_courses\",\n    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n  }\n}\n</code></pre> <p>Here we can see how the function <code>search_courses</code> was called and with what arguments, as listed in the <code>arguments</code> property in the JSON response.</p> <p>The conclusion the LLM was able to find the data to fit the arguments of the function as it was extracting it from the value provided to the <code>messages</code> parameter in the chat completion call. Below is a reminder of the <code>messages</code> value:</p> <pre><code>messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]\n</code></pre> <p>As you can see, <code>student</code>, <code>Azure</code> and <code>beginner</code> was extracted from <code>messages</code> and set as input to the function. Using functions this way is a great way to extract information from a prompt but also to provide structure to the LLM and have reusable functionality.</p> <p>Next, we need to see how we can use this in our app.</p>"},{"location":"lessons/generative-ai-for-beginners/11-integrating-with-function-calling/#integrating-function-calls-into-an-application","title":"Integrating Function Calls into an Application","text":"<p>After we have tested the formatted response from the LLM, now we can integrate this into an application.</p>"},{"location":"lessons/generative-ai-for-beginners/11-integrating-with-function-calling/#managing-the-flow","title":"Managing the flow","text":"<p>To integrate this into our application, let's take the following steps:</p> <ol> <li>First, let's make the call to the Open AI services and store the message in a variable called <code>response_message</code>.</li> </ol> <pre><code>response_message = response.choices[0].message\n</code></pre> <ol> <li>Now we will define the function that will call the Microsoft Learn API to get a list of courses:</li> </ol> <pre><code>import requests\n\ndef search_courses(role, product, level):\n  url = \"https://learn.microsoft.com/api/catalog/\"\n  params = {\n     \"role\": role,\n     \"product\": product,\n     \"level\": level\n  }\n  response = requests.get(url, params=params)\n  modules = response.json()[\"modules\"]\n  results = []\n  for module in modules[:5]:\n     title = module[\"title\"]\n     url = module[\"url\"]\n     results.append({\"title\": title, \"url\": url})\n  return str(results)\n</code></pre> <p>Note how we now create an actual Python function that maps to the function names introduced in the <code>functions</code> variable. We're also making real external API calls to fetch the data we need. In this case, we go against the Microsoft Learn API to search for training modules.</p> <p>Ok, so we created <code>functions</code> variables and a corresponding Python function, how do we tell the LLM how to map these two together so our Python function is called?</p> <ol> <li>To see if we need to call a Python function, we need to look into the LLM response and see if <code>function_call</code> is part of it and call the pointed out function. Here's how you can make the mentioned check below:</li> </ol> <pre><code># Check if the model wants to call a function\nif response_message.function_call.name:\n print(\"Recommended Function call:\")\n print(response_message.function_call.name)\n print()\n\n # Call the function.\n function_name = response_message.function_call.name\n\n available_functions = {\n         \"search_courses\": search_courses,\n }\n function_to_call = available_functions[function_name]\n\n function_args = json.loads(response_message.function_call.arguments)\n function_response = function_to_call(**function_args)\n\n print(\"Output of function call:\")\n print(function_response)\n print(type(function_response))\n\n\n # Add the assistant response and function response to the messages\n messages.append( # adding assistant response to messages\n     {\n         \"role\": response_message.role,\n         \"function_call\": {\n             \"name\": function_name,\n             \"arguments\": response_message.function_call.arguments,\n         },\n         \"content\": None\n     }\n )\n messages.append( # adding function response to messages\n     {\n         \"role\": \"function\",\n         \"name\": function_name,\n         \"content\":function_response,\n     }\n )\n</code></pre> <p>These three lines, ensure we extract the function name, the arguments and make the call:</p> <pre><code>function_to_call = available_functions[function_name]\n\nfunction_args = json.loads(response_message.function_call.arguments)\nfunction_response = function_to_call(**function_args)\n</code></pre> <p>Below is the output from running our code:</p> <p>Output</p> <p>```Recommended Function call:    {      \"name\": \"search_courses\",      \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"    }</p> <p>Output of function call:    [{'title': 'Describe concepts of cryptography', 'url': 'https://learn.microsoft.com/training/modules/describe-concepts-of-cryptography/?    WT.mc_id=api_CatalogApi'}, {'title': 'Introduction to audio classification with TensorFlow', 'url': 'https://learn.microsoft.com/en-    us/training/modules/intro-audio-classification-tensorflow/?WT.mc_id=api_CatalogApi'}, {'title': 'Design a Performant Data Model in Azure SQL    Database with Azure Data Studio', 'url': 'https://learn.microsoft.com/training/modules/design-a-data-model-with-ads/?    WT.mc_id=api_CatalogApi'}, {'title': 'Getting started with the Microsoft Cloud Adoption Framework for Azure', 'url':    'https://learn.microsoft.com/training/modules/cloud-adoption-framework-getting-started/?WT.mc_id=api_CatalogApi'}, {'title': 'Set up the    Rust development environment', 'url': 'https://learn.microsoft.com/training/modules/rust-set-up-environment/?WT.mc_id=api_CatalogApi'}]        ```</p> <ol> <li>Now we will send the updated message, <code>messages</code> to the LLM so we can receive a natural language response instead of an API JSON formatted response.</li> </ol> <pre><code>print(\"Messages in next request:\")\nprint(messages)\nprint()\n\nsecond_response = client.chat.completions.create(\n   messages=messages,\n   model=deployment,\n   function_call=\"auto\",\n   functions=functions,\n   temperature=0\n      )  # get a new response from GPT where it can see the function response\n\n\nprint(second_response.choices[0].message)\n</code></pre> <p>Output</p> <pre><code>{\n  \"role\": \"assistant\",\n  \"content\": \"I found some good courses for beginner students to learn Azure:\\n\\n1. [Describe concepts of cryptography] (https://learn.microsoft.com/training/modules/describe-concepts-of-cryptography/?WT.mc_id=api_CatalogApi)\\n2. [Introduction to audio classification with TensorFlow](https://learn.microsoft.com/training/modules/intro-audio-classification-tensorflow/?WT.mc_id=api_CatalogApi)\\n3. [Design a Performant Data Model in Azure SQL Database with Azure Data Studio](https://learn.microsoft.com/training/modules/design-a-data-model-with-ads/?WT.mc_id=api_CatalogApi)\\n4. [Getting started with the Microsoft Cloud Adoption Framework for Azure](https://learn.microsoft.com/training/modules/cloud-adoption-framework-getting-started/?WT.mc_id=api_CatalogApi)\\n5. [Set up the Rust development environment](https://learn.microsoft.com/training/modules/rust-set-up-environment/?WT.mc_id=api_CatalogApi)\\n\\nYou can click on the links to access the courses.\"\n}\n</code></pre>"},{"location":"lessons/generative-ai-for-beginners/11-integrating-with-function-calling/#assignment","title":"Assignment","text":"<p>To continue your learning of Azure OpenAI Function Calling you can build:</p> <ul> <li>More parameters of the function that might help learners find more courses.</li> <li>Create another function call that takes more information from the learner like their native language</li> <li>Create error handling when the function call and/or API call does not return any suitable courses</li> </ul> <p>Hint: Follow the Learn API reference documentation page to see how and where this data is available.</p>"},{"location":"lessons/generative-ai-for-beginners/11-integrating-with-function-calling/#great-work-continue-the-journey","title":"Great Work! Continue the Journey","text":"<p>After completing this lesson, check out our Generative AI Learning collection to continue leveling up your Generative AI knowledge!</p> <p>Head over to Lesson 12 where we will look at how to design UX for AI applications!</p>"},{"location":"lessons/generative-ai-for-beginners/12-designing-ux-for-ai-applications/","title":"Lesson 12","text":""},{"location":"lessons/generative-ai-for-beginners/12-designing-ux-for-ai-applications/#designing-ux-for-ai-applications","title":"Designing UX for AI Applications","text":"<p>(Click the image above to view video of this lesson)</p> <p>User experience is a very important aspect of building apps. Users need to be able to use your app in an efficient way to perform tasks. Being efficient is one thing but you also need to design apps so that they can be used by everyone, to make them accessible. This chapter will focus on this area so you hopefully end up designing an app that people can and want to use.</p>"},{"location":"lessons/generative-ai-for-beginners/12-designing-ux-for-ai-applications/#introduction","title":"Introduction","text":"<p>User experience is how a user interacts with and uses a specific product or service be it a system, tool, or design. When developing AI applications, developers not only focus on ensuring the user experience is effective but also ethical. In this lesson, we cover how to build Artificial Intelligence (AI) applications that address user needs.</p> <p>The lesson will cover the following areas:</p> <ul> <li>Introduction to User Experience and Understanding User Needs</li> <li>Designing AI Applications for Trust and Transparency</li> <li>Designing AI Applications for Collaboration and Feedback</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/12-designing-ux-for-ai-applications/#learning-goals","title":"Learning goals","text":"<p>After taking this lesson, you'll be able to:</p> <ul> <li>Understand how to build AI applications that meet the user needs.</li> <li>Design AI applications that promote trust and collaboration.</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/12-designing-ux-for-ai-applications/#prerequisite","title":"Prerequisite","text":"<p>Take some time and read more about user experience and design thinking.</p>"},{"location":"lessons/generative-ai-for-beginners/12-designing-ux-for-ai-applications/#introduction-to-user-experience-and-understanding-user-needs","title":"Introduction to User Experience and Understanding User Needs","text":"<p>In our fictitious education startup, we have two primary users, teachers and students. Each of the two users has unique needs. A user-centered design prioritizes the user ensuring the products are relevant and beneficial for those it is intended for.</p> <p>The application should be useful, reliable, accessible and pleasant to provide a good user experience.</p>"},{"location":"lessons/generative-ai-for-beginners/12-designing-ux-for-ai-applications/#usability","title":"Usability","text":"<p>Being useful means that the application has functionality that matches its intended purpose, such as automating the grading process or generating flashcards for revision. An application that automates the grading process should be able to accurately and efficiently assign scores to students' work based on predefined criteria. Similarly, an application that generates revision flashcards should be able to create relevant and diverse questions based on its data.</p>"},{"location":"lessons/generative-ai-for-beginners/12-designing-ux-for-ai-applications/#reliability","title":"Reliability","text":"<p>Being reliable means that the application can perform its task consistently and without errors. However, AI just like humans is not perfect and may be prone to errors. The applications may encounter errors or unexpected situations that require human intervention or correction. How do you handle errors? In the last section of this lesson, we will cover how AI systems and applications are designed for collaboration and feedback.</p>"},{"location":"lessons/generative-ai-for-beginners/12-designing-ux-for-ai-applications/#accessibility","title":"Accessibility","text":"<p>Being accessible means extending the user experience to users with various abilities, including those with disabilities, ensuring no one is left out. By following accessibility guidelines and principles, AI solutions become more inclusive, usable, and beneficial for all users.</p>"},{"location":"lessons/generative-ai-for-beginners/12-designing-ux-for-ai-applications/#pleasant","title":"Pleasant","text":"<p>Being pleasant means that the application is enjoyable to use. An appealing user experience can have a positive impact on the user encouraging them to return to the application and increasing business revenue.</p> <p></p> <p>Not every challenge can be solved with AI. AI comes in to augment your user experience, be it automating manual tasks, or personalizing user experiences.</p>"},{"location":"lessons/generative-ai-for-beginners/12-designing-ux-for-ai-applications/#designing-ai-applications-for-trust-and-transparency","title":"Designing AI Applications for Trust and Transparency","text":"<p>Building trust is critical when designing AI applications. Trust ensures a user is confident that the application will get the work done, deliver results consistently and the results are what the user needs. A risk in this area is mistrust and over trust. Mistrust occurs when a user has little or no trust in an AI system, this leads to the user rejecting your application. Over trust occurs when a user overestimates the capability of an AI system, leading to users trusting the AI system too much. For example, an automated grading system in the case of over trust might lead the teacher not to proof through some of the papers to ensure the grading system works well. This could result in unfair or inaccurate grades for the students, or missed opportunities for feedback and improvement.</p> <p>Two ways to ensure that trust is put right at the center of design are explainability and control.</p>"},{"location":"lessons/generative-ai-for-beginners/12-designing-ux-for-ai-applications/#explainability","title":"Explainability","text":"<p>When AI helps inform decisions such as imparting knowledge to future generations, it is critical for teachers and parents to understand how AI decisions are made. This is explainability - understanding how AI applications make decisions. Designing for explainability includes adding details of examples of what an AI application can do. For example, instead of \"Get started with AI teacher\", the system can use: \"Summarize your notes for easier revision using AI.\"</p> <p></p> <p>Another example is how AI uses user and personal data. For example, a user with the persona student may have limitations based on their persona. The AI may not be able to reveal answers to questions but may help guide the user to think through how they can solve a problem.</p> <p></p> <p>One last key part of explainability is the simplification of explanations. Students and teachers may not be AI experts, therefore explanations of what the application can or cannot do should be simplified and easy to understand.</p> <p></p>"},{"location":"lessons/generative-ai-for-beginners/12-designing-ux-for-ai-applications/#control","title":"Control","text":"<p>Generative AI creates a collaboration between AI and the user, where for instance a user can modify prompts for different results. Additionally, once an output is generated, users should be able to modify the results giving them a sense of control. For example, when using Bing, you can tailor your prompt based on format, tone and length. Additionally, you can add changes to your output and modify the output as shown below:</p> <p></p> <p>Another feature in Bing that allows a user to have control over the application is the ability to opt in and opt out of the data AI uses. For a school application, a student might want to use their notes as well as the teachers' resources as revision material.</p> <p></p> <p>When designing AI applications, intentionality is key in ensuring users do not over trust setting unrealistic expectations of its capabilities. One way to do this is by creating friction between the prompts and the results. Reminding the user, that this is AI and not a fellow human being</p>"},{"location":"lessons/generative-ai-for-beginners/12-designing-ux-for-ai-applications/#designing-ai-applications-for-collaboration-and-feedback","title":"Designing AI Applications for Collaboration and Feedback","text":"<p>As earlier mentioned generative AI creates a collaboration between the user and AI. Most engagements are with a user inputting a prompt and the AI generating an output. What if the output is incorrect? How does the application handle errors if they occur? Does the AI blame the user or take time to explain the error?</p> <p>AI applications should be built in to receive and give feedback. This not only helps the AI system improve but also builds trust with the users. A feedback loop should be included in the design, an example can be a simple thumbs up or down on the output.</p> <p>Another way to handle this is to clearly communicate the capabilities and limitations of the system. When a user makes an error requesting something beyond the AI capabilities, there should also be a way to handle this, as shown below.</p> <p></p> <p>System errors are common with applications where the user might need assistance with information outside the scope of the AI or the application may have a limit of how many questions/subjects a user can generate summaries. For example, an AI application trained with data on limited subjects for example, History and Math may not be able to handle questions around Geography. To mitigate this, the AI system can give a response like: \"Sorry, our product has been trained with data in the following subjects....., I cannot be able to respond to the question you asked.\"</p> <p>AI applications are not perfect, therefore, they are bound to make mistakes. When designing your applications, you should ensure you create room for feedback from users and error handling in a way that is simple and easily explainable.</p>"},{"location":"lessons/generative-ai-for-beginners/12-designing-ux-for-ai-applications/#assignment","title":"Assignment","text":"<p>Take any AI apps you've built so far, consider implementing the below steps in your app:</p> <ul> <li> <p>Pleasant: Consider how you can make your app more pleasant. Are you adding explanations everywhere, are you encouraging the user to explore? How are you wording your error messages?</p> </li> <li> <p>Usability: Building a web app. Make sure your app is navigable by both mouse and keyboard.</p> </li> <li> <p>Trust and transparency: Don't trust the AI completely and its output, consider how you would add a human to the process to verify the output. Also, consider and implement other ways to achieve trust and transparency.</p> </li> <li> <p>Control: Give the user control of the data they provide to the application. Implement a way a user can opt-in and opt-out of data collection in the AI application.</p> </li> </ul>"},{"location":"lessons/generative-ai-for-beginners/12-designing-ux-for-ai-applications/#continue-your-learning","title":"Continue Your Learning!","text":"<p>After completing this lesson, check out our Generative AI Learning collection to continue leveling up your Generative AI knowledge!</p> <p>Head over to Lesson 13 where we will look at how to securing AI applications!</p>"},{"location":"lessons/generative-ai-for-beginners/13-securing-ai-applications/","title":"Lesson 13","text":""},{"location":"lessons/generative-ai-for-beginners/13-securing-ai-applications/#securing-your-generative-ai-applications","title":"Securing Your Generative AI Applications","text":""},{"location":"lessons/generative-ai-for-beginners/13-securing-ai-applications/#introduction","title":"Introduction","text":"<p>This lesson will cover:</p> <ul> <li>Security within the context of AI systems.</li> <li>Common risks and threats to AI systems.</li> <li>Methods and considerations for securing AI systems.</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/13-securing-ai-applications/#learning-goals","title":"Learning Goals","text":"<p>After completing this lesson, you will have an understanding of:</p> <ul> <li>The threats and risks to AI systems.</li> <li>Common methods and practices for securing AI systems.</li> <li>How implementing security testing can prevent unexpected results and erosion of user trust.</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/13-securing-ai-applications/#what-does-security-mean-within-the-context-of-generative-ai","title":"What does security mean within the context of generative AI?","text":"<p>As Artificial Intelligence (AI) and Machine Learning (ML) technologies increasingly shape our lives, it\u2019s crucial to protect not only customer data but also the AI systems themselves. AI/ML is increasingly used in support of high-value decision-making processes in industries where the wrong decision may result in serious consequences.</p> <p>Here are key points to consider:</p> <ul> <li>Impact of AI/ML: AI/ML have significant impacts on daily life and as such safeguarding them has become essential.</li> <li>Security Challenges: This impact that AI/ML has needs proper attention in order to address the need to protect AI-based products from sophisticated attacks, whether by trolls or organized groups.</li> <li>Strategic Problems: The tech industry must proactively address strategic challenges to ensure long-term customer safety and data security.</li> </ul> <p>Additionally, Machine Learning models are largely unable to discern between malicious input and benign anomalous data. A significant source of training data is derived from uncurated, unmoderated, public datasets, which are open to 3rd-party contributions. Attackers don\u2019t need to compromise datasets when they're free to contribute to them. Over time, low-confidence malicious data becomes high-confidence trusted data, if the data structure/formatting remains correct.</p> <p>This is why it is critical to ensure the integrity and protection of the data stores your models use to make decisions with.</p>"},{"location":"lessons/generative-ai-for-beginners/13-securing-ai-applications/#understanding-the-threats-and-risks-of-ai","title":"Understanding the threats and risks of AI","text":"<p>In terms of AI and related systems, data poisoning stands out as the most significant security threat today. Data poisoning is when someone intentionally changes the information used to train an AI, causing it to make mistakes. This is due to the absence of standardized detection and mitigation methods, coupled with our reliance on untrusted or uncurated public datasets for training. To maintain data integrity and prevent a flawed training process, it is crucial to track the origin and lineage of your data. Otherwise, the old adage \u201cgarbage in, garbage out\u201d holds true, leading to compromised model performance.</p> <p>Here are examples of how data poisoning can affect your models:</p> <ol> <li>Label Flipping: In a binary classification task, an adversary intentionally flips the labels of a small subset of training data. For instance, benign samples are labeled as malicious, leading the model to learn incorrect associations.\\    Example: A spam filter misclassifying legitimate emails as spam due to manipulated labels.</li> <li>Feature Poisoning: An attacker subtly modifies features in the training data to introduce bias or mislead the model.\\    Example: Adding irrelevant keywords to product descriptions to manipulate recommendation systems.</li> <li>Data Injection: Injecting malicious data into the training set to influence the model\u2019s behavior.\\    Example: Introducing fake user reviews to skew sentiment analysis results.</li> <li>Backdoor Attacks: An adversary inserts a hidden pattern (backdoor) into the training data. The model learns to recognize this pattern and behaves maliciously when triggered.\\    Example: A face recognition system trained with backdoored images that misidentifies a specific person.</li> </ol> <p>The MITRE Corporation has created ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems), a knowledgebase of tactics and techniques employed by adversaries in real-world attacks on AI systems.</p> <p>There are a growing number of vulnerabilities in AI-enabled systems, as the incorporation of AI increases the attack surface of existing systems beyond those of traditional cyber-attacks. We developed ATLAS to raise awareness of these unique and evolving vulnerabilities, as the global community increasingly incorporates AI into various systems. ATLAS is modeled after the MITRE ATT&amp;CK\u00ae framework and its tactics, techniques, and procedures (TTPs) are complementary to those in ATT&amp;CK.</p> <p>Much like the MITRE ATT&amp;CK\u00ae framework, which is extensively used in traditional cybersecurity for planning advanced threat emulation scenarios, ATLAS provides an easily searchable set TTPs that can help to better understand and prepare for defending against emerging attacks.</p> <p>Additionally, the Open Web Application Security Project (OWASP) has created a \"Top 10 list\" of the most critical vulnerabilities found in applications utilizing LLMs. The list highlights the risks of threats such as the aforementioned data poisoning along with others such as:</p> <ul> <li>Prompt Injection: a technique where attackers manipulate a Large Language Model (LLM) through carefully crafted inputs, causing it to behave outside of its intended behavior.</li> <li>Supply Chain Vulnerabilities: The components and software that make up the applications used by an LLM, such as Python modules or external datasets, can themselves be compromised leading to unexpected results, introduced biases and even vulnerabilities in the underlying infrastructure.</li> <li>Overreliance: LLMs are fallible and have been prone to hallucinate, providing inaccurate or unsafe results. In several documented circumstances, people have taken the results at face value leading to unintended real-world negative consequences.</li> </ul> <p>Microsoft Cloud Advocate Rod Trent has written a free ebook, Must Learn AI Security, that dives deeply into these and other emerging AI threats and provides extensive guidance on how to best tackle these scenarios.</p>"},{"location":"lessons/generative-ai-for-beginners/13-securing-ai-applications/#security-testing-for-ai-systems-and-llms","title":"Security Testing for AI Systems and LLMs","text":"<p>Artificial intelligence (AI) is transforming various domains and industries, offering new possibilities and benefits for society. However, AI also poses significant challenges and risks, such as data privacy, bias, lack of explainability, and potential misuse. Therefore, it is crucial to ensure that AI systems are secure and responsible, meaning that they adhere to ethical and legal standards and can be trusted by users and stakeholders.</p> <p>Security testing is the process of evaluating the security of an AI system or LLM, by identifying and exploiting their vulnerabilities. This can be performed by developers, users, or third-party auditors, depending on the purpose and scope of the testing. Some of the most common security testing methods for AI systems and LLMs are:</p> <ul> <li>Data sanitization: This is the process of removing or anonymizing sensitive or private information from the training data or the input of an AI system or LLM. Data sanitization can help prevent data leakage and malicious manipulation by reducing the exposure of confidential or personal data.</li> <li>Adversarial testing: This is the process of generating and applying adversarial examples to the input or output of an AI system or LLM to evaluate its robustness and resilience against adversarial attacks. Adversarial testing can help identify and mitigate the vulnerabilities and weaknesses of an AI system or LLM that may be exploited by attackers.</li> <li>Model verification: This is the process of verifying the correctness and completeness of the model parameters or architecture of an AI system or LLM. Model verification can help detect and prevent model stealing by ensuring that the model is protected and authenticated.</li> <li>Output validation: This is the process of validating the quality and reliability of the output of an AI system or LLM. Output validation can help detect and correct malicious manipulation by ensuring that the output is consistent and accurate.</li> </ul> <p>OpenAI, a leader in AI systems, has setup a series of safety evaluations as part of their red teaming network initiative, aimed at testing the output AI systems in the hopes of contributing to AI safety.</p> <p>Evaluations can range from simple Q&amp;A tests to more-complex simulations. As concrete examples, here are sample evaluations developed by OpenAI for evaluating AI behaviors from a number of angles:</p>"},{"location":"lessons/generative-ai-for-beginners/13-securing-ai-applications/#persuasion","title":"Persuasion","text":"<ul> <li>MakeMeSay: How well can an AI system trick another AI system into saying a secret word?</li> <li>MakeMePay: How well can an AI system convince another AI system to donate money?</li> <li>Ballot Proposal: How well can an AI system influence another AI system\u2019s support of a political proposition?</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/13-securing-ai-applications/#steganography-hidden-messaging","title":"Steganography (hidden messaging)","text":"<ul> <li>Steganography: How well can an AI system \u200b\u200bpass secret messages without being caught by another AI system?</li> <li>Text Compression: How well can an AI system compress and decompress messages, to enable hiding secret messages?</li> <li>Schelling Point: How well can an AI system coordinate with another AI system, without direct communication?</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/13-securing-ai-applications/#ai-security","title":"AI Security","text":"<p>It's imperative that we aim to protect AI systems from malicious attacks, misuse, or unintended consequences. This includes taking steps to ensure the safety, reliability, and trustworthiness of AI systems, such as:</p> <ul> <li>Securing the data and algorithms that are used to train and run AI models</li> <li>Preventing unauthorized access, manipulation, or sabotage of AI systems</li> <li>Detecting and mitigating bias, discrimination, or ethical issues in AI systems</li> <li>Ensuring the accountability, transparency, and explainability of AI decisions and actions</li> <li>Aligning the goals and values of AI systems with those of humans and society</li> </ul> <p>AI security is important for ensuring the integrity, availability, and confidentiality of AI systems and data. Some of the challenges and opportunities of AI security are:</p> <ul> <li>Opportunity: Incorporating AI in cybersecurity strategies since it can play a crucial role in identifying threats and improving response times. AI can help automate and augment the detection and mitigation of cyberattacks, such as phishing, malware, or ransomware.</li> <li>Challenge: AI can also be used by adversaries to launch sophisticated attacks, such as generating fake or misleading content, impersonating users, or exploiting vulnerabilities in AI systems. Therefore, AI developers have a unique responsibility to design systems that are robust and resilient against misuse.</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/13-securing-ai-applications/#data-protection","title":"Data Protection","text":"<p>LLMs can pose risks to the privacy and security of the data that they use. For example, LLMs can potentially memorize and leak sensitive information from their training data, such as personal names, addresses, passwords, or credit card numbers. They can also be manipulated or attacked by malicious actors who want to exploit their vulnerabilities or biases. Therefore, it is important to be aware of these risks and take appropriate measures to protect the data used with LLMs. There are several steps that you can take to protect the data that is used with LLMs. These steps include:</p> <ul> <li>Limiting the amount and type of data that they share with LLMs: Only share the data that is necessary and relevant for the intended purposes, and avoid sharing any data that is sensitive, confidential, or personal. Users should also anonymize or encrypt the data that they share with LLMs, such as by removing or masking any identifying information, or using secure communication channels.</li> <li>Verifying the data that LLMs generate: Always check the accuracy and quality of the output generated by LLMs to ensure they don't contain any unwanted or inappropriate information.</li> <li>Reporting and alerting any data breaches or incidents: Be vigilant of any suspicious or abnormal activities or behaviors from LLMs, such as generating texts that are irrelevant, inaccurate, offensive, or harmful. This could be an indication of a data breach or security incident.</li> </ul> <p>Data security, governance, and compliance are critical for any organization that wants to leverage the power of data and AI in a multi-cloud environment. Securing and governing all your data is a complex and multifaceted undertaking. You need to secure and govern different types of data (structured, unstructured, and data generated by AI) in different locations across multiple clouds, and you need to account for existing and future data security, governance, and AI regulations. To protect your data, you need to adopt some best practices and precautions, such as:</p> <ul> <li>Use cloud services or platforms that offer data protection and privacy features.</li> <li>Use data quality and validation tools to check your data for errors, inconsistencies, or anomalies.</li> <li>Use data governance and ethics frameworks to ensure your data is used in a responsible and transparent manner.</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/13-securing-ai-applications/#emulating-real-world-threats-ai-red-teaming","title":"Emulating real-world threats - AI red teaming","text":"<p>Emulating real-world threats is now considered a standard practice in building resilient AI systems by employing similar tools, tactics, procedures to identify the risks to systems and test the response of defenders.</p> <p>The practice of AI red teaming has evolved to take on a more expanded meaning: it not only covers probing for security vulnerabilities, but also includes probing for other system failures, such as the generation of potentially harmful content. AI systems come with new risks, and red teaming is core to understanding those novel risks, such as prompt injection and producing ungrounded content. - Microsoft AI Red Team building future of safer AI</p> <p></p> <p>Below are key insights that have shaped Microsoft\u2019s AI Red Team program.</p> <ol> <li>Expansive Scope of AI Red Teaming:    AI red teaming now encompasses both security and Responsible AI (RAI) outcomes. Traditionally, red teaming focused on security aspects, treating the model as a vector (e.g., stealing the underlying model). However, AI systems introduce novel security vulnerabilities (e.g., prompt injection, poisoning), necessitating special attention. Beyond security, AI red teaming also probes fairness issues (e.g., stereotyping) and harmful content (e.g., glorification of violence). Early identification of these issues allows prioritization of defense investments.</li> <li>Malicious and Benign Failures:    AI red teaming considers failures from both malicious and benign perspectives. For example, when red teaming the new Bing, we explore not only how malicious adversaries can subvert the system but also how regular users may encounter problematic or harmful content. Unlike traditional security red teaming, which focuses mainly on malicious actors, AI red teaming accounts for a broader range of personas and potential failures.</li> <li>Dynamic Nature of AI Systems:    AI applications constantly evolve. In large language model applications, developers adapt to changing requirements. Continuous red teaming ensures ongoing vigilance and adaptation to evolving risks.</li> </ol> <p>AI red teaming is not all encompassing and should be considered a complementary motion to additional controls such as role-based access control (RBAC) and comprehensive data management solutions. It's meant to supplement a security strategy that focuses on employing safe and responsible AI solutions that account for privacy and security while aspiring to minimize biases, harmful content and misinformation that can erode user confidence.</p> <p>Here's a list of additional reading that can help you better understand how red teaming can help identify and mitigate risks in your AI systems:</p> <ul> <li>Planning red teaming for large language models (LLMs) and their applications</li> <li>What is the OpenAI Red Teaming Network?</li> <li>AI Red Teaming - A Key Practice for Building Safer and More Responsible AI Solutions</li> <li>MITRE ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems), a knowledgebase of tactics and techniques employed by adversaries in real-world attacks on AI systems.</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/13-securing-ai-applications/#knowledge-check","title":"Knowledge check","text":"<p>What could be a good approach to maintaining data integrity and preventing misuse?</p> <ol> <li>Have strong role-based controls for data access and data management</li> <li>Implement and audit data labeling to prevent data misrepresentation or misuse</li> <li>Ensure your AI infrastructure supports content filtering</li> </ol> <p>A:1, While all three are great recommendations, ensuring that you're assigning the proper data access privileges to users will go a long way to preventing manipulation and misrepresentation of the data used by LLMs.</p>"},{"location":"lessons/generative-ai-for-beginners/13-securing-ai-applications/#challenge","title":"\ud83d\ude80 Challenge","text":"<p>Read up more on how you can govern and protect sensitive information in the age of AI.</p>"},{"location":"lessons/generative-ai-for-beginners/13-securing-ai-applications/#great-work-continue-your-learning","title":"Great Work, Continue Your Learning","text":"<p>After completing this lesson, check out our Generative AI Learning collection to continue leveling up your Generative AI knowledge!</p> <p>Head over to Lesson 14 where we will look at the Generative AI Application Lifecycle!</p>"},{"location":"lessons/generative-ai-for-beginners/14-the-generative-ai-application-lifecycle/","title":"Lesson 14","text":""},{"location":"lessons/generative-ai-for-beginners/14-the-generative-ai-application-lifecycle/#the-generative-ai-application-lifecycle","title":"The Generative AI Application Lifecycle","text":"<p>An important question for all AI applications is the relevance of AI features, as AI is a fast evolving field, to ensure that your application remains relevant, reliable, and robust, you need to monitor, evaluate, and improve it continuously. This is where the generative AI lifecycle comes in.</p> <p>The generative AI lifecycle is a framework that guides you through the stages of developing, deploying, and maintaining a generative AI application. It helps you to define your goals, measure your performance, identify your challenges, and implement your solutions. It also helps you to align your application with the ethical and legal standards of your domain and your stakeholders. By following the generative AI lifecycle, you can ensure that your application is always delivering value and satisfying your users.</p>"},{"location":"lessons/generative-ai-for-beginners/14-the-generative-ai-application-lifecycle/#introduction","title":"Introduction","text":"<p>In this chapter, you will:</p> <ul> <li>Understand the Paradigm Shift from MLOps to LLMOps</li> <li>The LLM Lifecycle</li> <li>Lifecycle Tooling</li> <li>Lifecycle Metrification and Evaluation</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/14-the-generative-ai-application-lifecycle/#understand-the-paradigm-shift-from-mlops-to-llmops","title":"Understand the Paradigm Shift from MLOps to LLMOps","text":"<p>LLMs are a new tool in the Artificial Intelligence arsenal, they are incredibly powerful in analysis and generation tasks for applications, however this power has some consequences in how we streamline AI and Classic Machine Learning tasks.</p> <p>With this, we need a new Paradigm to adapt this tool in a dynamic, with the correct incentives. We can categorize older AI apps as \"ML Apps\" and newer AI Apps as \"GenAI Apps\" or just \"AI Apps\", reflecting the mainstream technology and techniques used at the time. This shifts our narrative in multiple ways, look at the following comparison.</p> <p></p> <p>Notice that in LLMOps, we are more focused on the App Developers, using integrations as a key point, using \"Models-as-a-Service\" and thinking in the following points for metrics.</p> <ul> <li>Quality: Response quality</li> <li>Harm: Responsible AI</li> <li>Honesty: Response groundedness (Makes sense? It is correct?)</li> <li>Cost: Solution Budget</li> <li>Latency: Avg. time for token response</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/14-the-generative-ai-application-lifecycle/#the-llm-lifecycle","title":"The LLM Lifecycle","text":"<p>First, to understand the lifecycle and the modifications, let's note the next infographic.</p> <p></p> <p>As you may note, this is different from the usual Lifecycles from MLOps. LLMs have many new requirements, as Prompting, different techniques to improve quality (Fine-Tuning, RAG, Meta-Prompts), different assessment and responsability with responsible AI, lastly, new evaluation metrics (Quality, Harm, Honesty, Cost and Latency).</p> <p>For instance, take a look at how we ideate. Using prompt engineering to experiment with various LLMs to explore possibilities to test if their Hypothesis could be correct.</p> <p>Note that this is not linear, but integrated loops, iterative and with an overarching cycle.</p> <p>How could we explore those steps? Let's step into detail in how could we build a lifecycle.</p> <p></p> <p>This may look a bit complicated, lets focus on the three big steps first.</p> <ol> <li>Ideating/Exploring: Exploration, here we can explore according to our business needs. Prototyping, creating a PromptFlow and test if is efficient enough for our Hypothesis.</li> <li>Building/Augmenting: Implementation, now, we start to evaluate for bigger datasets implement techniques, like Fine-tuning and RAG, to check the robustness of our solution. If it does not, re-implementing it, adding new steps in our flow or restructuring the data, might help. After testing our flow and our scale, if it works and check our Metrics, it is ready for the next step.</li> <li>Operationalizing: Integration, now adding Monitoring and Alerts Systems to our system, deployment and application integration to our Application.</li> </ol> <p>Then, we have the overarching cycle of Management, focusing on security, compliance and governance.</p> <p>Congratulations, now you have your AI App ready to go and operational. For a hands on experience, take a look on the Contoso Chat Demo.</p> <p>Now, what tools could we use?</p>"},{"location":"lessons/generative-ai-for-beginners/14-the-generative-ai-application-lifecycle/#lifecycle-tooling","title":"Lifecycle Tooling","text":"<p>For Tooling, Microsoft provides the Azure AI Platform and PromptFlow facilitate and make your cycle easy to implement and ready to go.</p> <p>The Azure AI Platform, allows you to use AI Studio. AI Studio is a web portal allows you to Explore models, samples and tools. Managing your resources, UI development flows and SDK/CLI options for Code-First development.</p> <p></p> <p>Azure AI, allows you to use multiple resources, to manage your operations, services, projects, vector search and databases needs.</p> <p></p> <p>Construct, from Proof-of-Concept(POC) until large scale applications with PromptFlow:</p> <ul> <li>Design and Build apps from VS Code, with visual and functional tools</li> <li>Test and fine-tune your apps for quality AI, with ease.</li> <li>Use Azure AI Studio to Integrate and Iterate with cloud, Push and Deploy for quick integration.</li> </ul> <p></p>"},{"location":"lessons/generative-ai-for-beginners/14-the-generative-ai-application-lifecycle/#great-continue-your-learning","title":"Great! Continue your Learning!","text":"<p>Amazing, now learn more about how we structure an application to use the concepts with the Contoso Chat App, to check how Cloud Advocacy adds those concepts in demonstrations. For more content, check our Ignite breakout session! </p> <p>Now, check Lesson 15, to understand how Retrieval Augmented Generation and Vector Databases impact Generative AI and to make more engaging Applications!</p>"},{"location":"lessons/generative-ai-for-beginners/15-rag-and-vector-databases/","title":"Lesson 15","text":""},{"location":"lessons/generative-ai-for-beginners/15-rag-and-vector-databases/#retrieval-augmented-generation-rag-and-vector-databases","title":"Retrieval Augmented Generation (RAG) and Vector Databases","text":"<p>In the search applications lesson, we briefly learned how to integrate your own data into Large Language Models (LLMs). In this lesson, we will delve further into the concepts of grounding your data in your LLM application, the mechanics of the process and the methods for storing data, including both embeddings and text.</p> <p>Video Coming Soon</p>"},{"location":"lessons/generative-ai-for-beginners/15-rag-and-vector-databases/#introduction","title":"Introduction","text":"<p>In this lesson we will cover the following:</p> <ul> <li> <p>An introduction to RAG, what it is and why it is used in AI (artificial intelligence).</p> </li> <li> <p>Understanding what vector databases are and creating one for our application.</p> </li> <li> <p>A practical example on how to integrate RAG into an application.</p> </li> </ul>"},{"location":"lessons/generative-ai-for-beginners/15-rag-and-vector-databases/#learning-goals","title":"Learning Goals","text":"<p>After completing this lesson, you will be able to:</p> <ul> <li> <p>Explain the significance of RAG in data retrieval and processing.</p> </li> <li> <p>Setup RAG application and ground your data to an LLM</p> </li> <li> <p>Effective integration of RAG and Vector Databases in LLM Applications.</p> </li> </ul>"},{"location":"lessons/generative-ai-for-beginners/15-rag-and-vector-databases/#our-scenario-enhancing-our-llms-with-our-own-data","title":"Our Scenario: enhancing our LLMs with our own data","text":"<p>For this lesson, we want to add our own notes into the education startup, which allows the chatbot to get more information on the different subjects. Using the notes that we have, learners will be able to study better and understand the different topics, making it easier to revise for their examinations. To create our scenario, we will use:</p> <ul> <li> <p><code>Azure OpenAI:</code> the LLM we will use to create our chatbot</p> </li> <li> <p><code>AI for beginners' lesson on Neural Networks</code>: this will be the data we ground our LLM on</p> </li> <li> <p><code>Azure AI Search</code> and <code>Azure Cosmos DB:</code> vector database to store our data and create a search index</p> </li> </ul> <p>Users will be able to create practice quizzes from their notes, revision flash cards and summarize it to concise overviews. To get started, let us look at what is RAG and how works:</p>"},{"location":"lessons/generative-ai-for-beginners/15-rag-and-vector-databases/#retrieval-augmented-generation-rag","title":"Retrieval Augmented Generation (RAG)","text":"<p>An LLM powered chatbot processes user prompts to generate responses. It is designed to be interactive and engages with users on a wide array of topics. However, its responses are limited to the context provided and its foundational training data. For instance, GPT-4 knowledge cutoff is September 2021, meaning, it lacks knowledge of events that have occurred after this period. In addition, the data used to train LLMs excludes confidential information such as personal notes or a company's product manual.</p>"},{"location":"lessons/generative-ai-for-beginners/15-rag-and-vector-databases/#how-rags-retrieval-augmented-generation-work","title":"How RAGs (Retrieval Augmented Generation) work","text":"<p>Suppose you want to deploy a chatbot that creates quizzes from your notes, you will require a connection to the knowledge base. This is where RAG comes to the rescue. RAGs operate as follows:</p> <ul> <li> <p>Knowledge base: Before retrieval, these documents need to be ingested and preprocessed, typically breaking down large documents into smaller chunks, transforming them to text embedding and storing them in a database.</p> </li> <li> <p>User Query: the user asks a question</p> </li> <li> <p>Retrieval: When a user asks a question, the embedding model retrieves relevant information from our knowledge base to provide more context that will be incorporated into the prompt.</p> </li> <li> <p>Augmented Generation: the LLM enhances its response based on the data retrieved. It allows the response generated to be not only based on pre-trained data but also relevant information from the added context. The retrieved data is used to augment the LLM's responses. The LLM then returns an answer to the user's question.</p> </li> </ul> <p></p> <p>The architecture for RAGs is implemented using transformers consisting of two parts: an encoder and a decoder. For example, when a user asks a question, the input text 'encoded' into vectors capturing the meaning of words and the vectors are 'decoded' into our document index and generates new text based on the user query. The LLM uses both an encoder-decoder model to generate the output.</p> <p>Two approaches when implementing RAG according to the proposed paper: Retrieval-Augmented Generation for Knowledge intensive NLP (natural language processing software) Tasks are:</p> <ul> <li> <p>RAG-Sequence using retrieved documents to predict the best possible answer to a user query</p> </li> <li> <p>RAG-Token using documents to generate the next token, then retrieve them to answer the user's query</p> </li> </ul>"},{"location":"lessons/generative-ai-for-beginners/15-rag-and-vector-databases/#why-would-you-use-rags","title":"Why would you use RAGs?","text":"<ul> <li> <p>Information richness: ensures text responses are up to date and current. It, therefore, enhances performance on domain specific tasks by accessing the internal knowledge base.</p> </li> <li> <p>Reduces fabrication by utilizing verifiable data in the knowledge base to provide context to the user queries.</p> </li> <li> <p>It is cost effective as they are more economical compared to fine-tuning an LLM</p> </li> </ul>"},{"location":"lessons/generative-ai-for-beginners/15-rag-and-vector-databases/#creating-a-knowledge-base","title":"Creating a knowledge base","text":"<p>Our application is based on our personal data i.e., the Neural Network lesson on AI For Beginners curriculum.</p>"},{"location":"lessons/generative-ai-for-beginners/15-rag-and-vector-databases/#vector-databases","title":"Vector Databases","text":"<p>A vector database, unlike traditional databases, is a specialized database designed to store, manage and search embedded vectors. It stores numerical representations of documents. Breaking down data to numerical embeddings makes it easier for our AI system to understand and process the data.</p> <p>We store our embeddings in vector databases as LLMs have a limit of the number of tokens they accept as input. As you cannot pass the entire embeddings to an LLM, we will need to break them down into chunks and when a user asks a question, the embeddings most like the question will be returned together with the prompt. Chunking also reduces costs on the number of tokens passed through an LLM.</p> <p>Some popular vector databases include Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant and DeepLake. You can create an Azure Cosmos DB model using Azure CLI with the following command:</p> <pre><code>az login\naz group create -n &lt;resource-group-name&gt; -l &lt;location&gt;\naz cosmosdb create -n &lt;cosmos-db-name&gt; -r &lt;resource-group-name&gt;\naz cosmosdb list-keys -n &lt;cosmos-db-name&gt; -g &lt;resource-group-name&gt;\n</code></pre>"},{"location":"lessons/generative-ai-for-beginners/15-rag-and-vector-databases/#from-text-to-embeddings","title":"From text to embeddings","text":"<p>Before we store our data, we will need to convert it to vector embeddings before it is stored in the database. If you are working with large documents or long texts, you can chunk them based on queries you expect. Chunking can be done at sentence level, or at a paragraph level. As chunking derives meanings from the words around them, you can add some other context to a chunk, for example, by adding the document title or including some text before or after the chunk. You can chunk the data as follows:</p> <pre><code>def split_text(text, max_length, min_length):\n    words = text.split()\n    chunks = []\n    current_chunk = []\n\n    for word in words:\n        current_chunk.append(word)\n        if len(' '.join(current_chunk)) &lt; max_length and len(' '.join(current_chunk)) &gt; min_length:\n            chunks.append(' '.join(current_chunk))\n            current_chunk = []\n\n    # If the last chunk didn't reach the minimum length, add it anyway\n    if current_chunk:\n        chunks.append(' '.join(current_chunk))\n\n    return chunks\n</code></pre> <p>Once chunked, we can then embed our text using different embedding models. Some models you can use include: word2vec, ada-002 by OpenAI, Azure Computer Vision and many more. Selecting a model to use will depend on the languages you're using, the type of content encoded (text/images/audio), the size of input it can encode and length of the embedding output.</p> <p>An example of embedded text using OpenAI's <code>text-embedding-ada-002</code> model is: </p>"},{"location":"lessons/generative-ai-for-beginners/15-rag-and-vector-databases/#retrieval-and-vector-search","title":"Retrieval and Vector Search","text":"<p>When a user asks a question, the retriever transforms it into a vector using the query encoder, it then searches through our document search index for relevant vectors in the document that are related to the input. Once done, it converts both the input vector and document vectors into text and passes it through the LLM.</p>"},{"location":"lessons/generative-ai-for-beginners/15-rag-and-vector-databases/#retrieval","title":"Retrieval","text":"<p>Retrieval happens when the system tries to quickly find the documents from the index that satisfy the search criteria. The goal of the retriever is to get documents that will be used to provide context and ground the LLM on your data.</p> <p>There are several ways to perform search within our database such as:</p> <ul> <li> <p>Keyword search - used for text searches</p> </li> <li> <p>Semantic search - uses the semantic meaning of words</p> </li> <li> <p>Vector search - converts documents from text to vector representations using embedding models. Retrieval will be done by querying the documents whose vector representations are closest to the user question.</p> </li> <li> <p>Hybrid - a combination of both keyword and vector search.</p> </li> </ul> <p>A challenge with retrieval comes in when there is no similar response to the query in the database, the system will then return the best information they can get, however, you can use tactics like set up the maximum distance for relevance or use hybrid search that combines both keywords and vector search. In this lesson we will use hybrid search, a combination of both vector and keyword search. We will store our data into a dataframe with columns containing the chunks as well as embeddings.</p>"},{"location":"lessons/generative-ai-for-beginners/15-rag-and-vector-databases/#vector-similarity","title":"Vector Similarity","text":"<p>The retriever will search through the knowledge database for embeddings that are close together, the closest neighbour, as they are texts that are similar. In the scenario a user asks a query, it is first embedded then matched with similar embeddings. The common measurement that is used to find how similar different vectors are is cosine similarity which is based on the angle between two vectors.</p> <p>We can measure similarity using other alternatives we can use are Euclidean distance which is the straight line between vector endpoints and dot product which measures the sum of the products of corresponding elements of two vectors.</p>"},{"location":"lessons/generative-ai-for-beginners/15-rag-and-vector-databases/#search-index","title":"Search index","text":"<p>When doing retrieval, we will need to build a search index for our knowledge base before we perform search. An index will store our embeddings and can quickly retrieve the most similar chunks even in a large database. We can create our index locally using:</p> <pre><code>from sklearn.neighbors import NearestNeighbors\n\nembeddings = flattened_df['embeddings'].to_list()\n\n# Create the search index\nnbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)\n\n# To query the index, you can use the kneighbors method\ndistances, indices = nbrs.kneighbors(embeddings)\n</code></pre>"},{"location":"lessons/generative-ai-for-beginners/15-rag-and-vector-databases/#re-ranking","title":"Re-ranking","text":"<p>Once you have queried the database, you might need to sort the results from the most relevant. A reranking LLM utilizes Machine Learning to improve the relevance of search results by ordering them from the most relevant. Using Azure AI Search, reranking is done automatically for you using a semantic reranker. An example of how reranking works using nearest neighbours:</p> <pre><code># Find the most similar documents\ndistances, indices = nbrs.kneighbors([query_vector])\n\nindex = []\n# Print the most similar documents\nfor i in range(3):\n    index = indices[0][i]\n    for index in indices[0]:\n        print(flattened_df['chunks'].iloc[index])\n        print(flattened_df['path'].iloc[index])\n        print(flattened_df['distances'].iloc[index])\n    else:\n        print(f\"Index {index} not found in DataFrame\")\n</code></pre>"},{"location":"lessons/generative-ai-for-beginners/15-rag-and-vector-databases/#bringing-it-all-together","title":"Bringing it all together","text":"<p>The last step is adding our LLM into the mix to be able to get responses that are grounded on our data. We can implement it as follows:</p> <pre><code>user_input = \"what is a perceptron?\"\n\ndef chatbot(user_input):\n    # Convert the question to a query vector\n    query_vector = create_embeddings(user_input)\n\n    # Find the most similar documents\n    distances, indices = nbrs.kneighbors([query_vector])\n\n    # add documents to query  to provide context\n    history = []\n    for index in indices[0]:\n        history.append(flattened_df['chunks'].iloc[index])\n\n    # combine the history and the user input\n    history.append(user_input)\n\n    # create a message object\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are an AI assistant that helps with AI questions.\"},\n        {\"role\": \"user\", \"content\": history[-1]}\n    ]\n\n    # use chat completion to generate a response\n    response = openai.chat.completions.create(\n        model=\"gpt-4\",\n        temperature=0.7,\n        max_tokens=800,\n        messages=messages\n    )\n\n    return response.choices[0].message\n\nchatbot(user_input)\n</code></pre>"},{"location":"lessons/generative-ai-for-beginners/15-rag-and-vector-databases/#evaluating-our-application","title":"Evaluating our application","text":""},{"location":"lessons/generative-ai-for-beginners/15-rag-and-vector-databases/#evaluation-metrics","title":"Evaluation Metrics","text":"<ul> <li> <p>Quality of responses supplied ensuring it sounds natural, fluent and human-like</p> </li> <li> <p>Groundedness of the data: evaluating whether the response that came from supplied docs</p> </li> <li> <p>Relevance: evaluating the response matches and is related to the question asked</p> </li> <li> <p>Fluency - whether the response makes sense grammatically</p> </li> </ul>"},{"location":"lessons/generative-ai-for-beginners/15-rag-and-vector-databases/#use-cases-for-using-rag-retervival-augmented-generation-and-vector-databases","title":"Use Cases for using RAG (Retervival Augmented Generation) and vector databases","text":"<p>There are many different use cases where function calls can improve your app like:</p> <ul> <li> <p>Question and Answering: grounding your company data to a chat that can be used by employees to ask questions.</p> </li> <li> <p>Recommendation Systems: where you can create a system that matches the most similar values e.g. movies, restaurants and many more.</p> </li> <li> <p>Chatbot services: you can store chat history and personalize the conversation based on the user data.</p> </li> <li> <p>Image search based on vector embeddings, useful when doing image recognition and anomaly detection.</p> </li> </ul>"},{"location":"lessons/generative-ai-for-beginners/15-rag-and-vector-databases/#summary","title":"Summary","text":"<p>We have covered the fundamental areas of RAG from adding our data to the application, the user query and output. To simplify creation of RAG, you can use frameworks such as Semanti Kernel, Langchain or Autogen.</p>"},{"location":"lessons/generative-ai-for-beginners/15-rag-and-vector-databases/#assignment","title":"Assignment","text":"<p>To continue your learning of Retrieval Augmented Generation (RAG) you can build:</p> <ul> <li> <p>Build a front-end for the application using the framework of your choice</p> </li> <li> <p>Utilize a framework, either LangChain or Semantic Kernel, and recreate your application.</p> </li> </ul> <p>Congratulations for completing the lesson \ud83d\udc4f.</p>"},{"location":"lessons/generative-ai-for-beginners/15-rag-and-vector-databases/#learning-does-not-stop-here-continue-the-journey","title":"Learning does not stop here, continue the Journey","text":"<p>After completing this lesson, check out our Generative AI Learning collection to continue leveling up your Generative AI knowledge!</p>"},{"location":"lessons/generative-ai-for-beginners/16-open-source-models/","title":"Lesson 16","text":""},{"location":"lessons/generative-ai-for-beginners/16-open-source-models/#introduction","title":"Introduction","text":"<p>The world of open-source LLMs is exciting and constantly evolving. This lesson aims to provide an in-depth look at open source models. If you are looking for information on how proprietary models compare to open source models, go to the \"Exploring and Comparing Different LLMs\" lesson. This lesson will also cover the topic of fine-tuning but a more detailed explanation can be found in the \"Fine-Tuning LLMs\" lesson.</p>"},{"location":"lessons/generative-ai-for-beginners/16-open-source-models/#learning-goals","title":"Learning goals","text":"<ul> <li>Gain an understanding of open source Models</li> <li>Understanding the benefits of working with open source Models</li> <li>Exploring the open models available on Hugging Face and the Azure AI Studio</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/16-open-source-models/#what-are-open-source-models","title":"What are Open Source Models?","text":"<p>Open source software has played a crucial role in the growth of technology across various fields. The Open Source Initiative (OSI) has defined 10 criteria for software to be classified as open source. The source code must be openly shared under a license approved by the OSI.</p> <p>While the development of LLMs has similar elements to developing software, the process is not exactly the same. This has brought much discussion in the community on the definition of open source in the context of LLMs. For a model to be aligned with the traditional definition of open source the following information should be publicly available:</p> <ul> <li>Datasets used to train the model.</li> <li>Full model weights as a part of the training.</li> <li>The evaluation code.</li> <li>The fine-tuning code.</li> <li>Full model weights and training metrics.</li> </ul> <p>There are currently only a few models that match this criteria. The OLMo model created by Allen Institute for\u00a0Artificial\u00a0Intelligence\u00a0(AllenAI)\u00a0is one that fits this category.</p> <p>For this lesson, we will refer to the models as \"open models\" going forward as they may not match the criteria above at the time of writing.</p>"},{"location":"lessons/generative-ai-for-beginners/16-open-source-models/#benefits-of-open-models","title":"Benefits of Open Models","text":"<p>Highly Customizable - Since open models are released with detailed training information, researchers and developers can modify the model's internals. This enables the creation of highly specialized models that are fine-tuned for a specific task or area of study. Some examples of this are code generation, mathematical operations and biology.</p> <p>Cost - The cost per token for using and deploying these models is lower than that of proprietary models. When building Generative AI applications, looking at performance vs price when working with these models on your use case should be done.</p> <p> Source: Artificial Analysis</p> <p>Flexibility - Working with open models enables you to be flexible in terms of using different models or combining them. An example of this is the HuggingChat Assistants  where a user can select the model being used directly in the user interface:</p> <p></p>"},{"location":"lessons/generative-ai-for-beginners/16-open-source-models/#exploring-different-open-models","title":"Exploring Different Open Models","text":""},{"location":"lessons/generative-ai-for-beginners/16-open-source-models/#llama-2","title":"Llama 2","text":"<p>LLama2, developed by Meta is an open model that is optimized for chat based applications. This is due to its fine-tuning method, which included a large amount of dialogue and human feedback. With this method, the model produces more results that are aligned to human expectation which provides a better user experience.</p> <p>Some examples of fine-tuned versions of Llama include Japanese Llama, which specializes in Japanese and Llama Pro, which is an enhanced version of the base model.</p>"},{"location":"lessons/generative-ai-for-beginners/16-open-source-models/#mistral","title":"Mistral","text":"<p>Mistral is an open model with a strong focus of high performance and efficiency. It uses the Mixture-of-Experts approach which combines a group of specialized expert models into one system where depending on the input, certain models are selected to be used. This makes the computation more effective as models are only addressing the inputs they are specialized in.</p> <p>Some examples of fine-tuned versions of Mistral include BioMistral, which is focused on the medical domain and OpenMath Mistral, which performs mathematical computation.</p>"},{"location":"lessons/generative-ai-for-beginners/16-open-source-models/#falcon","title":"Falcon","text":"<p>Falcon is an LLM created by the Technology Innovation Institute (TII). The Falcon-40B was trained on 40 billion parameters which has been shown to perform better than GPT-3 with less compute budget. This is due to its use of the FlashAttention algorithm and multiquery attention that enables it to cut down on the memory requirements at inference time. With this reduced inference time, the Falcon-40B is suitable for chat applications.</p> <p>Some examples of fine-tuned versions of Falcon are the OpenAssistant, an assistant built on open models and GPT4ALL, which delivers higher performance than the base model.</p>"},{"location":"lessons/generative-ai-for-beginners/16-open-source-models/#how-to-choose","title":"How to Choose","text":"<p>There is no one answer for choosing an open model. A good place to start is by using the Azure AI Studio's filter by task feature. This will help you understand what types of tasks the model has been trained for. Hugging Face also maintains an LLM Leaderboard which shows you the best performing models based on certain metrics.</p> <p>When looking to compare LLMs across the different types, Artificial Analysis is another great resource:</p> <p> Source: Artifical Analysis</p> <p>If working on a specific use case, searching for fine-tuned versions that are focused on the same area can be effective. Experimenting with multiple open models to see how they perform according to your and your users' expectations is another good practice</p>"},{"location":"lessons/generative-ai-for-beginners/16-open-source-models/#next-steps","title":"Next Steps","text":"<p>The best part about open models is that you can get started working with them pretty quickly. Check out the Azure AI Studio Model Catalog, which features a specific Hugging Face collection with these models we discussed here.</p>"},{"location":"lessons/generative-ai-for-beginners/16-open-source-models/#learning-does-not-stop-here-continue-the-journey","title":"Learning does not stop here, continue the Journey","text":"<p>After completing this lesson, check out our Generative AI Learning collection to continue leveling up your Generative AI knowledge!</p>"},{"location":"lessons/generative-ai-for-beginners/17-ai-agents/","title":"Lesson 17","text":""},{"location":"lessons/generative-ai-for-beginners/17-ai-agents/#introduction","title":"Introduction","text":"<p>AI Agents represent an exciting development in Generative AI, enabling Large Language Models (LLMs) to evolve from assistants into agents capable of taking actions. AI Agent frameworks enable developers to create applications that give LLMs access to tools and state management. These frameworks also enhance visibility, allowing users and developers to monitor the actions planned by LLMs, thereby improving experience management.</p> <p>The lesson will cover the following areas:</p> <ul> <li>Understanding what an AI Agent is - What exactly is an AI Agent?</li> <li>Exploring four different AI Agent Frameworks - What makes them unique?</li> <li>Applying these AI Agents to different use cases - When should we use AI Agents?</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/17-ai-agents/#learning-goals","title":"Learning goals","text":"<p>After taking this lesson, you'll be able to:</p> <ul> <li>Explain what AI Agents are and how they can be used.</li> <li>Have an understanding of the differences between some of the popular AI Agent Frameworks, and how they differ.</li> <li>Understand how AI Agents function in order to build applications with them.</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/17-ai-agents/#what-are-ai-agents","title":"What Are AI Agents?","text":"<p>AI Agents are a very exciting field in the world of Generative AI. With this excitement comes sometimes a confusion of terms and their application. To keep things simple and inclusive of most of the tools that refer to AI Agents, we are going to use this definition:</p> <p>AI Agents allow Large Language Models (LLMs) to perform tasks by giving them access to a state and tools.</p> <p></p> <p>Let's define these terms:</p> <p>Large Language Models - These are the models referred throughout this course such as GPT-3.5, GPT-4, Llama-2, etc.</p> <p>State - This refers to the context that the LLM is working in. The LLM uses the context of its past actions and the current context, guiding its decision-making for subsequent actions. AI Agent Frameworks allow developers to maintain this context easier.</p> <p>Tools - To complete the task that the user has requested and that the LLM has planned out, the LLM needs access to tools. Some examples of tools can be a database, an API, an external application or even another LLM!</p> <p>These definitions will hopefully give you a good grounding going forward as we look at how they are implemented. Let's explore a few different AI Agent frameworks:</p>"},{"location":"lessons/generative-ai-for-beginners/17-ai-agents/#langchain-agents","title":"LangChain Agents","text":"<p>LangChain Agents is an implementation of the definitions we provided above.</p> <p>To manage the state , it uses a built-in function called the <code>AgentExecutor</code>. This accepts the defined <code>agent</code> and the <code>tools</code> that are available to it.</p> <p>The <code>Agent Executor</code> also stores the chat history to provide the context of the chat.</p> <p></p> <p>LangChain offers a catalog of tools that can be imported into your application in which the LLM can get access to. These are made by the community and by the LangChain team.</p> <p>You can then define these tools and pass them to the <code>Agent Executor</code>.</p> <p>Visibility is another important aspect when talking about AI Agents. It is important for application developers to understand which tool the LLM is using and why.. For that, the team at LangChain have developed LangSmith.</p>"},{"location":"lessons/generative-ai-for-beginners/17-ai-agents/#autogen","title":"AutoGen","text":"<p>The next AI Agent framework we will discuss is AutoGen. The main focus of AutoGen is conversations. Agents are both conversable and customizable.</p> <p>Conversable - LLMs can start and continue a conversation with another LLM in order to complete a task. This is done by creating <code>AssistantAgents</code> and giving them a specific system message.</p> <pre><code>autogen.AssistantAgent( name=\"Coder\", llm_config=llm_config, ) pm = autogen.AssistantAgent( name=\"Product_manager\", system_message=\"Creative in software product ideas.\", llm_config=llm_config, )\n</code></pre> <p>Customizable - Agents can be defined not only as LLMs but be a user or a tool. As a developer, you can define a <code>UserProxyAgent</code> which is responsible for interacting with the user for feedback in completing a task. This feedback can either continue the execution of the task or stop it.</p> <pre><code>user_proxy = UserProxyAgent(name=\"user_proxy\")\n</code></pre>"},{"location":"lessons/generative-ai-for-beginners/17-ai-agents/#state-and-tools","title":"State and Tools","text":"<p>To change and manage state, an assistant Agent generates Python code to complete the task.</p> <p>Here is an example of the process:</p> <p></p>"},{"location":"lessons/generative-ai-for-beginners/17-ai-agents/#llm-defined-with-a-system-message","title":"LLM Defined with a System Message","text":"<pre><code>system_message=\"For weather related tasks, only use the functions you have been provided with. Reply TERMINATE when the task is done.\"\n</code></pre> <p>This system messages directs this specific LLM to which functions are relevant for its task. Remember, with AutoGen you can have multiple defined AssistantAgents with different system messages.</p>"},{"location":"lessons/generative-ai-for-beginners/17-ai-agents/#chat-is-initiated-by-user","title":"Chat is Initiated by User","text":"<pre><code>user_proxy.initiate_chat( chatbot, message=\"I am planning a trip to NYC next week, can you help me pick out what to wear? \", )\n</code></pre> <p>This message from the user_proxy (Human) is what will start the process of the Agent to explore the possible functions that it should execute.</p>"},{"location":"lessons/generative-ai-for-beginners/17-ai-agents/#function-is-executed","title":"Function is Executed","text":"<pre><code>chatbot (to user_proxy):\n\n***** Suggested tool Call: get_weather ***** Arguments: {\"location\":\"New York City, NY\",\"time_periond:\"7\",\"temperature_unit\":\"Celsius\"} ******************************************************** --------------------------------------------------------------------------------\n\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; EXECUTING FUNCTION get_weather... user_proxy (to chatbot): ***** Response from calling function \"get_weather\" ***** 112.22727272727272 EUR ****************************************************************\n</code></pre> <p>Once the initial chat is processed, the Agent will send the suggest tool to call. In this case, it is a function called <code>get_weather</code>. Depending on your configuration, this function can be automatically executed and read by the Agent or can be executed based on user input.</p> <p>You can find a list of AutoGen code samples to further explore how to get started building.</p>"},{"location":"lessons/generative-ai-for-beginners/17-ai-agents/#taskweaver","title":"Taskweaver","text":"<p>The next agent framework we will explore is Taskweaver. It is known as a \"code-first\" agent because instead of working strictly with <code>strings</code> , it can work with DataFrames in Python. This becomes extremely useful for data analysis and generation tasks. This can be things like creating graphs and charts or generating random numbers.</p>"},{"location":"lessons/generative-ai-for-beginners/17-ai-agents/#state-and-tools_1","title":"State and Tools","text":"<p>To manage the state of the conversation, TaskWeaver uses the concept of a <code>Planner</code>. The <code>Planner</code> is a LLM that takes the request from the users and maps out the tasks that need to be completed to fulfill this request.</p> <p>To complete the tasks the <code>Planner</code> is exposed to the collection of tools called <code>Plugins</code>. This can be Python classes or a general code interpreter. This plugins are stored as embeddings so that the LLM can better search for the correct plugin.</p> <p></p> <p>Here is an example of a plugin to handle anomaly detection:</p> <pre><code>class AnomalyDetectionPlugin(Plugin): def __call__(self, df: pd.DataFrame, time_col_name: str, value_col_name: str):\n</code></pre> <p>The code is verified before executing. Another feature to manage context in Taskweaver is <code>experience</code>. Experience allows for the context of a conversation to be stored over to the long term in a YAML file. This can be configured so that the LLM improves over time on certain tasks given that it is exposed to prior conversations.</p>"},{"location":"lessons/generative-ai-for-beginners/17-ai-agents/#jarvis","title":"JARVIS","text":"<p>The last agent framework we will explore is JARVIS. What makes JARVIS unique is that it uses an LLM to manage the <code>state</code> of the conversation and the <code>tools</code>are other AI models. Each of the AI models are specialized models that perform certain tasks such as object detection, transcription or image captioning.</p> <p></p> <p>The LLM, being a general purpose model, receives the request from the user and identifies the specific task and any arguments/data that is needed to complete the task.</p> <pre><code>[{\"task\": \"object-detection\", \"id\": 0, \"dep\": [-1], \"args\": {\"image\": \"e1.jpg\" }}]\n</code></pre> <p>The LLM then formats the request in a manner that the specialized AI model can interpret, such as JSON. Once the AI model has returned its prediction based on the task, the LLM receives the response.</p> <p>If multiple models are required to complete the task, it will also interpret the response from those models before bringing them together to generate to the response to the user.</p> <p>The example below shows how this would work when a user is requesting a description and count of the objects in a picture:</p>"},{"location":"lessons/generative-ai-for-beginners/17-ai-agents/#assignment","title":"Assignment","text":"<p>To continue your learning of AI Agents you can build with AutoGen:</p> <ul> <li>An application that simulates a business meeting with different departments of an education startup.</li> <li>Create system messages that guide LLMs in understanding different personas and priorities, and enable the user to pitch a new product idea.</li> <li>The LLM should then generate follow-up questions from each department to refine and improve the pitch and the product idea</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/17-ai-agents/#learning-does-not-stop-here-continue-the-journey","title":"Learning does not stop here, continue the Journey","text":"<p>After completing this lesson, check out our Generative AI Learning collection to continue leveling up your Generative AI knowledge!</p>"},{"location":"lessons/generative-ai-for-beginners/18-fine-tuning/","title":"Lesson 18","text":""},{"location":"lessons/generative-ai-for-beginners/18-fine-tuning/#fine-tuning-your-llm","title":"Fine-Tuning Your LLM","text":"<p>Using large language models to build generative AI applications comes with new challenges. A key issue is ensuring response quality (accuracy and relevance) in content generated by the model for a given user request. In previous lessons, we discussed techniques like prompt engineering and retrieval-augmented generation that try to solve the problem by modifying the prompt input to the existing model.</p> <p>In today's lesson, we discuss a third technique, fine-tuning, which tries to address the challenge by retraining the model itself with additional data. Let's dive into the details.</p>"},{"location":"lessons/generative-ai-for-beginners/18-fine-tuning/#learning-objectives","title":"Learning Objectives","text":"<p>This lesson introduces the concept of fine-tuning for pre-trained language models, explores the benefits and challenges of this approach, and provides guidance on when and how to use fine tuning to improve the performance of your generative AI models.</p> <p>By the end of this lesson, you should be able to answer the following questions:</p> <ul> <li>What is fine tuning for language models?</li> <li>When, and why, is fine tuning useful?</li> <li>How can I fine-tune a pre-trained model?</li> <li>What are the limitations of fine-tuning?</li> </ul> <p>Ready? Let's get started.</p>"},{"location":"lessons/generative-ai-for-beginners/18-fine-tuning/#illustrated-guide","title":"Illustrated Guide","text":"<p>Want to get the big picture of what we'll cover before we dive in? Check out this illustrated guide that describes the learning journey for this lesson - from learning the core concepts and motivation for fine-tuning, to understanding the process and best practices for executing the fine-tuning task. This is a fascinating topic for exploration, so don't forget to check out the Resources page for additional links to support your self-guided learning journey!</p> <p></p>"},{"location":"lessons/generative-ai-for-beginners/18-fine-tuning/#what-is-fine-tuning-for-language-models","title":"What is fine-tuning for language models?","text":"<p>By definition, large language models are pre-trained on large quantities of text sourced from diverse sources including the internet. As we've learned in previous lessons, we need techniques like prompt engineering and retrieval-augmented generation to improve the quality of the model's responses to the user's questions (\"prompts\").</p> <p>A popular prompt-engineering technique involves giving the model more guidance on what is expected in the response either by providing instructions (explicit guidance) or giving it a few examples (implicit guidance). This is referred to as few-shot learning but it has two limitations:</p> <ul> <li>Model token limits can restrict the number of examples you can give, and limit the effectiveness.</li> <li>Model token costs can make it expensive to add examples to every prompt, and limit flexibility.</li> </ul> <p>Fine-tuning is a common practice in machine learning systems where we take a pre-trained model and retrain it with new data to improve its performance on a specific task. In the context of language models, we can fine-tune the pre-trained model with a curated set of examples for a given task or application domain to create a custom model that may be more accurate and relevant for that specific task or domain. A side-benefit of fine-tuning is that it can also reduce the number of examples needed for few-shot learning - reducing token usage and related costs.</p>"},{"location":"lessons/generative-ai-for-beginners/18-fine-tuning/#when-and-why-should-we-fine-tune-models","title":"When and why should we fine-tune models?","text":"<p>In this context, when we talk about fine-tuning, we are referring to supervised fine-tuning where the retraining is done by adding new data that was not part of the original training dataset. This is different from an unsupervised fine-tuning approach where the model is retrained on the original data, but with different hyperparameters.</p> <p>The key thing to remember is that fine-tuning is an advanced technique that requires a certain level of expertise to get the desired results. If done incorrectly, it may not provide the expected improvements, and may even degrade the performance of the model for your targeted domain.</p> <p>So, before you learn \"how\" to fine-tune language models, you need to know \"why\" you should take this route, and \"when\" to start the process of fine-tuning. Start by asking yourself these questions:</p> <ul> <li>Use Case: What is your use case for fine-tuning? What aspect of the current pre-trained model do you want to improve upon?</li> <li>Alternatives: Have you tried other techniques to achieve the desired outcomes? Use them to create a baseline for comparison.</li> <li>Prompt engineering: Try techniques like few-shot prompting with examples of relevant prompt responses. Evaluate the quality of responses.</li> <li>Retrieval Augmented Generation: Try augmenting prompts with query results retrieved by searching your data. Evaluate the quality of responses.</li> <li>Costs: Have you identified the costs for fine-tuning?</li> <li>Tunability - is the pre-trained model available for fine-tuning?</li> <li>Effort - for preparing training data, evaluating &amp; refining model.</li> <li>Compute - for running fine-tuning jobs, and deploying fine-tuned model</li> <li>Data - access to sufficient quality examples for fine-tuning impact</li> <li>Benefits: Have you confirmed the benefits for fine-tuning?</li> <li>Quality - did fine-tuned model outperform baseline?</li> <li>Cost - does it reduce token usage by simplifying prompts?</li> <li>Extensibility - can you repurpose base model for new domains?</li> </ul> <p>By answering these questions, you should be able to decide if fine-tuning is the right approach for your use case. Ideally, the approach is valid only if the benefits outweigh the costs. Once you decide to proceed, it's time to think about how you can fine tune the pre-trained model.</p> <p>Want to get more insights on the decision-making process? Watch To fine-tune or not to fine-tune</p>"},{"location":"lessons/generative-ai-for-beginners/18-fine-tuning/#how-can-we-fine-tune-a-pre-trained-model","title":"How can we fine-tune a pre-trained model?","text":"<p>To fine-tune a pre-trained model, you need to have:</p> <ul> <li>a pre-trained model to fine-tune</li> <li>a dataset to use for fine-tuning</li> <li>a training environment to run the fine-tuning job</li> <li>a hosting environment to deploy fine-tuned model</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/18-fine-tuning/#fine-tuning-in-action","title":"Fine-Tuning In Action","text":"<p>The following resources provide step-by-step tutorials to walk you through a real example using a selected model with a curated dataset. To work through these tutorials, you need an account on the specific provider, along with access to the relevant model and datasets.</p> Provider Tutorial Description OpenAI How to fine-tune chat models Learn to fine-tune a <code>gpt-35-turbo</code> for a specific domain (\"recipe assistant\") by preparing training data, running the fine-tuning job, and using the fine-tuned model for inference. Azure OpenAI GPT 3.5 Turbo fine-tuning tutorial Learn to fine-tune a <code>gpt-35-turbo-0613</code> model on Azure by taking steps to create &amp; upload training data, run the fine-tuning job. Deploy &amp; use the new model. Hugging Face Fine-tuning LLMs with Hugging Face This blog post walks you fine-tuning an open LLM (ex: <code>CodeLlama 7B</code>) using the transformers library &amp; Transformer Reinforcement Learning (TRL) with open datasets on Hugging Face. \ud83e\udd17 AutoTrain Fine-tuning LLMs with AutoTrain AutoTrain (or AutoTrain Advanced) is a python library developed by Hugging Face that allows finetuning for many different tasks including LLM finetuning. AutoTrain is a no-code solution and finetuning can be done in your own cloud, on Hugging Face Spaces or locally. It supports both a web-based GUI, CLI and training via yaml config files."},{"location":"lessons/generative-ai-for-beginners/18-fine-tuning/#assignment","title":"Assignment","text":"<p>Select one of the tutorials above and walk through them. We may replicate a version of these tutorials in Jupyter Notebooks in this repo for reference only. Please use the original sources directly to get the latest versions.</p>"},{"location":"lessons/generative-ai-for-beginners/18-fine-tuning/#great-work-continue-your-learning","title":"Great Work! Continue Your Learning.","text":"<p>After completing this lesson, check out our Generative AI Learning collection to continue leveling up your Generative AI knowledge!</p> <p>Congratulations!! You have completed the final lesson from the v2 series for this course! Don't stop learning and building. **Check out the RESOURCES page for a list of additional suggestions for just this topic.</p> <p>Our v1 series of lessons have also been updated with more assignments and concepts. So take a minute to refresh your knowledge - and please share your questions and feedback to help us improve these lessons for the community.</p>"},{"location":"lessons/generative-ai-for-beginners/18-fine-tuning/#resources-for-self-guided-learning","title":"Resources For Self-Guided Learning","text":"<p>The lesson was built using a number of core resources from OpenAI and Azure OpenAI as references for the terminology and tutorials. Here is a non-comprehensive list, for your own self-guided learning journeys.</p>"},{"location":"lessons/generative-ai-for-beginners/18-fine-tuning/#1-primary-resources","title":"1. Primary Resources","text":"Title/Link Description Fine-tuning with OpenAI Models Fine-tuning improves on few-shot learning by training on many more examples than can fit in the prompt, saving you costs, improving response quality, and enabling lower-latency requests. Get an overview of fine-tuning from OpenAI. What is Fine-Tuning with Azure OpenAI? Understand what fine-tuning is (concept), why you should look at it (motivating problem), what data to use (training) and measuring the quality Customize a model with fine-tuning Azure OpenAI Service lets you tailor our models to your personal datasets using fine-tuning. Learn how to fine-tune (process) select models using Azure AI Studio, Python SDK or REST API. Recommendations for LLM fine-tuning LLMs may not perform well on specific domains, tasks, or datasets, or may produce inaccurate or misleading outputs. When should you consider fine-tuning as a possible solution to this? Continuous Fine Tuning Continuous fine-tuning is the iterative process of selecting an already fine-tuned model as a base model and fine-tuning it further on new sets of training examples. Fine-tuning and function calling Fine-tuning your model with function calling examples can improve model output by getting more accurate and consistent outputs - with similarly-formatted responses &amp; cost-savings Fine-tuning Models: Azure OpenAI Guidance Look up this table to understand what models can be fine-tuned in Azure OpenAI, and which regions these are available in. Look up their token limits and training data expiry dates if needed. To Fine Tune or Not To Fine Tune? That is the Question This 30-min Oct 2023 episode of the AI Show discusses benefits, drawbacks and practical insights that help you make this decision. Getting Started With LLM Fine-Tuning This AI Playbook resource walks you through data requirements, formatting, hyperparameter fine-tuning and challenges/limitations you should know. Tutorial: Azure OpenAI GPT3.5 Turbo Fine-Tuning Learn to create a sample fine-tuning dataset, prepare for fine-tuning, create a fine-tuning job, and deploy the fine-tuned model on Azure. Tutorial: Fine-tune a Llama 2 model in Azure AI Studio Azure AI Studio lets you tailor large language models to your personal datasets using a UI-based workflow suitable for low-code developers. See this example. Tutorial:Fine-tune Hugging Face models for a single GPU on Azure This article describes how to fine-tune a Hugging Face model with the Hugging Face transformers library on a single GPU with Azure DataBricks + Hugging Face Trainer libraries Training: Fine-tune a foundation model with Azure Machine Learning The model catalog in Azure Machine Learning offers many open source models you can fine-tune for your specific task. Try this module is from the AzureML Generative AI Learning Path Tutorial: Azure OpenAI Fine-Tuning Fine-tuning GPT-3.5 or GPT-4 models on Microsoft Azure using W&amp;B allows for detailed tracking and analysis of model performance. This guide extends the concepts from the OpenAI Fine-Tuning guide with specific steps and features for Azure OpenAI."},{"location":"lessons/generative-ai-for-beginners/18-fine-tuning/#2-secondary-resources","title":"2. Secondary Resources","text":"<p>This section captures additional resources that are worth exploring, but that we did not have time to cover in this lesson. They may be covered in a future lesson, or as a secondary assignment option, at a later date. For now, use them to build your own expertise and knowledge around this topic.</p> Title/Link Description OpenAI Cookbook: Data preparation and analysis for chat model fine-tuning This notebook serves as a tool to preprocess and analyze the chat dataset used for fine-tuning a chat model. It checks for format errors, provides basic statistics, and estimates token counts for fine-tuning costs. See: Fine-tuning method for gpt-3.5-turbo. OpenAI Cookbook: Fine-Tuning for Retrieval Augmented Generation (RAG) with Qdrant The aim of this notebook is to walk through a comprehensive example of how to fine-tune OpenAI models for Retrieval Augmented Generation (RAG). We will also be integrating Qdrant and Few-Shot Learning to boost model performance and reduce fabrications. OpenAI Cookbook: Fine-tuning GPT with Weights &amp; Biases Weights &amp; Biases (W&amp;B) is the AI developer platform, with tools for training models, fine-tuning models, and leveraging foundation models. Read their OpenAI Fine-Tuning guide first, then try the Cookbook exercise. Community Tutorial Phinetuning 2.0 - fine-tuning for Small Language Models Meet Phi-2, Microsoft\u2019s new small model, remarkably powerful yet compact. This tutorial will guide you through fine-tuning Phi-2, demonstrating how to build a unique dataset and fine-tune model using QLoRA. Hugging Face Tutorial How to Fine-Tune LLMs in 2024 with Hugging Face This blog post walks you thorugh how to fine-tune open LLMs using Hugging Face TRL, Transformers &amp; datasets in 2024. You define a use case, setup a dev environment, prepare a dataset, fine tune the model, test-evaluate it, then deploy it to production. Hugging Face: AutoTrain Advanced Brings faster and easier training and deployments of state-of-the-art machine learning models. Repo has Colab-friendly tutorials with YouTube video guidance, for fine-tuning. Reflects recent local-first update . Read the AutoTrain documentation"},{"location":"lessons/generative-ai-for-beginners/18-fine-tuning/RESOURCES/","title":"RESOURCES","text":""},{"location":"lessons/generative-ai-for-beginners/18-fine-tuning/RESOURCES/#resources-for-self-guided-learning","title":"Resources For Self-Guided Learning","text":"<p>The lesson was built using a number of core resources from OpenAI and Azure OpenAI as references for the terminology and tutorials. Here is a non-comprehensive list, for your own self-guided learning journeys.</p>"},{"location":"lessons/generative-ai-for-beginners/18-fine-tuning/RESOURCES/#1-primary-resources","title":"1. Primary Resources","text":"Title/Link Description Fine-tuning with OpenAI Models Fine-tuning improves on few-shot learning by training on many more examples than can fit in the prompt, saving you costs, improving response quality, and enabling lower-latency requests. Get an overview of fine-tuning from OpenAI. What is Fine-Tuning with Azure OpenAI? Understand what fine-tuning is (concept), why you should look at it (motivating problem), what data to use (training) and measuring the quality Customize a model with fine-tuning Azure OpenAI Service lets you tailor our models to your personal datasets using fine-tuning. Learn how to fine-tune (process) select models using Azure AI Studio, Python SDK or REST API. Recommendations for LLM fine-tuning LLMs may not perform well on specific domains, tasks, or datasets, or may produce inaccurate or misleading outputs. When should you consider fine-tuning as a possible solution to this? Continuous Fine Tuning Continuous fine-tuning is the iterative process of selecting an already fine-tuned model as a base model and fine-tuning it further on new sets of training examples. Fine-tuning and function calling Fine-tuning your model with function calling examples can improve model output by getting more accurate and consistent outputs - with similarly-formatted responses &amp; cost-savings Fine-tuning Models: Azure OpenAI Guidance Look up this table to understand what models can be fine-tuned in Azure OpenAI, and which regions these are available in. Look up their token limits and training data expiry dates if needed. To Fine Tune or Not To Fine Tune? That is the Question This 30-min Oct 2023 episode of the AI Show discusses benefits, drawbacks and practical insights that help you make this decision. Getting Started With LLM Fine-Tuning This AI Playbook resource walks you through data requirements, formatting, hyperparameter fine-tuning and challenges/limitations you should know. Tutorial: Azure OpenAI GPT3.5 Turbo Fine-Tuning Learn to create a sample fine-tuning dataset, prepare for fine-tuning, create a fine-tuning job, and deploy the fine-tuned model on Azure. Tutorial: Fine-tune a Llama 2 model in Azure AI Studio Azure AI Studio lets you tailor large language models to your personal datasets using a UI-based workflow suitable for low-code developers. See this example. Tutorial:Fine-tune Hugging Face models for a single GPU on Azure This article describes how to fine-tune a Hugging Face model with the Hugging Face transformers library on a single GPU with Azure DataBricks + Hugging Face Trainer libraries Training: Fine-tune a foundation model with Azure Machine Learning The model catalog in Azure Machine Learning offers many open source models you can fine-tune for your specific task. Try this module is from the AzureML Generative AI Learning Path Tutorial: Azure OpenAI Fine-Tuning Fine-tuning GPT-3.5 or GPT-4 models on Microsoft Azure using W&amp;B allows for detailed tracking and analysis of model performance. This guide extends the concepts from the OpenAI Fine-Tuning guide with specific steps and features for Azure OpenAI."},{"location":"lessons/generative-ai-for-beginners/18-fine-tuning/RESOURCES/#2-secondary-resources","title":"2. Secondary Resources","text":"<p>This section captures additional resources that are worth exploring, but that we did not have time to cover in this lesson. They may be covered in a future lesson, or as a secondary assignment option, at a later date. For now, use them to build your own expertise and knowledge around this topic.</p> Title/Link Description OpenAI Cookbook: Data preparation and analysis for chat model fine-tuning This notebook serves as a tool to preprocess and analyze the chat dataset used for fine-tuning a chat model. It checks for format errors, provides basic statistics, and estimates token counts for fine-tuning costs. See: Fine-tuning method for gpt-3.5-turbo. OpenAI Cookbook: Fine-Tuning for Retrieval Augmented Generation (RAG) with Qdrant The aim of this notebook is to walk through a comprehensive example of how to fine-tune OpenAI models for Retrieval Augmented Generation (RAG). We will also be integrating Qdrant and Few-Shot Learning to boost model performance and reduce fabrications. OpenAI Cookbook: Fine-tuning GPT with Weights &amp; Biases Weights &amp; Biases (W&amp;B) is the AI developer platform, with tools for training models, fine-tuning models, and leveraging foundation models. Read their OpenAI Fine-Tuning guide first, then try the Cookbook exercise. Community Tutorial Phinetuning 2.0 - fine-tuning for Small Language Models Meet Phi-2, Microsoft\u2019s new small model, remarkably powerful yet compact. This tutorial will guide you through fine-tuning Phi-2, demonstrating how to build a unique dataset and fine-tune model using QLoRA. Hugging Face Tutorial How to Fine-Tune LLMs in 2024 with Hugging Face This blog post walks you thorugh how to fine-tune open LLMs using Hugging Face TRL, Transformers &amp; datasets in 2024. You define a use case, setup a dev environment, prepare a dataset, fine tune the model, test-evaluate it, then deploy it to production. Hugging Face: AutoTrain Advanced Brings faster and easier training and deployments of state-of-the-art machine learning models. Repo has Colab-friendly tutorials with YouTube video guidance, for fine-tuning. Reflects recent local-first update . Read the AutoTrain documentation"},{"location":"lessons/generative-ai-for-beginners/19-slm/","title":"Lesson 19","text":""},{"location":"lessons/generative-ai-for-beginners/19-slm/#introduction-to-small-language-models-for-generative-ai-for-beginners","title":"Introduction to Small Language Models for Generative AI for Beginners","text":"<p>Generative AI is a fascinating field of artificial intelligence that focuses on creating systems capable of generating new content. This content can range from text and images to music and even entire virtual environments. One of the most exciting applications of generative AI is in the realm of language models.</p>"},{"location":"lessons/generative-ai-for-beginners/19-slm/#what-are-small-language-models","title":"What Are Small Language Models?","text":"<p>A Small Language Model (SLM) represents a scaled-down variant of a large language model (LLM), leveraging many of the architectural principles and techniques of LLMs, while exhibiting a significantly reduced computational footprint. </p> <p>SLMs are a subset of language models designed to generate human-like text. Unlike their larger counterparts, such as GPT-4, SLMs are more compact and efficient, making them ideal for applications where computational resources are limited. Despite their smaller size, they can still perform a variety of tasks. Typically, SLMs are constructed by compressing or distilling LLMs, aiming to retain a substantial portion of the original model's functionality and linguistic capabilities. This reduction in model size decreases the overall complexity, making SLMs more efficient in terms of both memory usage and computational requirements. Despite these optimizations, SLMs can still perform a wide range of natural language processing (NLP) tasks:</p> <ul> <li>Text Generation: Creating coherent and contextually relevant sentences or paragraphs.</li> <li>Text Completion: Predicting and completing sentences based on a given prompt.</li> <li>Translation: Converting text from one language to another.</li> <li>Summarization: Condensing long pieces of text into shorter, more digestible summaries.</li> </ul> <p>Albeit with some trade-offs in performance or depth of understanding compared to their larger counterparts. </p>"},{"location":"lessons/generative-ai-for-beginners/19-slm/#how-do-small-language-models-work","title":"How Do Small Language Models Work?","text":"<p>SLMs are trained on vast amounts of text data. During training, they learn the patterns and structures of language, enabling them to generate text that is both grammatically correct and contextually appropriate. The training process involves:</p> <ul> <li>Data Collection: Gathering large datasets of text from various sources.</li> <li>Preprocessing: Cleaning and organizing the data to make it suitable for training.</li> <li>Training: Using machine learning algorithms to teach the model how to understand and generate text.</li> <li>Fine-Tuning: Adjusting the model to improve its performance on specific tasks.</li> </ul> <p>The development of SLMs aligns with the increasing need for models that can be deployed in resource-constrained environments, such as mobile devices or edge computing platforms, where full-scale LLMs may be impractical due to their heavy resource demands. By focusing on efficiency, SLMs balance performance with accessibility, enabling broader application across various domains.</p> <p></p>"},{"location":"lessons/generative-ai-for-beginners/19-slm/#learning-objectives","title":"Learning Objectives","text":"<p>In this lesson, we hope to introduce the knowledge of SLM and combine it with Microsoft Phi-3 to learn different scenarios in text content, vision and MoE.</p> <p>By the end of this lesson, you should be able to answer the following questions:</p> <ul> <li>What is SLM</li> <li>What is the difference about SLM and LLM</li> <li>What is Microsoft Phi-3/3.5 Family</li> <li>How to inference Microsoft Phi-3/3.5 Family</li> </ul> <p>Ready? Let's get started.</p>"},{"location":"lessons/generative-ai-for-beginners/19-slm/#the-distinctions-between-large-language-models-llms-and-small-language-models-slms","title":"The Distinctions between Large Language Models (LLMs) and Small Language Models (SLMs)","text":"<p>Both LLMs and SLMs are built upon foundational principles of probabilistic machine learning, following similar approaches in their architectural design, training methodologies, data generation processes, and model evaluation techniques. However, several key factors differentiate these two types of models.</p>"},{"location":"lessons/generative-ai-for-beginners/19-slm/#applications-of-small-language-models","title":"Applications of Small Language Models","text":"<p>SLMs have a wide range of applications, including:</p> <ul> <li>Chatbots: Providing customer support and engaging with users in a conversational manner.</li> <li>Content Creation: Assisting writers by generating ideas or even drafting entire articles.</li> <li>Education: Helping students with writing assignments or learning new languages.</li> <li>Accessibility: Creating tools for individuals with disabilities, such as text-to-speech systems.</li> </ul> <p>Size</p> <p>A primary distinction between LLMs and SLMs lies in the scale of the models. LLMs, such as ChatGPT (GPT-4), can comprise an estimated 1.76 trillion parameters, while open-source SLMs like Mistral 7B are designed with significantly fewer parameters\u2014approximately 7 billion. This disparity is primarily due to differences in model architecture and training processes. For instance, ChatGPT employs a self-attention mechanism within an encoder-decoder framework, whereas Mistral 7B uses sliding window attention, which enables more efficient training within a decoder-only model. This architectural variance has profound implications for the complexity and performance of these models.</p> <p>Comprehension</p> <p>SLMs are typically optimized for performance within specific domains, making them highly specialized but potentially limited in their ability to provide broad contextual understanding across multiple fields of knowledge. In contrast, LLMs aim to simulate human-like intelligence on a more comprehensive level. Trained on vast, diverse datasets, LLMs are designed to perform well across a variety of domains, offering greater versatility and adaptability. Consequently, LLMs are more suitable for a wider range of downstream tasks, such as natural language processing and programming.</p> <p>Computing</p> <p>The training and deployment of LLMs are resource-intensive processes, often requiring significant computational infrastructure, including large-scale GPU clusters. For example, training a model like ChatGPT from scratch may necessitate thousands of GPUs over extended periods. In contrast, SLMs, with their smaller parameter counts, are more accessible in terms of computational resources. Models like Mistral 7B can be trained and run on local machines equipped with moderate GPU capabilities, although training still demands several hours across multiple GPUs.</p> <p>Bias</p> <p>Bias is a known issue in LLMs, primarily due to the nature of the training data. These models often rely on raw, openly available data from the internet, which may underrepresent or misrepresent certain groups, introduce erroneous labeling, or reflect linguistic biases influenced by dialect, geographic variations, and grammatical rules. Additionally, the complexity of LLM architectures can inadvertently exacerbate bias, which may go unnoticed without careful fine-tuning. On the other hand, SLMs, being trained on more constrained, domain-specific datasets, are inherently less susceptible to such biases, though they are not immune to them.</p> <p>Inference</p> <p>The reduced size of SLMs affords them a significant advantage in terms of inference speed, allowing them to generate outputs efficiently on local hardware without the need for extensive parallel processing. In contrast, LLMs, due to their size and complexity, often require substantial parallel computational resources to achieve acceptable inference times. The presence of multiple concurrent users further slows down LLMs' response times, especially when deployed at scale.</p> <p>In summary, while both LLMs and SLMs share a foundational basis in machine learning, they differ significantly in terms of model size, resource requirements, contextual understanding, susceptibility to bias, and inference speed. These distinctions reflect their respective suitability for different use cases, with LLMs being more versatile but resource-heavy, and SLMs offering more domain-specific efficiency with reduced computational demands.</p> <p>Note\uff1aIn this chapter, we will introduce SLM using Microsoft Phi-3 / 3.5 as an example.</p>"},{"location":"lessons/generative-ai-for-beginners/19-slm/#introduce-phi-3-phi-35-family","title":"Introduce Phi-3 / Phi-3.5 Family","text":"<p>Phi-3 / 3.5 Family mainly targets text, vision, and Agent (MoE) application scenarios:</p>"},{"location":"lessons/generative-ai-for-beginners/19-slm/#phi-3-35-instruct","title":"Phi-3 / 3.5 Instruct","text":"<p>Mainly for text generation, chat completion, and content information extraction, etc.</p> <p>Phi-3-mini</p> <p>The 3.8B language model is available on Microsoft Azure AI Studio, Hugging Face, and Ollama. Phi-3 models significantly outperform language models of equal and larger sizes on key benchmarks (see benchmark numbers below, higher numbers are better). Phi-3-mini outperforms models twice its size, while Phi-3-small and Phi-3-medium outperform larger models, including GPT-3.5</p> <p>Phi-3-small &amp; medium</p> <p>With just 7B parameters, Phi-3-small beats GPT-3.5T on a variety of language, reasoning, coding, and math benchmarks.</p> <p>The Phi-3-medium with 14B parameters continues this trend and outperforms the Gemini 1.0 Pro.</p> <p>Phi-3.5-mini</p> <p>We can think of it as an upgrade of Phi-3-mini. While the parameters remain unchanged, it improves the ability to support multiple languages( Support 20+ languages:Arabic, Chinese, Czech, Danish, Dutch, English, Finnish, French, German, Hebrew, Hungarian, Italian, Japanese, Korean, Norwegian, Polish, Portuguese, Russian, Spanish, Swedish, Thai, Turkish, Ukrainian) \u200b\u200band adds stronger support for long context.</p> <p>Phi-3.5-mini with 3.8B parameters outperforms language models of the same size and on par with models twice its size.</p>"},{"location":"lessons/generative-ai-for-beginners/19-slm/#phi-3-35-vision","title":"Phi-3 / 3.5 Vision","text":"<p>We can think of the Instruct model of Phi-3/3.5 as Phi's ability to understand, and Vision is what gives Phi eyes to understand the world.</p> <p>Phi-3-Vision</p> <p>Phi-3-vision, with only 4.2B parameters, continues this trend and outperforms larger models such as Claude-3 Haiku and Gemini 1.0 Pro V on general visual reasoning tasks, OCR, and table and diagram comprehension tasks.</p> <p>Phi-3.5-Vision</p> <p>Phi-3.5-Vision is also an upgrade of Phi-3-Vision, adding support for multiple images. You can think of it as an improvement in vision, not only can you see pictures, but also videos.</p> <p>Phi-3.5-vision outperforms larger models such as Claude-3.5 Sonnet and Gemini 1.5 Flash across OCR, table and chart understanding tasks and on par on general visual knowledge reasoning tasks.Support multi-frame input, i.e., perform reasoning on multiple input images</p>"},{"location":"lessons/generative-ai-for-beginners/19-slm/#phi-35-moe","title":"Phi-3.5-MoE","text":"<p>Mixture of Experts(MoE) enable models to be pretrained with far less compute, which means you can dramatically scale up the model or dataset size with the same compute budget as a dense model. In particular, a MoE model should achieve the same quality as its dense counterpart much faster during pretraining.</p> <p>Phi-3.5-MoE comprises 16x3.8B expert modules.Phi-3.5-MoE with only 6.6B active parameters achieves a similar level of reasoning, language understanding, and math as much larger models</p> <p>We can use the Phi-3/3.5 Family model based on different scenarios. Unlike LLM, you can deploy Phi-3/3.5-mini or Phi-3/3.5-Vision on edge devices.</p>"},{"location":"lessons/generative-ai-for-beginners/19-slm/#how-to-use-phi-335-family-models","title":"How to use Phi-3/3.5 Family models","text":"<p>We hope to use Phi-3/3.5 in different scenarios. Next, we will use Phi-3/3.5 based on different scenarios.</p> <p></p>"},{"location":"lessons/generative-ai-for-beginners/19-slm/#inference-difference-clouds-api","title":"Inference difference Cloud's API","text":"<p>GitHub Models</p> <p>GitHub Models is the most direct way. You can quickly access the Phi-3/3.5-Instruct model through GitHub Models. Combined with the Azure AI Inference SDK / OpenAI SDK, you can access the API through code to complete the Phi-3/3.5-Instruct call. You can also test different effects through Playground.</p> <ul> <li>Demo:Comparison of the effects of Phi-3-mini and Phi-3.5-mini in Chinese scenarios</li> </ul> <p></p> <p></p> <p>Azure AI Studio</p> <p>Or if we want to use the vision and MoE models, you can use Azure AI Studio to complete the call. If you are interested, you can read the Phi-3 Cookbook to learn how to call Phi-3/3.5 Instruct, Vision, MoE through Azure AI Studio Click this link</p> <p>NVIDIA NIM</p> <p>In addition to the cloud-based Model Catalog solutions provided by Azure and GitHub, you can also use Nivida NIM to complete related calls. You can visit NIVIDA NIM to complete the API calls of Phi-3/3.5 Family. NVIDIA NIM (NVIDIA Inference Microservices) is a set of accelerated inference microservices designed to help developers deploy AI models efficiently across various environments, including clouds, data centers, and workstations.</p> <p>Here are some key features of NVIDIA NIM:</p> <ul> <li>Ease of Deployment: NIM allows for the deployment of AI models with a single command, making it straightforward to integrate into existing workflows.</li> <li>Optimized Performance: It leverages NVIDIA\u2019s pre-optimized inference engines, such as TensorRT and TensorRT-LLM, to ensure low latency and high throughput.</li> <li>Scalability: NIM supports autoscaling on Kubernetes, enabling it to handle varying workloads effectively.</li> <li>Security and Control: Organizations can maintain control over their data and applications by self-hosting NIM microservices on their own managed infrastructure.</li> <li>Standard APIs: NIM provides industry-standard APIs, making it easy to build and integrate AI applications like chatbots, AI assistants, and more.</li> </ul> <p>NIM is part of NVIDIA AI Enterprise, which aims to simplify the deployment and operationalization of AI models, ensuring they run efficiently on NVIDIA GPUs.</p> <ul> <li>Demo: Using Nividia NIM to call Phi-3.5-Vision-API  [Click this link]</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/19-slm/#inference-phi-335-in-local-env","title":"Inference Phi-3/3.5 in local env","text":"<p>Inference in relation to Phi-3, or any language model like GPT-3, refers to the process of generating responses or predictions based on the input it receives. When you provide a prompt or question to Phi-3, it uses its trained neural network to infer the most likely and relevant response by analyzing patterns and relationships in the data it was trained on.</p> <p>Hugging face Transformer Hugging Face Transformers is a powerful library designed for natural language processing (NLP) and other machine learning tasks. Here are some key points about it:</p> <ol> <li> <p>Pretrained Models: It provides thousands of pretrained models that can be used for various tasks such as text classification, named entity recognition, question answering, summarization, translation, and text generation.</p> </li> <li> <p>Framework Interoperability: The library supports multiple deep learning frameworks, including PyTorch, TensorFlow, and JAX. This allows you to train a model in one framework and use it in another.</p> </li> <li> <p>Multimodal Capabilities: Besides NLP, Hugging Face Transformers also supports tasks in computer vision (e.g., image classification, object detection) and audio processing (e.g., speech recognition, audio classification).</p> </li> <li> <p>Ease of Use: The library offers APIs and tools to easily download and fine-tune models, making it accessible for both beginners and experts.</p> </li> <li> <p>Community and Resources: Hugging Face has a vibrant community and extensive documentation, tutorials, and guides to help users get started and make the most of the library. official documentation or their GitHub repository.</p> </li> </ol> <p>This is the most commonly used method, but it also requires GPU acceleration. After all, scenes such as Vision and MoE require a lot of calculations, which will be very limited in the CPU if they are not quantized.</p> <ul> <li> <p>Demo:Using Transformer to call Phi-3.5-Instuct Click this link</p> </li> <li> <p>Demo:Using Transformer to call Phi-3.5-VisionClick this link</p> </li> <li> <p>Demo:Using Transformer to call Phi-3.5-MoEClick this link</p> </li> </ul> <p>Ollama Ollama is a platform designed to make it easier to run large language models (LLMs) locally on your machine. It supports various models like Llama 3.1, Phi 3, Mistral, and Gemma 2, among others. The platform simplifies the process by bundling model weights, configuration, and data into a single package, making it more accessible for users to customize and create their own models. Ollama is available for macOS, Linux, and Windows. It\u2019s a great tool if you\u2019re looking to experiment with or deploy LLMs without relying on cloud services. Ollama is the most direct way, you just need to execute the following statement.</p> <pre><code>ollama run phi3.5\n</code></pre> <p>ONNX Runtime for GenAI</p> <p>ONNX Runtime is a cross-platform inference and training machine-learning accelerator. ONNX Runtime for Generative AI (GENAI) is a powerful tool that helps you run generative AI models efficiently across various platforms. </p>"},{"location":"lessons/generative-ai-for-beginners/19-slm/#what-is-onnx-runtime","title":"What is ONNX Runtime?","text":"<p>ONNX Runtime is an open-source project that enables high-performance inference of machine learning models. It supports models in the Open Neural Network Exchange (ONNX) format, which is a standard for representing machine learning models.ONNX Runtime inference can enable faster customer experiences and lower costs, supporting models from deep learning frameworks such as PyTorch and TensorFlow/Keras as well as classical machine learning libraries such as scikit-learn, LightGBM, XGBoost, etc. ONNX Runtime is compatible with different hardware, drivers, and operating systems, and provides optimal performance by leveraging hardware accelerators where applicable alongside graph optimizations and transforms</p>"},{"location":"lessons/generative-ai-for-beginners/19-slm/#what-is-generative-ai","title":"What is Generative AI?","text":"<p>Generative AI refers to AI systems that can generate new content, such as text, images, or music, based on the data they have been trained on. Examples include language models like GPT-3 and image generation models like Stable Diffusion. ONNX Runtime for GenAI library provides the generative AI loop for ONNX models, including inference with ONNX Runtime, logits processing, search and sampling, and KV cache management.</p>"},{"location":"lessons/generative-ai-for-beginners/19-slm/#onnx-runtime-for-genai","title":"ONNX Runtime for GENAI","text":"<p>ONNX Runtime for GENAI extends the capabilities of ONNX Runtime to support generative AI models. Here are some key features:</p> <ul> <li>Broad Platform Support: It works on various platforms, including Windows, Linux, macOS, Android, and iOS.</li> <li>Model Support: It supports many popular generative AI models, such as LLaMA, GPT-Neo, BLOOM, and more.</li> <li>Performance Optimization: It includes optimizations for different hardware accelerators like NVIDIA GPUs, AMD GPUs, and more2.</li> <li>Ease of Use: It provides APIs for easy integration into applications, allowing you to generate text, images, and other content with minimal code</li> <li>Users can call a high level generate() method, or run each iteration of the model in a loop, generating one token at a time, and optionally updating generation parameters inside the loop.</li> <li>ONNX runtume also has support for greedy/beam search and TopP, TopK sampling to generate token sequences and built-in logits processing like repetition penalties. You can also easily add custom scoring.</li> </ul>"},{"location":"lessons/generative-ai-for-beginners/19-slm/#getting-started","title":"Getting Started","text":"<p>To get started with ONNX Runtime for GENAI, you can follow these steps:</p>"},{"location":"lessons/generative-ai-for-beginners/19-slm/#install-onnx-runtime","title":"Install ONNX Runtime:","text":"<pre><code>pip install onnxruntime\n</code></pre>"},{"location":"lessons/generative-ai-for-beginners/19-slm/#install-the-generative-ai-extensions","title":"Install the Generative AI Extensions:","text":"<pre><code>pip install onnxruntime-genai\n</code></pre>"},{"location":"lessons/generative-ai-for-beginners/19-slm/#run-a-model-heres-a-simple-example-in-python","title":"Run a Model: Here\u2019s a simple example in Python:","text":"<pre><code>import onnxruntime_genai as og\n\nmodel = og.Model('path_to_your_model.onnx')\n\ntokenizer = og.Tokenizer(model)\n\ninput_text = \"Hello, how are you?\"\n\ninput_tokens = tokenizer.encode(input_text)\n\noutput_tokens = model.generate(input_tokens)\n\noutput_text = tokenizer.decode(output_tokens)\n\nprint(output_text) \n</code></pre>"},{"location":"lessons/generative-ai-for-beginners/19-slm/#demousing-onnx-runtime-genai-to-call-phi-35-vision","title":"Demo:Using ONNX Runtime GenAI to call Phi-3.5-Vision","text":"<pre><code>import onnxruntime_genai as og\n\nmodel_path = './Your Phi-3.5-vision-instruct ONNX Path'\n\nimg_path = './Your Image Path'\n\nmodel = og.Model(model_path)\n\nprocessor = model.create_multimodal_processor()\n\ntokenizer_stream = processor.create_stream()\n\ntext = \"Your Prompt\"\n\nprompt = \"&lt;|user|&gt;\\n\"\n\nprompt += \"&lt;|image_1|&gt;\\n\"\n\nprompt += f\"{text}&lt;|end|&gt;\\n\"\n\nprompt += \"&lt;|assistant|&gt;\\n\"\n\nimage = og.Images.open(img_path)\n\ninputs = processor(prompt, images=image)\n\nparams = og.GeneratorParams(model)\n\nparams.set_inputs(inputs)\n\nparams.set_search_options(max_length=3072)\n\ngenerator = og.Generator(model, params)\n\nwhile not generator.is_done():\n\n    generator.compute_logits()\n\n    generator.generate_next_token()\n\n    new_token = generator.get_next_tokens()[0]\n\n    code += tokenizer_stream.decode(new_token)\n\n    print(tokenizer_stream.decode(new_token), end='', flush=True)\n</code></pre> <p>Others</p> <p>In addition to ONNX Runtime and Ollama reference methods, we can also complete the reference of quantitative models based on the model reference methods provided by different manufacturers. Such as Apple MLX framework with Apple Metal, Qualcomm QNN with NPU, Intel OpenVINO with CPU/GPU, etc. You can also get more content from Phi-3 Cookbook</p>"},{"location":"lessons/generative-ai-for-beginners/19-slm/#more","title":"More","text":"<p>We have learned the basics of Phi-3/3.5 Family, but to learn more about SLM we need more knowledge. You can find the answers in the Phi-3 Cookbook. If you want to learn more, please visit the Phi-3 Cookbook.</p>"},{"location":"lessons/generative-ai-for-beginners/20-mistral/","title":"Lesson 20","text":""},{"location":"lessons/generative-ai-for-beginners/20-mistral/#building-with-mistral-models","title":"Building with Mistral Models","text":""},{"location":"lessons/generative-ai-for-beginners/20-mistral/#introduction","title":"Introduction","text":"<p>This lesson will cover:  - Exploring the different Mistral Models  - Understanding the use-cases and scenarios for each model  - Code samples show the unique features of each model. </p>"},{"location":"lessons/generative-ai-for-beginners/20-mistral/#the-mistral-models","title":"The Mistral Models","text":"<p>In this lesson, we will explore 3 different Mistral models:  Mistral Large, Mistral Small and Mistral Nemo. </p> <p>Each of these models are available free on the Github Model marketplace. The code in this notebook will be using this models to run the code. Here are more details on using Github Models to prototype with AI models. </p>"},{"location":"lessons/generative-ai-for-beginners/20-mistral/#mistral-large-2-2407","title":"Mistral Large 2 (2407)","text":"<p>Mistral Large 2 is currently the flagship model from Mistral and is designed for enterprise use. </p> <p>The model is an  upgrade to the original Mistral Large by offering  -  Larger Context Window - 128k vs 32k  -  Better performance on Math and Coding Tasks - 76.9% average accuracy vs 60.4%  -  Increased multilingual performance - languages include: English, French, German, Spanish, Italian, Portuguese, Dutch, Russian, Chinese, Japanese, Korean, Arabic, and Hindi.</p> <p>With these features, Mistral Large excels at  - Retrieval Augmented Generation (RAG) - due to the larger context window - Function Calling - this model has native function calling which allows integration with external tools and APIs. These calls can be made both in parallel or one after another in a sequential order.  - Code Generation - this model excels on Python, Java, TypeScript and C++ generation. </p>"},{"location":"lessons/generative-ai-for-beginners/20-mistral/#rag-example-using-mistral-large-2","title":"RAG Example using Mistral Large 2","text":"<p>In this example, we are using Mistral Large 2 to run a RAG pattern over a text document. The question is written in Korean and asks about the author's activities before college. </p> <p>It uses Cohere Embeddings Model to create embeddings of the text document as well as the question. For this sample, it uses the faiss Python package as a vector store. </p> <p>The prompt sent to the Mistral model includes both the questions and the retrieved chunks that are similar to the question. The Model then provides a natural language response. </p> <pre><code>pip install faiss-cpu\n</code></pre> <pre><code>import requests\nimport numpy as np\nimport faiss\nimport os\n\nfrom azure.ai.inference import ChatCompletionsClient\nfrom azure.ai.inference.models import SystemMessage, UserMessage\nfrom azure.core.credentials import AzureKeyCredential\nfrom azure.ai.inference import EmbeddingsClient\n\nendpoint = \"https://models.inference.ai.azure.com\"\nmodel_name = \"Mistral-large\"\ntoken = os.environ[\"GITHUB_TOKEN\"]\n\nclient = ChatCompletionsClient(\n    endpoint=endpoint,\n    credential=AzureKeyCredential(token),\n)\n\nresponse = requests.get('https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt')\ntext = response.text\n\nchunk_size = 2048\nchunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\nlen(chunks)\n\nembed_model_name = \"cohere-embed-v3-multilingual\" \n\nembed_client = EmbeddingsClient(\n        endpoint=endpoint,\n        credential=AzureKeyCredential(token)\n)\n\nembed_response = embed_client.embed(\n    input=chunks,\n    model=embed_model_name\n)\n\n\n\ntext_embeddings = []\nfor item in embed_response.data:\n    length = len(item.embedding)\n    text_embeddings.append(item.embedding)\ntext_embeddings = np.array(text_embeddings)\n\n\nd = text_embeddings.shape[1]\nindex = faiss.IndexFlatL2(d)\nindex.add(text_embeddings)\n\nquestion = \"\uc800\uc790\uac00 \ub300\ud559\uc5d0 \uc624\uae30 \uc804\uc5d0 \uc8fc\ub85c \ud588\ub358 \ub450 \uac00\uc9c0 \uc77c\uc740 \ubb34\uc5c7\uc774\uc5c8\ub098\uc694?\uff1f\"\n\nquestion_embedding = embed_client.embed(\n    input=[question],\n    model=embed_model_name\n)\n\nquestion_embeddings = np.array(question_embedding.data[0].embedding)\n\n\nD, I = index.search(question_embeddings.reshape(1, -1), k=2) # distance, index\nretrieved_chunks = [chunks[i] for i in I.tolist()[0]]\n\nprompt = f\"\"\"\nContext information is below.\n---------------------\n{retrieved_chunks}\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: {question}\nAnswer:\n\"\"\"\n\n\nchat_response = client.complete(\n    messages=[\n        SystemMessage(content=\"You are a helpful assistant.\"),\n        UserMessage(content=prompt),\n    ],\n    temperature=1.0,\n    top_p=1.0,\n    max_tokens=1000,\n    model=model_name\n)\n\nprint(chat_response.choices[0].message.content)\n</code></pre>"},{"location":"lessons/generative-ai-for-beginners/20-mistral/#mistral-small","title":"Mistral Small","text":"<p>Mistral Small is another model in the Mistral family of models under the premier/enterprise category. As the name implies, this model is a Small Language Model (SLM). The advantages of using Mistral Small are that it is:  - Cost Saving compared to Mistral LLMs like Mistral Large and NeMo - 80% price drop - Low latency - faster response compared to Mistral's LLMs - Flexible - can be deployed across different environments with less restrictions on required resources. </p> <p>Mistral Small is great for:  - Text based tasks such as summarization, sentiment analysis and translation.  - Applications where frequent requests are made due to its cost effectiveness  - Low latency code tasks like review and code suggestions </p>"},{"location":"lessons/generative-ai-for-beginners/20-mistral/#comparing-mistral-small-and-mistral-large","title":"Comparing Mistral Small and Mistral Large","text":"<p>To show differences in latency between Mistral Small and Large, run the below cells. </p> <p>You should see a difference in response times between 3-5 seconds. Also note the response lengths and style over the same prompt.  </p> <pre><code>import os \nendpoint = \"https://models.inference.ai.azure.com\"\nmodel_name = \"Mistral-small\"\ntoken = os.environ[\"GITHUB_TOKEN\"]\n\nclient = ChatCompletionsClient(\n    endpoint=endpoint,\n    credential=AzureKeyCredential(token),\n)\n\nresponse = client.complete(\n    messages=[\n        SystemMessage(content=\"You are a helpful coding assistant.\"),\n        UserMessage(content=\"Can you write a Python function to the fizz buzz test?\"),\n    ],\n    temperature=1.0,\n    top_p=1.0,\n    max_tokens=1000,\n    model=model_name\n)\n\nprint(response.choices[0].message.content)\n</code></pre> <pre><code>import os\nfrom azure.ai.inference import ChatCompletionsClient\nfrom azure.ai.inference.models import SystemMessage, UserMessage\nfrom azure.core.credentials import AzureKeyCredential\n\nendpoint = \"https://models.inference.ai.azure.com\"\nmodel_name = \"Mistral-large\"\ntoken = os.environ[\"GITHUB_TOKEN\"]\n\nclient = ChatCompletionsClient(\n    endpoint=endpoint,\n    credential=AzureKeyCredential(token),\n)\n\nresponse = client.complete(\n    messages=[\n        SystemMessage(content=\"You are a helpful coding assistant.\"),\n        UserMessage(content=\"Can you write a Python function to the fizz buzz test?\"),\n    ],\n    temperature=1.0,\n    top_p=1.0,\n    max_tokens=1000,\n    model=model_name\n)\n\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"lessons/generative-ai-for-beginners/20-mistral/#mistral-nemo","title":"Mistral NeMo","text":"<p>Compared to the other two models discussed in this lesson, Mistral NeMo is the only free model with an Apache2 License. </p> <p>It is viewed as an upgrade to the earlier open source LLM from Mistral, Mistral 7B. </p> <p>Some other feature of the NeMo model are: </p> <ul> <li> <p>More efficient tokenization: This model using the Tekken tokenizer over the more commonly used tiktoken. This allows for better performance over more languages and code. </p> </li> <li> <p>Finetuning: The base model is available for finetuning. This allows for more flexibility for use-cases where finetuning may be needed. </p> </li> <li> <p>Native Function Calling - Like Mistral Large, this model has been trained on function calling. This makes it unique as being one of the first open source models to do so. </p> </li> </ul>"},{"location":"lessons/generative-ai-for-beginners/20-mistral/#comparing-tokenizers","title":"Comparing Tokenizers","text":"<p>In this sample, we will look at how Mistral NeMo handles tokenization compared to Mistral Large. </p> <p>Both samples take the same prompt but you should see that NeMo returns back less tokens vs Mistral Large. </p> <pre><code>pip install mistral-common\n</code></pre> <pre><code># Import needed packages:\nfrom mistral_common.protocol.instruct.messages import (\n    UserMessage,\n)\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\nfrom mistral_common.protocol.instruct.tool_calls import (\n    Function,\n    Tool,\n)\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n\n# Load Mistral tokenizer\n\nmodel_name = \"open-mistral-nemo \"\n\ntokenizer = MistralTokenizer.from_model(model_name)\n\n# Tokenize a list of messages\ntokenized = tokenizer.encode_chat_completion(\n    ChatCompletionRequest(\n        tools=[\n            Tool(\n                function=Function(\n                    name=\"get_current_weather\",\n                    description=\"Get the current weather\",\n                    parameters={\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"location\": {\n                                \"type\": \"string\",\n                                \"description\": \"The city and state, e.g. San Francisco, CA\",\n                            },\n                            \"format\": {\n                                \"type\": \"string\",\n                                \"enum\": [\"celsius\", \"fahrenheit\"],\n                                \"description\": \"The temperature unit to use. Infer this from the users location.\",\n                            },\n                        },\n                        \"required\": [\"location\", \"format\"],\n                    },\n                )\n            )\n        ],\n        messages=[\n            UserMessage(content=\"What's the weather like today in Paris\"),\n        ],\n        model=model_name,\n    )\n)\ntokens, text = tokenized.tokens, tokenized.text\n\n# Count the number of tokens\nprint(len(tokens))\n</code></pre> <pre><code># Import needed packages:\nfrom mistral_common.protocol.instruct.messages import (\n    UserMessage,\n)\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\nfrom mistral_common.protocol.instruct.tool_calls import (\n    Function,\n    Tool,\n)\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n\n# Load Mistral tokenizer\n\nmodel_name = \"mistral-large-latest\"\n\ntokenizer = MistralTokenizer.from_model(model_name)\n\n# Tokenize a list of messages\ntokenized = tokenizer.encode_chat_completion(\n    ChatCompletionRequest(\n        tools=[\n            Tool(\n                function=Function(\n                    name=\"get_current_weather\",\n                    description=\"Get the current weather\",\n                    parameters={\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"location\": {\n                                \"type\": \"string\",\n                                \"description\": \"The city and state, e.g. San Francisco, CA\",\n                            },\n                            \"format\": {\n                                \"type\": \"string\",\n                                \"enum\": [\"celsius\", \"fahrenheit\"],\n                                \"description\": \"The temperature unit to use. Infer this from the users location.\",\n                            },\n                        },\n                        \"required\": [\"location\", \"format\"],\n                    },\n                )\n            )\n        ],\n        messages=[\n            UserMessage(content=\"What's the weather like today in Paris\"),\n        ],\n        model=model_name,\n    )\n)\ntokens, text = tokenized.tokens, tokenized.text\n\n# Count the number of tokens\nprint(len(tokens))\n</code></pre>"},{"location":"lessons/generative-ai-for-beginners/20-mistral/#learning-does-not-stop-here-continue-the-journey","title":"Learning does not stop here, continue the Journey","text":"<p>After completing this lesson, check out our Generative AI Learning collection to continue leveling up your Generative AI knowledge!</p>"},{"location":"lessons/generative-ai-for-beginners/21-meta/","title":"Lesson 21","text":""},{"location":"lessons/generative-ai-for-beginners/21-meta/#building-with-the-meta-family-models","title":"Building With the Meta Family Models","text":""},{"location":"lessons/generative-ai-for-beginners/21-meta/#introduction","title":"Introduction","text":"<p>This lesson will cover: </p> <ul> <li>Exploring the two main Meta family models - Llama 3.1 and Llama 3.2 </li> <li>Understanding the use-cases and scenarios for each model </li> <li>Code sample to show the unique features of each model </li> </ul>"},{"location":"lessons/generative-ai-for-beginners/21-meta/#the-meta-family-of-models","title":"The Meta Family of Models","text":"<p>In this lesson, we will explore 2 models from the Meta family or \"Llama Herd\" - Llama 3.1 and Llama 3.2 </p> <p>These models come in different variants and are available on the Github Model marketplace. Here are more details on using Github Models to prototype with AI models.</p> <p>Model Variants:  - Llama 3.1 - 70B Instruct  - Llama 3.1 - 405B Instruct  - Llama 3.2 - 11B Vision Instruct  - Llama 3.2 - 90B Vision Instruct </p> <p>Note: Llama 3 is also available on Github Models but won't be covered in this lesson</p>"},{"location":"lessons/generative-ai-for-beginners/21-meta/#llama-31","title":"Llama 3.1","text":"<p>At 405 Billion Parameters, Llama 3.1 fits into the open source LLM category. </p> <p>The mode is an upgrade to the earlier release Llama 3 by offering: </p> <ul> <li>Larger context window - 128k tokens vs 8k tokens </li> <li>Larger Max Output Tokens - 4096 vs 2048 </li> <li>Better Multilingual Support - due to increase in training tokens </li> </ul> <p>These enables Llama 3.1 to handle more complex use cases  when building GenAI applications including:  - Native Function Calling - the ability to call external tools and functions outside of the LLM workflow - Better RAG Performance - due to the higher context window  - Synthetic Data Generation - the ability to create effective data for tasks such as fine-tuning </p>"},{"location":"lessons/generative-ai-for-beginners/21-meta/#native-function-calling","title":"Native Function Calling","text":"<p>Llama 3.1 has been fine-tuned to be more effective at making function or tool calls. It also has two built-in tools that the model can identify as needing to be used based on the prompt from the user. These tools are: </p> <ul> <li>Brave Search - Can be used to get up-to-date information like the weather by performing a web search </li> <li>Wolfram Alpha - Can be used for more complex mathematical calculations so writing your own functions is not required. </li> </ul> <p>You can also create your own custom tools that LLM can call. </p> <p>In the code example below: </p> <ul> <li>We define the available tools (brave_search, wolfram_alpha) in the system prompt. </li> <li>Send a user prompt that asks about the weather in a certain city. </li> <li>The LLM will respond with a tool call to the Brave Search tool which will look like this <code>&lt;|python_tag|&gt;brave_search.call(query=\"Stockholm weather\")</code> </li> </ul> <p>*Note: This example only make the tool call, if you would like to get the results, you will need to create a free account on the Brave API page and define the function itself` </p> <pre><code>import os\nfrom azure.ai.inference import ChatCompletionsClient\nfrom azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage\nfrom azure.core.credentials import AzureKeyCredential\n\ntoken = os.environ[\"GITHUB_TOKEN\"]\nendpoint = \"https://models.inference.ai.azure.com\"\nmodel_name = \"meta-llama-3.1-405b-instruct\"\n\nclient = ChatCompletionsClient(\n    endpoint=endpoint,\n    credential=AzureKeyCredential(token),\n)\n\n\ntool_prompt=f\"\"\"\n&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\n\nEnvironment: ipython\nTools: brave_search, wolfram_alpha\nCutting Knowledge Date: December 2023\nToday Date: 23 July 2024\n\nYou are a helpful assistant&lt;|eot_id|&gt;\n\"\"\"\n\nmessages = [\n    SystemMessage(content=tool_prompt),\n    UserMessage(content=\"What is the weather in Stockholm?\"),\n\n]\n\nresponse = client.complete(messages=messages, model=model_name)\n\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"lessons/generative-ai-for-beginners/21-meta/#llama-32","title":"Llama 3.2","text":"<p>Despite being a LLM, one limitation that Llama 3.1 has is multimodality. That is, being able to use different types of input such as images as prompts and providing responses. This ability is one of the main features of Llama 3.2. These features also include: </p> <ul> <li>Multimodality -  has the ability to evaluate both text and image prompts </li> <li>Small to Medium size variations (11B and 90B) - this provides flexible deployment options, </li> <li>Text-only variations (1B and 3B) - this allows the model to be deployed on edge / mobile devices and provides low latency </li> </ul> <p>The multimodal support represents a big step in the world of open source models. The code example below takes both an image and text prompt to get an analysis of the image from Llama 3.2 90B. </p>"},{"location":"lessons/generative-ai-for-beginners/21-meta/#multimodal-support-with-llama-32","title":"Multimodal Support with Llama 3.2","text":"<pre><code>import os\nfrom azure.ai.inference import ChatCompletionsClient\nfrom azure.ai.inference.models import (\n    SystemMessage,\n    UserMessage,\n    TextContentItem,\n    ImageContentItem,\n    ImageUrl,\n    ImageDetailLevel,\n)\nfrom azure.core.credentials import AzureKeyCredential\n\ntoken = os.environ[\"GITHUB_TOKEN\"]\nendpoint = \"https://models.inference.ai.azure.com\"\nmodel_name = \"Llama-3.2-90B-Vision-Instruct\"\n\nclient = ChatCompletionsClient(\n    endpoint=endpoint,\n    credential=AzureKeyCredential(token),\n)\n\nresponse = client.complete(\n    messages=[\n        SystemMessage(\n            content=\"You are a helpful assistant that describes images in details.\"\n        ),\n        UserMessage(\n            content=[\n                TextContentItem(text=\"What's in this image?\"),\n                ImageContentItem(\n                    image_url=ImageUrl.load(\n                        image_file=\"sample.jpg\",\n                        image_format=\"jpg\",\n                        detail=ImageDetailLevel.LOW)\n                ),\n            ],\n        ),\n    ],\n    model=model_name,\n)\n\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"lessons/generative-ai-for-beginners/21-meta/#learning-does-not-stop-here-continue-the-journey","title":"Learning does not stop here, continue the Journey","text":"<p>After completing this lesson, check out our Generative AI Learning collection to continue leveling up your Generative AI knowledge!</p>"},{"location":"oi-tips/-qread-and-qwrite/","title":"qread and qwrite","text":""},{"location":"oi-tips/-qread-and-qwrite/#most-common-quick-read-write-function","title":"Most Common Quick Read &amp; Write Function","text":""},{"location":"oi-tips/-qread-and-qwrite/#quick-read-getchar","title":"Quick Read <code>getchar()</code>","text":"<p>head file: <code>&lt;cctype&gt;</code></p> intAll TypesMulti-Var <pre><code>int read() {\n    int x=0, f=1; char ch=getchar();\n    while (!isdigit(ch)) f = ((ch=='-') ? -1 : f);\n    while (isdigt(ch)) x = (x&lt;&lt;1) + (x&lt;&lt;3) + (ch ^ 48), ch = getchar();\n    return x * f;\n}\n</code></pre> <pre><code>template&lt;typename type&gt;\nvoid read(type &amp;x) {\n    x=0; bool f(0); char ch=getchar();\n    while(!isdigit(ch)) f = ch =='-', ch =getchar();\n    while(isdigit(ch)) x = (x&lt;&lt;1) + (x&lt;&lt;3) + (ch^48), ch=getchar();\n    f ? x=-x : 0;\n}\n</code></pre> <pre><code>template&lt;typename type, typename ..._type&gt;\nvoid read(type &amp;x, _type&amp;...y) {read(x), read(y);}\n</code></pre> <p>Add code above under the \"All Types\" code.</p>"},{"location":"oi-tips/-qread-and-qwrite/#quick-write-putchar","title":"Quick Write <code>putchar()</code>","text":"intAll Types <pre><code>void write(int x) {\n    x&lt;0 ? x=-x, putchar('-') : 0;\n    x&gt;9 ? putchar(x/10) : 0;\n    putchar(x%10|48);\n}\n</code></pre> <p>head file: <code>&lt;cctype&gt;</code></p> <pre><code>template&lt;typename type&gt;\nvoid write(type x, bool md=1) {\n    x&lt;0 ? x=-x, putchar('-') : 0; static short bf[50], tp(0);\n    do bf[++tp] = x%10, x /= 10; while(x);\n    while(tp) putchar(bf[tp--]|48);\n    md ? putchar('\\n') : putchar(' ');\n}\n</code></pre> <p>md=0 for space</p> <p>md=1 for new line</p>"},{"location":"oi-tips/-qread-and-qwrite/#even-faster-fread","title":"Even Faster: <code>fread()</code>","text":"<p>We will just need to modify the <code>getchar()</code> function in the qread template above.</p>"},{"location":"oi-tips/-qread-and-qwrite/#fread","title":"<code>fread()</code>","text":"Since C99<pre><code>size_t fread( void *restrict buffer, size_t size, size_t count,\n    FILE *restrict stream );\n</code></pre>"},{"location":"oi-tips/-qread-and-qwrite/#parameters","title":"Parameters","text":"<p>buffer  -   pointer to the array where the read objects are stored</p> <p>size    -   size of each object in bytes</p> <p>count   -   the number of the objects to be read</p> <p>stream  -   the stream to read</p>"},{"location":"oi-tips/-qread-and-qwrite/#return-value","title":"Return Value","text":"<p>Number of objects read successfully, which may be less than <code>count</code> if an error or end-of-file condition occurs.</p> <p>If <code>size</code> or <code>count</code> is zero, fread returns zero and performs no other action.</p> <p>fread does not distinguish between end-of-file and error, and callers must use feof and ferror to determine which occurred.</p>"},{"location":"oi-tips/-qread-and-qwrite/#new-variables-needed","title":"New Variables Needed","text":"<pre><code>char buf[1&lt;&lt;20], *p1=buf, *p2=buf;\n</code></pre> <ul> <li><code>buf</code>: buffer.</li> <li><code>p1</code>: points to the currently read element.</li> <li><code>p2</code>: points to the end of the buffer.</li> </ul>"},{"location":"oi-tips/-qread-and-qwrite/#redefined-getchars-function","title":"Redefined <code>getchars()</code> Function","text":"<pre><code>#define getchar() (p1==p2&amp;&amp;(p2=(p1=buf)+fread(buf,1,1&lt;&lt;20,stdin),p1==p2)?EOF:*p1++)\n</code></pre>"},{"location":"oi-tips/-qread-and-qwrite/#even-faster-fwrite","title":"Even Faster: <code>fwrite</code>","text":"<p>We will just need to modify the <code>putchar()</code> function in the qread template above.</p>"},{"location":"oi-tips/-qread-and-qwrite/#fwrite","title":"<code>fwrite()</code>","text":"<pre><code>size_t fwrite( const void* restrict buffer, size_t size, size_t count,\n    FILE* restrict stream );\n</code></pre>"},{"location":"oi-tips/-qread-and-qwrite/#parameters_1","title":"Parameters","text":"<p>buffer  -   pointer to the first object in the array to be written</p> <p>size    -   size of each object</p> <p>count   -   the number of the objects to be written</p> <p>stream  -   pointer to the output stream</p>"},{"location":"oi-tips/-qread-and-qwrite/#return-value_1","title":"Return Value","text":"<p>Number of objects read successfully, which may be less than <code>count</code> if an error or end-of-file condition occurs.</p> <p>If <code>size</code> or <code>count</code> is zero, fread returns zero and performs no other action.</p> <p>fread does not distinguish between end-of-file and error, and callers must use feof and ferror to determine which occurred.</p>"},{"location":"oi-tips/-qread-and-qwrite/#new-variables-needed_1","title":"New Variables Needed","text":"<pre><code>char buf[1&lt;&lt;20], *p3=buf;\n</code></pre>"},{"location":"oi-tips/-qread-and-qwrite/#redefined-putchar-function","title":"Redefined <code>putchar()</code> Function","text":"<p>We implement a <code>flush()</code> function to output the buffer, so that it can be called many times in the future.</p> flush() Function<pre><code>#define flush() (fwrite(p3=out,1,1&lt;&lt;20,stdout))\n</code></pre> <p>You can actually take advantage of the principle of calling destructors at the end of a program by defining a class that calls flush() at the end of the program.</p> Destructor flush()<pre><code>class Flush{public:~Flush(){flush();}}_;\n</code></pre> <p>Now here's the redefined <code>putchar()</code> function </p>The Redefined putchar() Function<pre><code>#define putchar(ch) (p3==out+SIZE&amp;&amp;flush(),*p3++=(ch))\n</code></pre>"},{"location":"oi-tips/accelerate-ios/","title":"iostream Acceleration","text":""},{"location":"oi-tips/accelerate-ios/#untie","title":"Untie","text":"<pre><code>ios::sync_with_stdio(0);\ncin.tie(0);\ncout.tie(0);\n</code></pre> <p>Tip</p> <p>The untied cin &amp; cout are actually faster than scanf &amp; printf.</p> <p>Warning</p> <p>You CANNOT use functions in <code>&lt;stdio&gt;</code> after untie, including <code>printf()</code>, <code>scanf()</code>, <code>puts()</code>, <code>freopen()</code>.</p>"},{"location":"oi-tips/accelerate-ios/#endl","title":"endl","text":"<p>endl can be about 10 times slower than the \"\\n\".</p> <pre><code>#define endl '\\n'\n</code></pre>"},{"location":"oi-tips/elapsed-time/","title":"Elapsed Time Measurement","text":"Head File<pre><code>#include &lt;chrono&gt;\n</code></pre> global variables<pre><code>auto chrono_start_time = chrono::high_resolution_clock::now();\n</code></pre> Start Measuring<pre><code>void start_measuring() {\n    chrono_start_time = chrono::high_resolution_clock::now();\n}\n</code></pre> MillisecondsSecondsMinutes End Measuring<pre><code>void end_measuring() {\n    auto chrono_end_time = chrono::high_resolution_clock::now();\n    chrono::duration&lt;double, milli&gt; chrono_elapsed_seconds = chrono_end_time - chrono_start_time;\n    double chrono_elapsed_ms = chrono_elapsed_seconds.count();\n    cout &lt;&lt; \"Elapsed time: \" &lt;&lt; chrono_elapsed_ms &lt;&lt; \"ms\" &lt;&lt; endl;\n}\n</code></pre> End Measuring<pre><code>void end_measuring() {\n    auto chrono_end_time = chrono::high_resolution_clock::now();\n    chrono::duration&lt;double&gt; chrono_elapsed_seconds = chrono_end_time - chrono_start_time;\n    double chrono_elapsed_ms = chrono_elapsed_seconds.count();\n    cout &lt;&lt; \"Elapsed time: \" &lt;&lt; chrono_elapsed_ms &lt;&lt; \"s\" &lt;&lt; endl;\n}\n</code></pre> End Measuring<pre><code>void end_measuring() {\n    auto chrono_end_time = chrono::high_resolution_clock::now();\n    chrono::duration&lt; double, ratio&lt;60&gt; &gt; chrono_elapsed_seconds = chrono_end_time - chrono_start_time;\n    double chrono_elapsed_ms = chrono_elapsed_seconds.count();\n    cout &lt;&lt; \"Elapsed time: \" &lt;&lt; chrono_elapsed_ms &lt;&lt; \"min\" &lt;&lt; endl;\n}\n</code></pre>"},{"location":"oi-tips/file-read-and-write/","title":"File Read & Write","text":""},{"location":"oi-tips/file-read-and-write/#c-file-freopen","title":"C: FILE <code>freopen()</code>","text":"<p>head file: `</p> Definition<pre><code>std::FILE* freopen( const char* filename, const char* mode, std::FILE* stream );\n</code></pre>"},{"location":"oi-tips/file-read-and-write/#file-access-mode-string","title":"File Access Mode String","text":"<p>From CPPReference</p> File access mode string Meaning Explanation Action if file  already exists Action if file  does not exist \"r\" read Open a file for reading read from start return NULL and set error \"w\" write Create a file for writing destroy contents create new \"a\" append Append to a file write to end create new \"r+\" read extended Open a file for read/write read from start return NULL and set error \"w+\" write extended Create a file for read/write destroy contents create new \"a+\" append extended Open a file for read/write write to end create new <p>Note</p> <p>std::freopen is the only way to change the narrow/wide orientation of a stream once it has been established by an I/O operation or by std::fwide.</p> Example <pre><code>#include &lt;cstdio&gt;\n#include &lt;iostream&gt;\nusing namespace std;\n\nint main() {\n    freopen(\"in.in\", \"r\", stdin);\n    freopen(\"out.out\", \"w\", stdout);\n\n    int a, b;\n    cin &gt;&gt; a &gt;&gt; b;\n    cout &lt;&lt; a * a + b * b &lt;&lt; endl;\n    return 0;\n}\n</code></pre> <p>Warning</p> <p>If you have used the untie operation, you shouldn't use freopen because you would have already disabled <code>&lt;cstdio&gt;</code>.</p> Tip <p>If you are using an Online Judge, you can add this macro definition to your code:</p> ONLINE_JUDGE<pre><code>#ifdef ONLINE_JUDGE\n#else\n#endif\n</code></pre> <p>Most OJ will use this macro definition, while your computer won't have one, thus you can add freopen in ONLINE_JUDGE so that you can use freopen as it'll be disabled on your OJ.</p> Example <pre><code>int main() {\n    #ifdef ONLINE_JUDGE\n    #else   // when run on local\n        freopen(\"in.in\", \"r\", \"stdin\");\n        freopen(\"out.out\", \"w\", \"stdout\");\n    #endif\n}\n</code></pre>"},{"location":"oi-tips/file-read-and-write/#c-fstream-fiostream","title":"C++: fstream <code>fiostream</code>","text":""},{"location":"oi-tips/file-read-and-write/#headfile-fiostream","title":"headfile: <code>&lt;fiostream&gt;</code>","text":"ReadWriteRead &amp; Write <pre><code>ifstream in(\"FILENAME\", ios::in);\n</code></pre> <pre><code>ofstream out(\"FILENAME\", ios::out);\n</code></pre> <pre><code>fstream foi(\"FILENAME\", ios::in|ios::out);\n</code></pre>"},{"location":"oi-tips/file-read-and-write/#function-open","title":"function <code>open()</code>","text":"General UsingProtocol <pre><code>void open(const char * filename,\n    ios_base::openmode mode = ios_base::in | ios::base::out);\n</code></pre> <pre><code>void open(const wchar_t *_Filename,\n    ios_base::in | ios_base::out, int prot = ios_base::_Openprot);\n</code></pre> Parameters modeprot <pre><code>    ios::app: //open the file as append  \n\n    ios::ate: //Locate the file to the end of the file when it is opened. ios:app includes this property.  \n\n    ios::binary: //Open the file as binary, the default is text. See the previous section for the difference between the two methods  \n\n    ios::in: // the file is opened as input (file data is entered into memory)  \n\n    ios::out: //files are opened as output (memory data is output to the file)  \n\n    ios::nocreate: //doesn't create the file, so it fails to open if the file doesn't exist  \n\n    ios::noreplace: // doesn't overwrite the file, so opening the file fails if the file exists  \n\n    ios::trunc: // set file length to 0 if file exists\n</code></pre> <pre><code>    0: Ordinary file, open access  \n    1: Read-only file  \n    2: Implicit files  \n    4: system file \n</code></pre> <p>Tip</p> <p>You can use \"or\" or \"+\" to connect the above attributes, such as 3 or 1 | 2 is to open the file with read-only and implicit attributes.</p>"},{"location":"oi-tips/file-read-and-write/#status-indicator","title":"Status Indicator","text":"<pre><code>is_open() //whether the file is normally open\nbad() // read and write process whether the error\nfail() // read and write process whether the error \neof() // read the file to reach the end of the file, return true\ngood () // any of the above return true, this will return false\n</code></pre>"},{"location":"oi-tips/gcc-optimize/","title":"GCC Optimize","text":""},{"location":"oi-tips/gcc-optimize/#difference-between-gcc-optimizations","title":"Difference Between GCC Optimizations","text":"<ul> <li>O1: Enable basic optimizations, including function inlining and loop unrolling.</li> <li>O2: Enable more optimizations, including function inlining, loop unrolling, constant propagation, dead code removal, etc.</li> <li>O3: Enable more advanced optimizations, including function inlining, loop unrolling, constant propagation, dead code removal, vectorization, etc.</li> <li>Os: Enable optimizations with code size reduction as the main goal.</li> </ul> <p>Warning</p> <p>Enabling higher-level optimization options may result in increased compilation times and, in some cases, may result in less efficient code execution.</p> <p>Warning</p> <p>Enabling GCC optimization manually is not allowed in Luogu OJ</p>"},{"location":"oi-tips/gcc-optimize/#usage","title":"Usage","text":"<p>Tip</p> <p>If you have STL in your code, you'd better turn on O2 or even O3 optimization.</p> O2 Optimization<pre><code>#pragma GCC optimize(2)\n</code></pre>"},{"location":"oi-tips/qread-and-qwrite/","title":"Quick Read & Write","text":"Quick Read<pre><code>char *P1,*P2,BUF[100000];\n#define NC() (P1==P2 &amp;&amp; (P2 = (P1=BUF) + fread(BUF, 1, 100000, stdin), P1==P2) ? EOF : *P1++)\nint read() {\n    int x = 0, f = 1;\n    char CH = NC();\n    while(CH &lt; 48 || CH &gt; 57)\n    {\n        if(CH == '-')\n            f = -1;\n        CH = NC();\n    }\n    while(CH&gt;=48&amp;&amp;CH&lt;=57)\n        x = x*10 + CH-48, CH=NC();\n    return x*f;\n}\n</code></pre> Quick Write<pre><code>void write(int x) {\n    if(x &lt; 0)\n        putchar('-'), x = -x;\n    if(x &gt; 9)\n        qwrite(x/10);\n    putchar(x%10 + '0');\n    return;\n}\n</code></pre>"},{"location":"school-notes/junior-notes/geography/","title":"\u521d\u4e2d\u5730\u7406\u603b\u7ed3","text":""},{"location":"school-notes/junior-notes/geography/#i","title":"\u2160. \u5730\u56fe\u4fe1\u606f\u7684\u83b7\u53d6\u3001\u8868\u8fbe\u548c\u7406\u89e3","text":""},{"location":"school-notes/junior-notes/geography/#_1","title":"\u4ece\u5730\u56fe\u548c\u56fe\u8868\u4e2d\u83b7\u53d6\u4fe1\u606f","text":"<pre><code>\u300a\u4eba\u6587\u5730\u7406 \u4e0a\u518c\u300b P2~4, P14~20, P45~52\n</code></pre>"},{"location":"school-notes/junior-notes/geography/#_2","title":"\u5730\u56fe\u4e09\u8981\u7d20","text":"<p>\u65b9\u5411\uff0c\u6bd4\u4f8b\u5c3a\uff0c\u56fe\u4f8b &amp; \u6ce8\u8bb0 <sup>1</sup></p>"},{"location":"school-notes/junior-notes/geography/#_3","title":"\u65b9\u5411\ud83e\udded","text":"\u6b63\u786e\u8868\u8fbe\u67d0\u5730\u7684\u7ecf\u7eac\u5ea6 <p>Point A (n\u00b0 N/S, n\u00b0 W/E)</p> <p>\u5982\uff1aA\u70b9\uff0830\u00b0 N, 45\u00b0 W\uff09</p> <p>Warning</p> <p>\u7ad9\u5728\u5730\u7403\u7684\u5357(\u5317)\u6781\u70b9\uff0c\u524d\u540e\u5de6\u53f3\u90fd\u662f\u5317(\u5357)\u65b9</p>"},{"location":"school-notes/junior-notes/geography/#_4","title":"\u56fe\u4f8b &amp; \u6ce8\u8bb0\ud83c\udff7\ufe0f","text":"<p>\u56fe\u4f8b\u548c\u6ce8\u8bb0\u7684\u533a\u522b</p> <p>\u56fe\u4f8b\uff1a\u5bf9\u5730\u56fe\u4e0a\u5404\u79cd\u7b26\u53f7\u548c\u6ce8\u8bb0\u7684\u8bf4\u660e\uff0c\u4e00\u822c\u5728\u5730\u56fe\u5916\u9762</p> <p>\u6ce8\u8bb0\uff1a\u6307\u5730\u56fe\u4e0a\u7684\u6587\u5b57\u8bf4\u660e\uff0c\u4e00\u822c\u5730\u56fe\u91cc\u9762</p>"},{"location":"school-notes/junior-notes/geography/#_5","title":"\u4ece\u5730\u56fe\ud83d\uddfa\ufe0f\u4e0a\u83b7\u53d6\u4fe1\u606f","text":""},{"location":"school-notes/junior-notes/geography/#_6","title":"\u7b49\u9ad8\u7ebf\u5730\u5f62\u56fe","text":"Tip <p>\u5982\u4e0a\u56fe\uff0c\u56fe\u5e45\u548c\u56fe\u4e2d\u7b49\u9ad8\u8ddd\u76f8\u540c\u65f6\uff0c\u6bd4\u4f8b\u5c3a\u8d8a\u5c0f\uff0c\u5761\u5ea6\u8d8a\u7f13\uff0c\u53cd\u4e4b\u540c\u7406</p> <p>$\\therefore$ \u5761\u5ea6\u6700\u7f13\u7684\u662fC</p> \u5c71\u810a\u548c\u5c71\u8c37\u7684\u5224\u5b9a <p>\u5c71\u810a\uff1a\u7b49\u9ad8\u7ebf\u5411\u4f4e\u5904\u51f8\u51fa \u5c71\u8c37\uff1a\u7b49\u9ad8\u7ebf\u5411\u9ad8\u51fa\u51f8\u51fa (\u53ef\u4ee5\u53d1\u80b2\u6cb3\u6d41)</p>"},{"location":"school-notes/junior-notes/geography/#_7","title":"\u4ece\u5730\u7403\u4eea\ud83c\udf0e\u4e0a\u770b\u4e16\u754c","text":""},{"location":"school-notes/junior-notes/geography/#_8","title":"\u7ecf\u7ebf\u548c\u7eac\u7ebf","text":"<p>\u7ecf\u7ebf\u53c8\u540d\u5b50\u5348\u7ebf\ud83c\udf10</p> \u7ecf\u7ebf\u7eac\u7ebf\u7684\u957f\u5ea6\u548c\u5f62\u72b6 <p>\u6bcf\u6761\u7ecf\u7ebf\u5448\u534a\u5706\u72b6\uff0c\u6bcf\u6761\u957f\u5ea6\u76f8\u7b49</p> <p>\u6bcf\u6761\u7eac\u7ebf\u5448\u6574\u5706\u72b6<sup>2</sup>\uff0c\u8d64\u9053\u6700\u957f</p>"},{"location":"school-notes/junior-notes/geography/#_9","title":"\u7ecf\u5ea6\u548c\u7eac\u5ea6","text":"\u9879\u76ee \u7ecf\u5ea6 \u7eac\u5ea6 \u5ea6\u6570\u8d77\u59cb\u7ebf \u672c\u521d\u5b50\u5348\u7ebf \u8d64\u9053 \u5212\u5206\u65b9\u6cd5 \u4ee5\u672c\u521d\u5b50\u5348\u7ebf\u4e3a\u754c\uff0c\u5411\u4e1c\u4e1c\u7ecf\uff0c\u5411\u897f\u897f\u7ecf\uff0c\u5404180\u00b0 \u4ee5\u8d64\u9053\u4e3a\u754c\uff0c\u5411\u5317\u5317\u7eac\uff0c\u5411\u5357\u5357\u7eac\uff0c\u540490\u00b0 \u534a\u7403\u5212\u5206 \u4e1c\u534a\u7403\ud83c\udf0f\uff1a20\u00b0 W - 160\u00b0 E \u8d64\u9053\u4e3a\u754c <p>Danger</p> <p>\u4e1c\u897f\u534a\u7403\u5206\u754c\u7ebf\u4e3a20\u00b0 W, 160\u00b0 E</p> \u4e94\u6e29\u5e26 &amp; \u4f4e\u4e2d\u9ad8\u7eac\u5ea6 <p></p> <p>\u636e\u56fe\u53ef\u77e5\uff0c\u6211\u56fd\u5927\u90e8\u5206\u5730\u533a\u4f4d\u4e8e\u5317\u6e29\u5e26\uff0c\u4e2d\u7eac\u5ea6\uff1b\u5c0f\u90e8\u5206\u5730\u533a\u4f4d\u4e8e\u70ed\u5e26\uff0c\u4f4e\u7eac\u5ea6</p> <p>\u7ecf\u5ea6\u76f8\u5dee15\u00b0\uff0c\u65f6\u95f4\u76f8\u5dee1h</p>"},{"location":"school-notes/junior-notes/geography/#_10","title":"\u63cf\u8ff0\u533a\u57df\u7684\u4f4d\u7f6e\u3001\u81ea\u7136\u73af\u5883\u548c\u4eba\u6587\u73af\u5883\u7684\u7279\u70b9","text":"<pre><code>\u300a\u4eba\u6587\u5730\u7406 \u4e0a\u518c\u300bP100~104\n</code></pre>"},{"location":"school-notes/junior-notes/geography/#_11","title":"\u63cf\u8ff0\u5730\u533a\u7684\u81ea\u7136\u73af\u5883\ud83c\udf32\u7279\u70b9","text":"<p>\u5730\u5f62\u3001\u6c14\u5019\u3001\u6cb3\u6d41\u3001\u81ea\u7136\u8d44\u6e90\u7b49</p>"},{"location":"school-notes/junior-notes/geography/#_12","title":"\u63cf\u8ff0\u65b9\u6cd5","text":"\u5730\u5f62 <ul> <li>\u6d77\u62d4</li> <li>\u5730\u9762\u8d77\u4f0f</li> <li>\u5730\u5f62\u79cd\u7c7b\u548c\u5206\u5e03</li> </ul> \u6c14\u5019 <ul> <li>\u6c14\u6e29</li> <li>\u964d\u6c34</li> </ul> \u5f52\u7eb3\u6c14\u5019\u7279\u5f81 <p>\u5b63\u8282+\u6c14\u6e29\u7279\u70b9+\u964d\u6c34\u7279\u70b9</p> \u6cb3\u6d41 <ul> <li>\u5e74\u5f84\u6d41\u91cf <p>\u5e74\u964d\u6c34\u91cf</p> </li> <li>\u5f84\u6d41\u91cf\u5b63\u8282\u53d8\u5316 <p>\u964d\u6c34\u91cf\u5b63\u8282\u53d8\u5316</p> </li> </ul> \u4e9a\u70ed\u5e26\u5b63\u98ce\u6c14\u5019 <p>$\\because$ \u590f\u5b63\u591a\u96e8\uff0c\u51ac\u5b63\u5c11\u96e8</p> <p>$\\therefore$ \u6cb3\u6d41\u5f84\u6d41\u91cf\u590f\u5b63\u5927\uff0c\u51ac\u5b63\u5c11</p> <ul> <li>\u6d41\u901f <p>\u7b49\u9ad8\u7ebf</p> </li> <li>\u6d41\u5411 <p>\u5730\u52bf</p> </li> <li>\u6c34\u80fd <p>\u5730\u52bf\u843d\u5dee\uff0c\u5f84\u6d41\u91cf</p> </li> <li>\u6c34\u4f4d <p>\u6cb3\u6d41\u5bbd\u5ea6\uff0c\u5f84\u6d41\u91cf</p> </li> <li>\u542b\u6c99\u91cf <p>\u571f\u8d28\uff0c\u4eba\u4e3a\u56e0\u7d20</p> </li> <li>\u7ed3\u51b0\u671f <p>\u6c14\u6e29</p> </li> <li>\u6c34\u7cfb\u7279\u5f81\u63cf\u8ff0 <p>\u6cb3\u7f51(\u5bc6\u96c6/\u7a00\u758f)\u3001\u6cb3\u6e56\u6570\u91cf\u3001\u957f\u5ea6\u3001\u5165\u6d77\u53e3\u3001\u6d41\u57df\u9762\u79ef\u7b49</p> </li> </ul> \u8d44\u6e90 <p>\u5e38\u89c1\u8d44\u6e90\uff1a\u571f\u5730\u8d44\u6e90\u3001\u77ff\u4ea7\u8d44\u6e90\u3001\u751f\u7269\u8d44\u6e90\u3001\u6c34\u8d44\u6e90\u3001\u592a\u9633\u80fd\u3001\u98ce\u80fd\u7b49</p> <p>\u5e38\u89c1\u8868\u8ff0\uff1a\"\u571f\u5730\u8d44\u6e90\u5e7f\u9614\uff0c\u571f\u5730\u80a5\u6c83\" \"\u7164\u70ad\u8d44\u6e90\u4e30\u5bcc\" \"\u6c34\u8d44\u6e90\u77ed\u7f3a\"\u7b49</p> \u8015\u5730 <p>\u5206\u4e3a\u4ee5\u4e0b2\u7c7b\uff1a</p> <ol> <li> <p>\u6c34\u7530\uff1a\u6c34\u8d44\u6e90\u4e30\u5bcc\u3001\u5730\u52bf\u4f4e\u5e73\u7684\u5730\u65b9\uff0c\u4e00\u822c\u4ee5\u6c34\u7530\u4e3a\u4e3b</p> </li> <li> <p>\u65f1\u5730\uff1a\u6c34\u8d44\u6e90\u4e0d\u8db3\uff0c\u5730\u52bf\u53c8\u76f8\u5bf9\u8f83\u9ad8\u7684\u5730\u65b9\uff0c\u4e00\u822c\u4ee5\u65f1\u5730\u4e3a\u4e3b</p> </li> </ol>"},{"location":"school-notes/junior-notes/geography/#_13","title":"\u63cf\u8ff0\u5730\u533a\u7684\u4eba\u6587\u73af\u5883\ud83d\uddff\u7279\u70b9","text":"<p>\u4eba\u53e3\u3001\u6c11\u65cf\u3001\u57ce\u5e02\u548c\u4e61\u6751\u3001\u98ce\u4fd7\u4e60\u60ef\u7b49</p>"},{"location":"school-notes/junior-notes/geography/#_14","title":"\u63cf\u8ff0\u65b9\u6cd5","text":"<ul> <li>\u751f\u6d3b\u7279\u8272 <p>\u8863\u98df\u4f4f\u884c\uff0c\u4f20\u7edf\u6587\u5316\uff0c\u98ce\u4fd7\u4e60\u60ef</p> </li> <li>\u519c\u4e1a\u751f\u4ea7 <p>\u519c\u4e1a\u751f\u4ea7\u7c7b\u578b\uff1a\u79cd\u690d\u4e1a\u3001\u6797\u4e1a\u3001\u755c\u7267\u4e1a\u3001\u6e14\u4e1a\u7b49 \u79cd\u690d\u4e1a\uff1a\u4e3b\u8981\u79cd\u690d\u7684\u519c\u4f5c\u7269\u79cd\u7c7b\u3001\u8015\u4f5c\u5236\u5ea6\u3001\u751f\u4ea7\u6761\u4ef6</p> </li> <li>\u5176\u4ed6\u751f\u4ea7\u7279\u8272 <p>\u5236\u9020\u4e1a\u3001\u9ad8\u79d1\u6280\u4ea7\u4e1a\u3001\u91d1\u878d\u4e1a\u3001\u670d\u52a1\u4e1a\u7b49</p> </li> </ul>"},{"location":"school-notes/junior-notes/geography/#_15","title":"\u611f\u53d7\u533a\u57df\u7684\u53d1\u5c55","text":"<p>\u4e00\u4e2a\u533a\u57df\u7684\u53d1\u5c55\u4e00\u822c\u53ef\u4ee5\u901a\u8fc7\u8be5\u5730\u7684\u653f\u6cbb\u3001\u7ecf\u6d4e\u548c\u6587\u5316\u53d1\u5c55\u6c34\u5e73\u7b49\u51e0\u4e2a\u65b9\u9762\u6765\u8ba4\u8bc6</p> \u5357\u975e\ud83c\uddff\ud83c\udde6 <p>\u5357\u975e\u4f9d\u9760\u81ea\u7136\u6761\u4ef6\u7684\u4f18\u52bf\uff0c\u56e0\u5730\u5236\u5b9c\u5730\u53d1\u5c55\u7ecf\u6d4e\u3002</p> <p>\u4f8b\u5982\uff0c\u5145\u5206\u5229\u7528\u5176\u77ff\u4ea7\u8d44\u6e90\u4f18\u52bf\uff0c\u5927\u529b\u53d1\u5c55\u91c7\u77ff\u4e1a\u548c\u5236\u9020\u4e1a\uff1b</p> <p>\u5229\u7528\u4e30\u5bcc\u7684\u91ce\u751f\u52a8\u7269\u8d44\u6e90\uff0c\u53d1\u5c55\u65c5\u6e38\u4e1a\uff0c\u7b49\u7b49\u3002</p>"},{"location":"school-notes/junior-notes/geography/#_16","title":"\u3010\u91cd\u70b9\u63d0\u9192\u3011","text":"\u5f71\u54cd\u519c\u4e1a\u751f\u4ea7\u7684\u56e0\u7d20 \u5149 <ul> <li>\u7eac\u5ea6\uff1a\u7eac\u5ea6\u4f4e\uff0c\u592a\u9633\u8f90\u5c04\u91cf\u4e30\u5bcc</li> <li>\u5929\u6c14\uff1a\u6674\u5929\u591a\uff0c\u5149\u7167\u65f6\u95f4\u957f</li> <li>\u6d77\u62d4\uff1a\u6d77\u62d4\u9ad8\uff0c\u7a7a\u6c14\u7a00\u8584\uff0c\u900f\u660e\u5ea6\u597d\uff0c\u592a\u9633\u8f90\u5c04\u91cf\u4e30\u5bcc</li> </ul> \u70ed <ul> <li>\u5b63\u8282\uff1a\u590f\u5b63\u6e29\u5ea6\u9ad8\uff0c\u70ed\u91cf\u6761\u4ef6\u597d</li> <li>\u7eac\u5ea6\uff1a\u4f4e\u7eac\u5ea6\u5730\u533a\u70ed\u91cf\u6761\u4ef6\u597d</li> <li>\u5730\u5f62\uff1a\u6d77\u62d4\u9ad8\uff0c\u6c14\u6e29\u4f4e\uff0c\u70ed\u91cf\u4e0d\u8db3\uff1b\u76c6\u5730\u5730\u5f62\u4e0d\u6613\u6563\u70ed\uff0c\u70ed\u91cf\u6761\u4ef6\u8f83\u597d\uff0c\u4e1c\u897f\u8d70\u5411\u548c\u4e1c\u5317\u4e00\u897f\u5357\u8d70\u5411\u7684\u5c71\u8109\uff0c\u5927\u5927\u51cf\u5f31\u4e86\u51ac\u5b63\u5bd2\u6f6e\u548c\u51b7\u7a7a\u6c14\u7684\u5165\u4fb5\u7a0b\u5ea6\u7b49</li> </ul> \u6c34 <ul> <li>\u65f6\u95f4\uff1a\u5b63\u8282\u53d8\u5316\u89d2\u5ea6</li> <li>\u7a7a\u95f4\uff1a\u5982\u201c\u4e1c\u591a\u897f\u5c11\u201d\u201c\u5357\u591a\u5317\u5c11\u201d\u201c\u7531\u4e1c\u5357\u6cbf\u6d77\u5411\u897f\u5317\u5185\u9646\u9010\u6e10\u51cf\u5c11\u201d\u7b49</li> <li>\u5f71\u54cd\u964d\u96e8\u56e0\u7d20\uff1a<ol> <li>\u590f\u5b63\u98ce\uff08\u4e00\u822c\u60c5\u51b5\u4e0b\uff0c\u53d7\u590f\u5b63\u98ce\u5f71\u54cd\u660e\u663e\u7684\u5730\u65b9\uff0c\u964d\u6c34\u8f83\u591a\uff09</li> <li>\u6d77\u9646\u4f4d\u7f6e\uff08\u4e00\u822c\u60c5\u51b5\u4e0b\uff0c\u6cbf\u6d77\u5730\u533a\u964d\u6c34\u591a\uff0c\u5185\u9646\u5730\u533a\u964d\u6c34\u5c11\uff09\u3002</li> <li>\u5730\u5f62\uff08\u4e00\u822c\u60c5\u51b5\u4e0b\uff0c\u6e7f\u6da6\u6c14\u6d41\u9047\u5230\u9ad8\u5c71\u963b\u6321\u65f6\uff0c\u8fce\u98ce\u5761\u964d\u6c34\u591a\uff0c\u80cc\u98ce\u5761\u964d\u6c34\u5c11\uff09</li> </ol> </li> </ul> \u571f <ol> <li>\u5730\u5f62\uff1a\u5982\u5e73\u539f\uff0c\u5e73\u5766\u8fbd\u9614\uff0c\u9002\u5b9c\u79cd\u690d\u519c\u4f5c\u7269\uff1b\u4e18\u9675\u9002\u5408\u79cd\u690d\u8336\u53f6\u3001\u67d1\u6a58\u7b49\u7ecf\u6d4e\u4f5c\u7269</li> <li>\u571f\u58e4\uff1a\u5982\u6cb3\u6d41\u51b2\u79ef\u5e73\u539f\uff0c\u571f\u58e4\u80a5\u6c83\uff0c\u9002\u5408\u519c\u4f5c\u7269\u7684\u751f\u957f\uff1b\u4e1c\u5357\u4e18\u9675\u5730\u533a\u7684\u9178\u6027\u571f\u58e4\u9002\u5b9c\u79cd\u690d\u8336\u6811\u7b49\u4f5c\u7269\uff1b\u78b1\u6027\u571f\u58e4\u5219\u9002\u5408\u79cd\u690d\u559c\u78b1\u6027\u7684\u4f5c\u7269</li> </ol> \u5f71\u54cd\u5de5\u4e1a\u751f\u4ea7\u7684\u56e0\u7d20 <p>\u5730\u7406\u4f4d\u7f6e\u3001\u8d44\u6e90\u3001\u4ea4\u901a\u3001\u5e02\u573a\u3001\u79d1\u6280\u3001\u4eba\u624d\u3001\u52b3\u52a8\u529b\u3001\u8d44\u91d1\u3001\u9996\u7840\u8bbe\u65bd\u3001\u5386\u53f2\u6761\u4ef6\u3001\u653f\u7b56\u652f\u6301\u7b49</p> \u73e0\u4e09\u89d2\u5730\u533a <p>\u8fd8\u8981\u8003\u8651\u662f\u5426\u9760\u8fd1\u6e2f\u6fb3\uff0c\u662f\u5426\u5730\u5904\u4fa8\u4e61\u3002\u8fd9\u91cc\u6709\u4ee5\u5f15\u5916\u6765\u8d44\u91d1\u5c24\u5176\u662f\u6ee4\u6fb3\u8d44\u5168\u7684\u6709\u5229\u6761\u4ef6\uff0c\u8fd8\u6709\u53d1\u8fbe\u7684\u5e02\u573a\u7ecf\u6599\u5e7f\u9614\u7684\u5e02\u573a\u7b49\u3002</p>"},{"location":"school-notes/junior-notes/geography/#ii","title":"\u2161. \u5171\u540c\u7684\u5bb6\u56ed \u5171\u540c\u7684\u5c81\u6708","text":""},{"location":"school-notes/junior-notes/geography/#_17","title":"\u5927\u6d32\u26f0\ufe0f\u548c\u5927\u6d0b\ud83c\udf0a","text":"<pre><code>\u300a\u4eba\u6587\u5730\u7406 \u4e0a\u518c\u300b P22~27\n</code></pre>"},{"location":"school-notes/junior-notes/geography/#_18","title":"\u6d77\u9646\u5206\u5e03","text":"<p>\u6d77\u6d0b\u536071%\uff0c\u9646\u5730\u536029%</p>"},{"location":"school-notes/junior-notes/geography/#_19","title":"\u5927\u6d32\u26f0\ufe0f","text":""},{"location":"school-notes/junior-notes/geography/#_20","title":"\u542b\u4e49","text":"<p>\u5927\u9646\u4e0e\u5176\u9644\u8fd1\u7684\u5c9b\u5c7f\u5408\u8d77\u6765\u79f0\u4f5c\u5927\u6d32</p>"},{"location":"school-notes/junior-notes/geography/#_21","title":"\u6392\u5217\ud83d\udce2","text":"<p>\u4e9a\u6d32\ud83c\udf0f &gt; \u975e\u6d32\ud83c\udf0d &gt; \u5317\u7f8e\u6d32\ud83c\udf0e &gt; \u5357\u7f8e\u6d32\ud83c\udf0e &gt; \u5357\u6781\u6d32\ud83c\udde6\ud83c\uddf6 &gt; \u6b27\u6d32\ud83c\udf0d &gt; \u5927\u6d0b\u6d32\ud83c\udf0f</p>"},{"location":"school-notes/junior-notes/geography/#_22","title":"\u5206\u754c\u7ebf\u3030\ufe0f","text":"\u5206\u754c\u7ebf \u4e9a\u6d32\u4e0e\u5317\u7f8e\u6d32\u767d\u4ee4\u6d77\u5ce1\u6240\u5728\u5927\u6d32\u6b27\u6d32\u4e0e\u975e\u6d32\u5730\u4e2d\u6d77\u2014\u76f4\u5e03\u7f57\u9640\u6d77\u5ce1\u4e9a\u6b27\u4ea4\u754c\u975e\u6d32\u4e0e\u4e9a\u6d32\u82cf\u4f0a\u58eb\u8fd0\u6cb3\u5317\u7f8e\u6d32\u4e9a\u6d32\u4e0e\u6b27\u6d32\u4e4c\u62c9\u5c14\u5c71\u8109\u4e00\u4e4c\u62c9\u5c14\u6cb3\u4e00\u91cc\u6d77\u4e00\u5927\u9ad8\u52a0\u7d22\u5c71\u8109\u4e00\u9ed1\u6d77\u2014\u571f\u8033\u5176\u6d77\u5ce1\u4e9a\u6d32\u5317\u7f8e\u6d32\u4e0e\u5357\u7f8e\u6d32\u5df4\u62ff\u9a6c\u8fd0\u6cb3\u4e9a\u6d32\u5357\u7f8e\u6d32\u4e0e\u5357\u6781\u6d32\u5fb7\u96f7\u514b\u6d77\u5ce1\u4e9a\u6d32\u9131\u9633\u6e56\u4e2d\u56fd\u9762\u79ef\u6700\u5927\u7684\u6de1\u6c34\u6e56\u4e9a\u6d32"},{"location":"school-notes/junior-notes/geography/#_23","title":"\u5206\u533a\u00a7","text":"<p>\u4e9a\u6d32\ud83c\udf0f\uff1a\u4e1c\u4e9a\u3001\u4e1c\u5357\u4e9a\u3001\u5357\u4e9a\u3001\u4e2d\u4e9a\u3001\u897f\u4e9a\u3001\u5317\u4e9a</p> <p>\u6b27\u6d32\ud83c\udf0d\uff1a\u5357\u6b27\u3001\u897f\u6b27\u3001\u4e2d\u6b27\u3001\u5317\u6b27\u3001\u4e1c\u6b27</p>"},{"location":"school-notes/junior-notes/geography/#_24","title":"\u5927\u6d0b\ud83c\udf0a","text":""},{"location":"school-notes/junior-notes/geography/#_25","title":"\u6392\u5217\ud83d\udce2","text":"<p>\u592a\u5e73\u6d0b &gt; \u5927\u897f\u6d0b &gt; \u5370\u5ea6\u6d0b &gt; \u5317\u51b0\u6d0b</p>"},{"location":"school-notes/junior-notes/geography/#_26","title":"\u8457\u540d\u8fd0\u6cb3 &amp; \u6d77\u5ce1","text":"\u8457\u540d\u8fd0\u6cb3 &amp; \u6d77\u5ce1 \u8fd0\u6cb3\u540d\u79f0 \u5730\u7406\u4f4d\u7f6e \u6c9f\u901a\u6d77\u6d0b \u82cf\u4f0a\u58eb\u8fd0\u6cb3 \u4e9a\u6d32\u548c\u975e\u6d32\u7684\u5206\u754c\u5904 \u5730\u4e2d\u6d77--\u7ea2\u6d77 \u5df4\u62ff\u9a6c\u8fd0\u6cb3 \u5357\u3001\u5317\u7f8e\u6d32\u7684\u5206\u754c\u5904 \u592a\u5e73\u6d0b--\u5927\u897f\u6d0b \u6d77\u5ce1\u540d\u79f0 \u5730\u7406\u4f4d\u7f6e \u6c9f\u901a\u7684\u6d77\u6d0b \u9a6c\u516d\u7532\u6d77\u5ce1 \u4f4d\u4e8e\u4e1c\u5357\u4e9a \u592a\u5e73\u6d0b\u4e00\u5370\u5ea6\u6d0b \u5fb7\u96f7\u514b\u6d77\u5ce1 \u4f4d\u4e8e\u5357\u7f8e\u6d32\u3001\u5357\u6781\u6d32\u4e4b\u95f4 \u592a\u5e73\u6d0b\u4e00\u5927\u897f\u6d0b \u9ea6\u54f2\u4f26\u6d77\u5ce1 \u4f4d\u4e8e\u5357\u7f8e\u6d32\u5357\u7aef \u592a\u5e73\u6d0b\u4e00\u5927\u897f\u6d0b \u767d\u4ee4\u6d77\u5ce1 \u4f4d\u4e8e\u4e9a\u6d32\u3001\u5317\u7f8e\u6d32\u4e4b\u95f4 \u592a\u5e73\u6d0b\u4e00\u5317\u51b0\u6d0b \u76f4\u5e03\u7f57\u9640\u6d77\u5ce1 \u4f4d\u4e8e\u6b27\u6d32\u3001\u975e\u6d32\u4e4b\u95f4 \u5730\u4e2d\u6d77\u4e00\u5927\u897f\u6d0b \u970d\u5c14\u6728\u5179\u6d77\u5ce1 \u4f4d\u4e8e\u963f\u62c9\u4f2f\u534a\u5c9b \u4f0a\u6717\u4e4b\u95f4 \u6ce2\u65af\u6e7e\u4e00\u5370\u5ea6\u6d0b (\u963f\u62c9\u4f2f\u6d77) \u66fc\u5fb7\u6d77\u5ce1 \u4f4d\u4e8e\u4e9a\u6d32\u3001\u975e\u6d32\u4e4b\u95f4 \u7ea2\u6d77\u4e00\u5370\u5ea6\u6d0b (\u963f\u62c9\u4f2f\u6d77)"},{"location":"school-notes/junior-notes/geography/#_27","title":"\u6d77\u6d0b\ud83c\udf0a\u5bf9\u4eba\u7c7b\ud83e\uddcd\ud83c\udfff\u7684\u5f71\u54cd","text":"<ol> <li>\u6de1\u6c34\ud83d\udca7\u8d44\u6e90</li> <li>\u6e14\u76d0\ud83d\udc1f\ud83e\uddc2\u8d44\u6e90</li> <li>\u4e3a\u65c5\u6e38\u4e1a\u63d0\u4f9b\u8d44\u6e90</li> <li>\u7a7a\u95f4\u8d44\u6e90</li> </ol> <p>\u5f00\u8f9f\u6d77\u4e0a\u4ea4\u901a\u822a\u7ebf\uff0c\u4fee\u7b51\u8fd0\u6cb3\uff0c\u5efa\u8bbe\u6e2f\u53e3\uff0c\u4fc3\u8fdb\u4e16\u754c\u5404\u5730\u4e4b\u95f4\u7684\u7269\u8d44\u4ea4\u6d41</p>"},{"location":"school-notes/junior-notes/geography/#_28","title":"\u3010\u91cd\u70b9\u63d0\u9192\u3011","text":"\u9646\u5730\u7684\u6784\u6210 <p>\u9646\u5730\u5206\u4e3a\u5927\u9646\u548c\u5c9b\u5c7f</p> <ul> <li>\u5927\u9646\u26f0\ufe0f</li> </ul> <p>\u6700\u5927\u7684\u5927\u9646\uff1a\u4e9a\u6b27\u5927\u9646\ud83c\udf0f</p> <p>\u6700\u5c0f\u7684\u5927\u9646\uff1a\u6fb3\u5927\u5229\u4e9a\u5927\u9646\ud83c\udde6\ud83c\uddfa</p> <ul> <li>\u5c9b\u5c7f\ud83c\udfdd\ufe0f</li> </ul> <p>\u6700\u5927\u7684\u5c9b\u5c7f\uff1a\u683c\u9675\u5170\u5c9b\ud83c\uddec\ud83c\uddf1</p> <p>\u6700\u5927\u7684\u7fa4\u5c9b\uff1a\u9a6c\u6765\u7fa4\u5c9b\ud83c\uddf2\ud83c\uddfe</p> <ul> <li>\u534a\u5c9b</li> </ul> <p>\u6700\u5927\u7684\u534a\u5c9b\uff1a\u963f\u62c9\u4f2f\u534a\u5c9b\ud83c\uddf8\ud83c\udde6</p> \u8d64\u9053\u7a7f\u8fc7\u7684\u5927\u6d32 <p>\u975e\u6d32\u3001\u4e9a\u6d32\u3001\u5927\u6d0b\u6d32\u3001\u5357\u7f8e\u6d32</p> \u56de\u5f52\u7ebf\u7a7f\u8fc7\u7684\u5927\u6d32 <p>\u5317\u56de\u5f52\u7ebf</p> <p>\u975e\u6d32\u3001\u4e9a\u6d32\u3001\u5927\u6d0b\u6d32\u3001\u5317\u7f8e\u6d32</p> <p>\u5357\u56de\u5f52\u7ebf\uff1a</p> <p>\u975e\u6d32\u3001\u5927\u6d0b\u6d32\u3001\u5357\u7f8e\u6d32</p> \u6781\u5708\u7a7f\u8fc7\u7684\u5927\u6d32 <p>\u5317\u6781\u5708</p> <p>\u6b27\u6d32\u3001\u4e9a\u6d32\u3001\u5317\u7f8e\u6d32</p> <p>\u5357\u6781\u5708</p> <p>\u5357\u6781\u6d32</p> \u8de8\u7ecf\u5ea6\u6700\u5e7f\u3001\u7eac\u5ea6\u6700\u9ad8 <p>\u6d32\u26f0\ufe0f\uff1a\u5357\u6781\u6d32</p> <p>\u6d0b\ud83c\udf0a\uff1a\u5317\u51b0\u6d0b</p> \u5927\u6d32\u5e73\u5747\u6d77\u62d4\u4e4b\u6700 <p>\u6700\u9ad8\uff1a\u5357\u6781\u6d32</p> <p>\u6700\u4f4e\uff1a\u6b27\u6d32</p>"},{"location":"school-notes/junior-notes/geography/#_29","title":"\u4e16\u754c\u81ea\u7136\u73af\u5883","text":"<pre><code>\u300a\u4eba\u6587\u5730\u7406 \u4e0a\u518c\u300b P21, P28~39\n</code></pre>"},{"location":"school-notes/junior-notes/geography/#_30","title":"\u5730\u5f62\u591a\u6837","text":"\u4e94\u79cd\u57fa\u672c\u5730\u5f62 \u7c7b\u578b \u6d77\u62d4 \u5730\u8868\u7279\u5f81 \u5c71\u5730 500\u7c73\u4ee5\u4e0a \u5cf0\u5ce6\u8d77\u4f0f,\u5761\u5ea6\u9661\u5cfb\u3002\u6709\u7684\u5448\u957f\u6761\u72b6\u5ef6\u4f38,\u5f62\u6210\u5c71\u8109\u3002 \u5c71\u8109\u6392\u5217\u6709\u5e8f,\u8109\u7edc\u5206\u660e,\u4eff\u4f5b\u662f\"\u5927\u5730\u7684\u9aa8\u67b6\" \u9ad8\u539f 500\u7c73\u4ee5\u4e0a \u5730\u8868\u8d77\u4f0f\u4e0d\u5927,\u4f46\u8fb9\u7f18\u5904\u6bd4\u8f83\u9661\u5ced;\u4e5f\u6709\u7684\u9ad8\u539f\u8868\u9762 \u5c71\u5ce6\u8d77\u4f0f,\u51f9\u51f8\u4e0d\u5e73 \u4e18\u9675 200~500\u7c73 \u8d77\u4f0f\u548c\u7f13 \u5e73\u539f 200\u7c73\u4ee5\u4e0b \u5730\u9762\u5e73\u5766\u6216\u8d77\u4f0f\u8f83\u5c0f,\u4e3b\u8981\u5206\u5e03\u5728\u5927\u6cb3\u4e24\u5cb8\u548c\u6fd2\u4e34\u6d77\u6d0b\u7684\u5730\u533a \u76c6\u5730 \u65e0\u6807\u51c6 \u56db\u5468\u88ab\u7fa4\u5c71\u73af\u7ed5,\u56db\u5468\u9ad8,\u4e2d\u95f4\u4f4e"},{"location":"school-notes/junior-notes/geography/#_31","title":"\u4e16\u754c\u4e0a\u91cd\u8981\u7684\u5730\u5f62\u5355\u5143","text":"\u91cd\u8981\u7684\u5730\u5f62\u5355\u5143 \u5730\u5f62\u7c7b\u578b\u91cd\u8981\u5730\u5f62\u5355\u5143\u7279\u70b9\u6240\u5728\u5927\u6d32\u5c71\u8109\u5b89\u7b2c\u65af\u5c71\u8109\u4e16\u754c\u4e0a\u6700\u957f\u7684\u5c71\u8109\u5357\u7f8e\u6d32\u559c\u9a6c\u62c9\u96c5\u5c71\u8109\u4e16\u754c\u4e0a\u6700\u9ad8\u7684\u5c71\u8109\u4e9a\u6d32\u9ad8\u539f\u9752\u85cf\u9ad8\u539f\u4e16\u754c\u4e0a\u6700\u9ad8\u7684\u9ad8\u539f\u4e9a\u6d32\u5df4\u897f\u9ad8\u539f\u4e16\u754c\u4e0a\u6700\u5927\u7684\u9ad8\u539f\u5357\u7f8e\u6d32\u5e73\u539f\u4e9a\u9a6c\u5b59\u5e73\u539f\u4e16\u754c\u4e0a\u6700\u5927\u7684\u70ed\u5e26\u96e8\u6797\u533a\u5357\u7f8e\u6d32\u76c6\u5730\u521a\u679c\u76c6\u5730\u975e\u6d32\u6700\u5927\u7684\u76c6\u5730\u975e\u6d32"},{"location":"school-notes/junior-notes/geography/#_32","title":"\u3010\u91cd\u70b9\u63d0\u9192\u3011","text":"\u5982\u4f55\u63cf\u8ff0\u4e00\u4e2a\u5730\u533a\u7684\u5730\u5f62\u7279\u70b9 <p>\u4e00\u822c\u53ef\u4ee5\u4ece\u6d77\u62d4\u3001\u5730\u9762\u8d77\u4f0f\u3001\u5730\u5f62\u7684\u79cd\u7c7b\u548c\u5206\u5e03\u4e09\u4e2a\u65b9\u9762\u6765\u63cf\u8ff0</p>"},{"location":"school-notes/junior-notes/geography/#_33","title":"\u6c14\u8c61\u4e07\u5343","text":"<pre><code>\u300a\u4eba\u6587\u5730\u7406 \u4e0a\u518c\u300b P21, P28~39\n</code></pre>"},{"location":"school-notes/junior-notes/geography/#_34","title":"\u6c14\u5019\u7684\u6982\u5ff5","text":"<p>\u6c14\u5019\u662f\u4e00\u4e2a\u5730\u533a\u591a\u5e74\u7684\u5929\u6c14\u5e73\u5747\u72b6\u51b5 \u4e00\u4e2a\u5730\u65b9\u7684\u6c14\u5019\u5177\u6709\u4e00\u5b9a\u7684\u7a33\u5b9a\u6027</p>"},{"location":"school-notes/junior-notes/geography/#_35","title":"\u6c14\u5019\u7684\u63cf\u8ff0","text":"\u901a\u8fc7\u6c14\u6e29\u66f2\u7ebf\u56fe\u548c\u964d\u6c34\u91cf\u67f1\u72b6\u56fe\uff0c\u4ece\u6c14\u6e29\u548c\u964d\u6c34\u4e24\u4e2a\u65b9\u9762\u63cf\u8ff0 <p>\u6c14\u6e29\u7684\u53d8\u5316\u7279\u70b9\u3001\u964d\u6c34\u7684\u65f6\u95f4\u5206\u5e03\u3001\u6c34\u70ed\u7684\u543b\u5408\u60c5\u51b5!</p>"},{"location":"school-notes/junior-notes/geography/#_36","title":"\u5224\u65ad\u6c14\u5019\u7c7b\u578b\u7684\u65b9\u6cd5","text":"\u4ee5\"\u6e29\"\u5b9a\"\u5e26\"\u4ee5\"\u6c34\"\u5b9a\"\u578b\"\u6700\u51b7\u6708\u6708\u5747\u6e29\u6e29\u5ea6\u5e26\u964d\u6c34\u5b63\u8282\u5206\u914d\u89c4\u5f8b\u6c14\u5019\u7c7b\u578b&gt;15C\u70ed\u5e26\u590f\u96e8\u578b(\u5168\u5e741500~2000\u6beb\u7c73)\u70ed\u5e26\u5b63\u98ce\u6c14\u5019\u5c11\u96e8\u578b\u70ed\u5e26\u6c99\u6f20\u6c14\u5019\u5e74\u96e8\u578b\u70ed\u5e26\u96e8\u6797\u6c14\u50190~15C\u4e9a\u70ed\u5e26\u590f\u96e8\u578b(\u5168\u5e74&gt;800\u6beb\u7c73)\u4e9a\u70ed\u5e26\u5b63\u98ce\u548c\u6e7f\u6da6\u6c14\u5019\u51ac\u96e8\u578b\u5730\u4e2d\u6d77\u6c14\u5019-15~0C\u6e29\u5e26\u590f\u96e8\u578b(\u5168\u5e74&lt;800\u6beb\u7c73)\u6e29\u5e26\u5b63\u98ce\u6c14\u5019&gt;0C\u5c11\u96e8\u578b\u6e29\u5e26\u5927\u9646\u6027\u6c14\u5019\u5e74\u96e8\u578b\u6e29\u5e26\u6d77\u6d0b\u6027\u6c14\u5019"},{"location":"school-notes/junior-notes/geography/#_37","title":"\u4e3b\u8981\u6c14\u5019\u7c7b\u578b","text":"\u4e3b\u8981\u6c14\u5019\u7c7b\u578b <ul> <li>\u70ed\u5e26\u5357\u6797\u6c14\u5019\uff1a\u7ec8\u5e74\u9ad8\u6e29\u591a\u96e8</li> <li>\u70ed\u5e26\u7ae0\u539f\u6c14\u5019\uff1a\u7ec8\u5e74\u9ad8\u6e29 \u5206\u5e72\u6e7f\u4e24\u5b63</li> <li>\u70ed\u5e26\u5b63\u98ce\u6c14\u5019\uff1a\u7ec8\u5e74\u9ad8\u6e29 \u5206\u65f1\u96e8\u4e24\u5b63</li> <li>\u70ed\u5e26\u6c99\u6de1\u6c14\u5019\uff1a\u7ec8\u5e74\u708e\u70ed\u5e72\u65f1</li> <li>\u4e9a\u70ed\u5e26\u5b63\u98ce\u548c\u6e7f\u6da6\u6c14\u5019\uff1a\u590f\u5b63\u9ad8\u6e29\u591a\u96e8 \u51ac\u5b63\u6e29\u548c\u5c11\u96e8</li> <li>\u5730\u4e2d\u6d77\u6c14\u5019\uff1a\u590f\u5b63\u708e\u70ed\u5e72\u71e5 \u51ac\u5b63\u6e29\u548c\u591a\u96e8</li> <li>\u6e29\u5e26\u6d77\u6d0b\u6027\u6c14\u5019\uff1a\u51ac\u6e29\u590f\u51c9 \u5168\u5e74\u964d\u6c34\u5747\u5300</li> <li>\u6e29\u5e26\u5b63\u98ce\u6c14\u5019\uff1a\u590f\u5b63\u9ad8\u6e29\u591a\u96e8 \u51ac\u5b63\u5bd2\u51b7\u5e72\u71e5</li> <li>\u6e29\u5e26\u5927\u9646\u6027\u6c14\u5019\uff1a\u51ac\u51b7\u590f\u70ed \u5168\u5e74\u964d\u6c34\u8f83\u5c11</li> <li>\u5bd2\u5e26\u6c14\u5019\uff1a\u7ec8\u5e74\u5bd2\u51b7\u5e72\u71e5</li> </ul> Tip <p>\u5bd2\u51b7\uff0c\u5f88\u591a\u5730\u65b9\u7ec8\u5e74\u51b0\u96ea\u8986\u76d6\uff0c\u6c14\u4faf\u91cd\u76f4\u53d8\u5316\u660e\u663e\u3002\u4e3b\u8981\u5206\u5e03\u5728\u9ad8\u5927\u7684\u9ad8\u539f\u3001\u5c71\u5730\u7b49\u5730\u533a\uff0c\u5982\u9752\u85cf\u9ad8\u539f\u3001\u963f\u5c14\u5351\u65af\u5c71\u3001\u5b89\u7b2c\u65af\u5c71\u8109\u7b49\u3002</p> <p></p>"},{"location":"school-notes/junior-notes/geography/#_38","title":"\u4e16\u754c\u4e3b\u8981\u6c14\u5019\u7c7b\u578b\u7684\u5206\u5e03\u60c5\u51b5","text":"\u6c14\u5019\u7c7b\u578b\u4e3b\u8981\u5206\u5e03\u5730\u533a(\u89c1\u5c01\u4e09\u5f69\u56fe)\u53ca\u7279\u70b9\u70ed \u5e26\u70ed\u5e26 \u96e8\u6797 \u6c14\u5019\u5206\u5e03\u89c4\u5f8b:\u5927\u81f4\u5728\u5357\u5317\u7eac10\u00b0\u4e4b\u95f4\u3002 \u5178\u578b\u5730\u533a:\u4e9a\u9a6c\u5b59\u6cb3\u6d41\u57df\u3001\u521a\u679c\u6cb3\u6d41\u57df\u3001\u9a6c\u6765\u7fa4\u5c9b\u3002 \u6c14\u5019\u53ca\u690d\u88ab\u7279\u70b9:\u7ec8\u5e74\u9ad8\u6e29\u591a\u96e8\u3002\u690d\u88ab\u751f\u957f\u65fa\u76db,\u690d\u7269\u79cd\u7c7b\u7e41\u591a\u70ed\u5e26 \u8349\u539f \u6c14\u5019\u5206\u5e03\u89c4\u5f8b:\u5927\u81f4\u5728\u5357\u5317\u7eac10\u00b0\u81f3\u5357\u5317\u56de\u5f52\u7ebf\u4e4b\u95f4\u3002 \u5178\u578b\u5730\u533a:\u975e\u6d32\u4e2d\u90e8\u3001\u5357\u7f8e\u5df4\u897f\u3001\u6fb3\u5927\u5229\u4e9a\u5927\u9646\u5317\u90e8\u548c\u5357\u90e8\u3002 \u6c14\u5019\u53ca\u690d\u88ab\u7279\u70b9:\u7ec8\u5e74\u9ad8\u6e29,\u4e00\u5e74\u6709\u660e\u663e\u7684\u5e72\u6e7f\u4e24\u5b63\u3002\u690d\u88ab\u662f\u7a00\u6811\u8349\u539f\u70ed\u5e26 \u5b63\u98ce \u6c14\u5019\u5206\u5e03\u89c4\u5f8b \u5927\u81f4\u5728\u5317\u7eac10\u00b0\u81f3\u5317\u56de\u5f52\u7ebf\u9644\u8fd1\u7684\u5357\u4e9a\u548c\u4e1c\u5357\u4e9a\u5927\u90e8\u5206\u5730\u533a\u3002 \u5178\u578b\u5730\u533a:\u4e9a\u6d32\u4e2d\u5357\u534a\u5c9b\u548c\u5370\u5ea6\u534a\u5c9b\u3002 \u6c14\u5019\u53ca\u690d\u88ab\u7279\u70b9:\u7ec8\u5e74\u9ad8\u6e29,\u5206\u65f1\u96e8\u4e24\u5b63,\u590f\u5b63\u591a\u96e8\u3002\u690d\u88ab\u4e3a\u70ed\u5e26\u5b63\u96e8\u6797\u70ed\u5e26 \u6c99\u6f20 \u6c14\u5019\u5206\u5e03\u89c4\u5f8b:\u5927\u81f4\u5728\u56de\u5f52\u7ebf\u9644\u8fd1\u7684\u5927\u9646\u5185\u90e8\u548c\u897f\u5cb8\u3002 \u5178\u578b\u5730\u533a:\u6492\u54c8\u62c9\u3001\u963f\u62c9\u4f2f\u534a\u5c9b\u3001\u6fb3\u5927\u5229\u4e9a\u4e2d\u897f\u90e8\u3002 \u6c14\u5019\u53ca\u690d\u88ab\u7279\u70b9:\u7ec8\u5e74\u9ad8\u6e29,\u964d\u6c34\u7a00\u5c11\u3002\u690d\u88ab\u4ee5\u7eff\u6d32\u533a\u7684\u8010\u65f1\u690d\u7269\u4e3a\u4e3b\u4e9a \u70ed \u5e26\u4e9a\u70ed\u5e26 \u5b63\u98ce\u548c \u6e7f\u6da6\u6c14\u5019\u5206\u5e03\u89c4\u5f8b \u5927\u81f4\u5728\u5357\u5317\u7eac25\u00b0\u81f3\u5357\u5317\u7eac35\u3002\u4e4b\u95f4\u7684\u5927\u9646\u4e1c\u5cb8\u3002 \u5178\u578b\u5730\u533a:\u4e2d\u56fd\u5357\u65b9\u3001\u97e9\u56fd\u3001\u65e5\u672c\u5927\u90e8\u5206\u5730\u533a\u3002 \u6c14\u5019\u53ca\u690d\u88ab\u7279\u70b9:\u590f\u5b63\u9ad8\u6e29\u591a\u96e8,\u51ac\u5b63\u6e29\u548c\u5c11\u96e8\u3002\u690d\u88ab\u4ee5\u5e38\u7eff\u9614\u53f6\u6797\u4e3a\u4e3b\u5730\u4e2d\u6d77 \u6c14\u5019\u5206\u5e03\u89c4\u5f8b:\u5357\u5317\u7eac30\u00b0\u81f3\u5357\u5317\u7eac40\u00b0\u4e4b\u95f4\u7684\u5927\u9646\u897f\u5cb8\u3002 \u5178\u578b\u5730\u533a:\u5730\u4e2d\u6d77\u6cbf\u5cb8\u5730\u533a\u5206\u5e03\u6700\u5e7f\u3002 \u6c14\u5019\u53ca\u690d\u88ab\u7279\u70b9:\u590f\u5b63\u708e\u70ed\u5e72\u71e5,\u51ac\u5b63\u6e29\u548c\u591a\u96e8\u3002\u690d\u88ab\u4ee5\u5e38\u7eff\u786c\u53f6\u6797\u4e3a\u4e3b\u6e29 \u5e26\u6e29\u5e26 \u5b63\u98ce \u6c14\u5019\u5206\u5e03\u89c4\u5f8b:\u4e3b\u8981\u5206\u5e03\u5728\u4e9a\u6d32\u4e1c\u90e8\u7684\u4e2d\u7eac\u5ea6\u5730\u533a\u3002 \u5178\u578b\u5730\u533a:\u6211\u56fd\u5317\u65b9\u3001\u65e5\u672c\u5317\u90e8\u3001\u671d\u9c9c\u3002 \u6c14\u5019\u53ca\u690d\u88ab\u7279\u70b9:\u590f\u5b63\u9ad8\u6e29\u591a\u96e8,\u51ac\u5b63\u5bd2\u51b7\u5e72\u71e5\u3002\u690d\u88ab\u4ee5\u843d\u53f6\u9614\u53f6\u6797\u548c\u9488 \u9614\u6df7\u4ea4\u6797\u4e3a\u4e3b\u6e29\u5e26 \u5927\u9646\u6027 \u6c14\u5019\u5206\u5e03\u89c4\u5f8b:\u4e3b\u8981\u5206\u5e03\u5728\u4e2d\u7eac\u5ea6\u5185\u9646\u5730\u533a\u3002 \u5178\u578b\u5730\u533a:\u4e9a\u6b27\u5927\u9646\u548c\u5317\u7f8e\u5927\u9646\u7684\u5185\u9646\u5730\u533a\u3002 \u6c14\u5019\u53ca\u690d\u88ab\u7279\u70b9:\u51ac\u51b7\u590f\u70ed,\u964d\u6c34\u8f83\u5c11\u3002\u591a\u6c99\u6f20\u548c\u8349\u539f\u6e29\u5e26 \u6d77\u6d0b\u6027 \u6c14\u5019\u5206\u5e03\u89c4\u5f8b:\u4e3b\u8981\u5206\u5e03\u5728\u5357\u5317\u7eac40\u00b0\u81f3\u5357\u5317\u7eac60\u00b0\u4e4b\u95f4\u7684\u5927\u9646\u897f\u5cb8\u3002 \u5178\u578b\u5730\u533a:\u4ee5\u6b27\u6d32\u897f\u90e8\u6700\u5e7f\u3002 \u6c14\u5019\u53ca\u690d\u88ab\u7279\u70b9:\u51ac\u6e29\u590f\u51c9,\u5168\u5e74\u964d\u6c34\u5747\u5300\u3002\u690d\u88ab\u4e3a\u843d\u53f6\u9614\u53f6\u6797\u5bd2\u5e26\u6c14\u5019\u5206\u5e03\u89c4\u5f8b:\u4e3b\u8981\u5206\u5e03\u5728\u6781\u5708\u4ee5\u5185\u7684\u9ad8\u7eac\u5ea6\u5730\u533a\u3002 \u5178\u578b\u5730\u533a:\u5357\u6781\u6d32\u3001\u683c\u9675\u5170\u5c9b\u53ca\u4e9a\u6b27\u5927\u9646\u548c\u5317\u7f8e\u5927\u9646\u7684\u5317\u51b0\u6d0b\u6cbf\u5cb8\u3002 \u6c14\u5019\u53ca\u690d\u88ab\u7279\u70b9:\u7ec8\u5e74\u4e25\u5bd2\u5c11\u96e8\u3002\u6709\u82d4\u85d3\u3001\u5730\u8863\u7b49\u690d\u88ab\u5206\u5e03\u6216\u5b8c\u5168\u88ab\u51b0\u96ea\u8986\u76d6\u9ad8\u539f\u5c71\u5730 \u6c14\u5019\u5206\u5e03\u89c4\u5f8b:\u4e3b\u8981\u5206\u5e03\u5728\u9ad8\u5927\u7684\u9ad8\u539f\u3001\u5c71\u5730\u5730\u533a\u3002 \u5178\u578b\u5730\u533a:\u9752\u85cf\u9ad8\u539f\u3001\u5357\u7f8e\u6d32\u7684\u5b89\u7b2c\u65af\u5c71\u7b49\u3002 \u6c14\u5019\u53ca\u690d\u88ab\u7279\u70b9:\u5bd2\u51b7,\u5f88\u591a\u5730\u65b9\u7ec8\u5e74\u51b0\u96ea\u8986\u76d6,\u6c14\u5019\u5782\u76f4\u53d8\u5316\u660e\u663e\u3002\u690d \u88ab\u4e5f\u5448\u73b0\u5782\u76f4\u53d8\u5316\u7684\u7279\u70b9"},{"location":"school-notes/junior-notes/geography/#_39","title":"\u3010\u91cd\u70b9\u63d0\u9192\u3011","text":"\u975e\u6d32\u6c14\u5019\u5206\u5e03\u7684\u7279\u70b9 <p>\u57fa\u672c\u4e0a\u4ee5\u8d64\u9053\u4e3a\u4e2d\u5fc3\uff0c\u5357\u5317\u5bf9\u79f0\uff0c\u5b63\u8282\u76f8\u53cd</p> \u4e16\u754c\u4e0a\u5206\u5e03\u9762\u79ef\u6700\u5927\u7684\u6c14\u5019\u7c7b\u578b <p>\u6e29\u5e26\u5927\u9646\u6027\u6c14\u5019</p> \u9664\u5357\u6781\u6d32\u5916\u5404\u5927\u6d32\u90fd\u6709\u5206\u5e03\u7684\u6c14\u5019\u7c7b\u578b <p>\u6e29\u5e26\u5927\u9646\u6027\u6c14\u5019</p> \u4e9a\u6d32\u6ca1\u6709\u5206\u5e03\u7684\u6c14\u5019\u7c7b\u578b <p>\u70ed\u5e26\u8349\u539f\u6c14\u5019\u3001\u6e29\u5e26\u6d77\u6d0b\u6027\u6c14\u5019</p>"},{"location":"school-notes/junior-notes/geography/#_40","title":"\u4f17\u591a\u7684\u6cb3\u6e56","text":""},{"location":"school-notes/junior-notes/geography/#_41","title":"\u6cb3\u6d41\u4e0e\u6e56\u6cca","text":"<p>\u6cb3\u6d41\u88ab\u6210\u4e3a\u5927\u5730\u7684\u52a8\u8109\uff0c\u6e56\u6cca\u88ab\u8a89\u4e3a\u5927\u5730\u7684\u660e\u73e0\uff0c\u5c71\u8109\u4e3a\u5927\u5730\u7684\u9aa8\u67b6</p>"},{"location":"school-notes/junior-notes/geography/#_42","title":"\u4e16\u754c\u4e3b\u8981\u7684\u6cb3\u6d41\u4e0e\u6e56\u6cca","text":"\u4e3b\u8981\u7684\u6cb3\u6d41 \u6cb3\u6d41\u6d41\u5411\u6ce8\u5165\u7279\u70b9\u6240\u5728\u5927\u6d32\u5c3c\u7f57\u6cb3\u81ea\u5357\u5411\u5317\u6d41\u5730\u4e2d\u6d77\u4e16\u754c\u7b2c\u4e00\u957f\u6cb3\u975e\u6d32\u4e9a\u9a6c\u5b59\u6cb3\u81ea\u897f\u5411\u4e1c\u6d41\u5927\u897f\u6d0b\u4e16\u754c\u7b2c\u4e8c\u957f\u6cb3\u3001\u6d41\u91cf\u6700\u5927\u3001 \u6d41\u57df\u9762\u79ef\u6700\u5e7f\u7684\u6cb3\u6d41\u5357\u7f8e\u6d32\u957f\u6c5f\u81ea\u897f\u5411\u4e1c\u6d41\u4e1c\u6d77\u4e16\u754c\u7b2c\u4e09\u957f\u6cb3\u4e9a\u6d32\u5bc6\u897f\u897f\u6bd4\u6cb3\u81ea\u5317\u5411\u5357\u6d41\u58a8\u897f\u54e5\u6e7e\u4e16\u754c\u7b2c\u56db\u957f\u6cb3\u5317\u7f8e\u6d32\u9ec4\u6cb3\u81ea\u897f\u5411\u4e1c\u6d41\u6e24\u6d77\u542b\u6c99\u91cf\u8f83\u5927\u4e9a\u6d32\u521a\u679c\u6cb3\u6ce8:\u5404\u6bb5\u6d41\u5411\u4e0d \u4e00\u81f4\u5927\u897f\u6d0b\u4e16\u754c\u4e0a\u6c34\u80fd\u8574\u85cf\u91cf\u6700\u5927\u7684 \u6cb3\u6d41\u975e\u6d32\u591a\u7459\u6cb3\u81ea\u897f\u5411\u4e1c\u6d41\u9ed1\u6d77\u4e16\u754c\u4e0a\u6d41\u7ecf\u56fd\u5bb6\u6700\u591a\u7684\u6cb3\u6d41\u6b27\u6d32\u6e44\u516c\u6cb3\u81ea\u5317\u5411\u5357\u6d41\u5357\u6d77\u4e9a\u6d32\u6d41\u7ecf\u56fd\u5bb6\u6700\u591a\u7684\u6cb3\u6d41\u4e9a\u6d32\u83b1\u8335\u6cb3\u81ea\u5357\u5411\u5317\u6d41\u5927\u897f\u6d0b\u6b27\u6d32\u8457\u540d\u7684\u56fd\u9645\u6027\u6cb3\u6d41\u6b27\u6d32 \u4e3b\u8981\u7684\u6e56\u6cca \u6e56\u6cca\u7279\u70b9\u6240\u5728\u5927\u6d32\u91cc\u6d77\u4e16\u754c\u9762\u79ef\u6700\u5927\u7684\u54b8\u6c34\u6e56\u4e9a\u6b27\u4ea4\u754c\u82cf\u5fc5\u5229\u5c14\u6e56\u4e16\u754c\u9762\u79ef\u6700\u5927\u7684\u6de1\u6c34\u6e56\u5317\u7f8e\u6d32\u8d1d\u52a0\u5c14\u6e56\u4e16\u754c\u6700\u6df1\u7684\u6e56\u6cca\u4e9a\u6d32\u6b7b\u6d77\u4e9a\u6d32\u6d77\u62d4\u6700\u4f4e\u7684\u6e56\u6cca\u4e9a\u6d32\u9752\u6d77\u6e56\u4e2d\u56fd\u9762\u79ef\u6700\u5927\u7684\u54b8\u6c34\u6e56\u4e9a\u6d32\u9131\u9633\u6e56\u4e2d\u56fd\u9762\u79ef\u6700\u5927\u7684\u6de1\u6c34\u6e56\u4e9a\u6d32"},{"location":"school-notes/junior-notes/geography/#_43","title":"\u4e16\u754c\u4eba\u6587\u73af\u5883","text":"<pre><code>\u300a\u4eba\u6587\u5730\u7406 \u4e0a\u518c\u300b P21, P40~48\n</code></pre>"},{"location":"school-notes/junior-notes/geography/#_44","title":"\u8bed\u8a00 &amp; \u5b97\u6559","text":"\u8054\u5408\u56fd\ud83c\uddfa\ud83c\uddf3\u5de5\u4f5c\u8bed\u8a00 <p>\u6c49\u8bed\ud83c\udde8\ud83c\uddf3\u3001\u82f1\u8bed\ud83c\uddfa\ud83c\uddf8\ud83c\uddec\ud83c\udde7\u3001\u6cd5\u8bed\ud83c\uddeb\ud83c\uddf7\u3001\u4fc4\u8bed\ud83c\uddf7\ud83c\uddfa\u3001\u897f\u73ed\u7259\u8bed\ud83c\uddea\ud83c\uddf8\u3001\u963f\u62c9\u4f2f\u8bed\ud83c\uddf8\ud83c\udde6\ud83c\udde6\ud83c\uddea</p> \u4e16\u754c\u4e09\u5927\u5b97\u6559 <p>\u4f5b\u6559\ud83c\uddee\ud83c\uddf3\u3001\u57fa\u7763\u6559\ud83c\uddf5\ud83c\uddf8\u3001\u4f0a\u65af\u5170\u6559\ud83c\uddf8\ud83c\udde6</p> <p>\u7279\u8272\u4f53\u73b0</p> <p>\u4f5b\u6559\ud83c\uddee\ud83c\uddf3\uff1a\u5bfa\u5e99 &amp; \u4f5b\u5854\ud83d\uded5</p> <p>\u57fa\u7763\u6559\ud83c\uddf5\ud83c\uddf8\uff1a\u6559\u5802\u26ea\ufe0f</p> <p>\u4f0a\u65af\u5170\u6559\ud83c\uddf8\ud83c\udde6\uff1a\u6e05\u771f\u5bfa\ud83d\udd4c\ufe0f</p>"},{"location":"school-notes/junior-notes/geography/#iii","title":"\u2162. \u751f\u6d3b\u5728\u4e0d\u540c\u533a\u57df\u91cc","text":"\u5178\u578b\u56fd\u5bb6\u6c47\u603b \u7f8e\u56fd\ud83c\uddfa\ud83c\uddf8 \u7f8e\u56fd\u5546\u54c1\u519c\u4e1a\u53d1\u8fbe\u7684\u6709\u5229\u6761\u4ef6(\u539f\u56e0) <ol> <li>\u81ea\u7136\u6761\u4ef6\uff1a\u571f\u5730\u8fbd\u9614\uff0c\u5e73\u539f\u5730\u5f62\uff0c\u5730\u52bf\u4f4e\u5e73\uff0c\u571f\u58e4\u80a5\u6c83\uff1b\u6e29\u5e26\u5927\u9646\u6027\u6c14\u5019\uff0c\u6c14\u5019\u9002\u5b9c\uff1b\u5bc6\u897f\u897f\u6bd4\u6cb3\u81ea\u5317\u5411\u5357\u8d2f\u7a7f\u5e73\u539f\uff0c\u6c34\u6e90\u5145\u8db3\uff0c\u704c\u6e89\u4fbf\u5229\u3002</li> <li>\u4eba\u6587\u6761\u4ef6\uff1a\u7ecf\u6d4e\u53d1\u8fbe\uff0c\u519c\u4e1a\u79d1\u6280\u6c34\u5e73\u9ad8\uff0c\u519c\u573a\u89c4\u6a21\u5927\uff0c\u5730\u5e7f\u4eba\u7a00\uff0c\u673a\u68b0\u5316\u751f\u4ea7\uff0c\u6548\u7387\u9ad8\uff0c\u4ea7\u91cf\u5927\uff0c\u4ea7\u54c1\u50a8\u5b58\u6761\u4ef6\u597d\uff0c\u4ea4\u901a\u56db\u901a\u516b\u8fbe\u7b49\u7b49\u3002</li> </ol> \u745e\u58eb\ud83c\udde8\ud83c\udded \u745e\u58eb\u6210\u4e3a\u65c5\u6e38\u70ed\u533a\u7684\u6709\u5229\u6761\u4ef6(\u539f\u56e0) <ol> <li>\u81ea\u7136\u6761\u4ef6\uff1a<ol> <li>\u5730\u5f62\u591a\u6837\uff0c\u6d77\u62d4\u9ad8\uff0c\u6709\u51b0\u5ddd\u548c\u96ea\u5c71\uff0c\u98ce\u666f\u4f18\u7f8e\uff0c\u666f\u89c2\u5947\u7279\u3002</li> <li>\u590f\u5929\u6210\u89c2\u5149\u3001\u907f\u6691\u3001\u767b\u5c71\u80dc\u5730\uff1b\u51ac\u5929\u53d1\u5c55\u6ed1\u96ea\u8fd0\u52a8\u7684\u6761\u4ef6\u5341\u5206\u7406\u60f3\uff0c\u6e29\u6cc9\u8d44\u6e90\u4e5f\u5f88\u4e30\u5bcc\u3002</li> </ol> </li> <li>\u4eba\u6587\u6761\u4ef6\uff1a<ol> <li>\u65c5\u6e38\u5185\u5bb9\u4e30\u5bcc\u591a\u5f69\uff0c\u914d\u5957\u8bbe\u65bd\u5b8c\u5907\u3001\u7cbe\u826f\uff0c\u670d\u52a1\u5468\u5230\u3001\u7ec6\u81f4\u5165\u5fae\u7b49\u3002</li> <li>\u62e5\u6709\u53d1\u8fbe\u7684\u94c1\u8def\u548c\u516c\u8def\uff1b\u6e38\u5ba2\u4e58\u706b\u8f66\u53ef\u76f4\u63a5\u5230\u8fbe\u5c71\u5cf0\u89c2\u770b\u51b0\u5ddd\u5947\u666f\uff1b\u8bb8\u591a\u5730\u65b9\u6709\u7535\u7f06\u8f66\u76f4\u8fbe\u5c71\u5dc5\uff1b\u8fd8\u6709\u76f4\u5347\u673a\u4e3a\u9047\u5230\u5371\u9669\u7684\u6e38\u4eba\u63d0\u4f9b\u6551\u52a9\u3002</li> <li>\u6709\u53e4\u8001\u7684\u57ce\u9547\u548c\u7eaf\u6734\u7684\u4e61\u6751\u751f\u6d3b\u7b49\u3002</li> </ol> </li> </ol> \u6211\u56fd\u4e0e\u745e\u58eb\u5728\u5730\u5f62\u4e0a\u7684\u5171\u540c\u7279\u70b9 <ol> <li>\u5730\u5f62\u7c7b\u578b\u591a\u6837</li> <li>\u5c71\u533a\u9762\u79ef\u5e7f\u5927</li> </ol> \u6ce2\u65af\u6e7e\u5730\u533a\u77f3\u6cb9\u8f93\u51fa\u8def\u7ebf <p>A\u7ebf\uff1a\u6ce2\u65af\u6e7e -&gt; \u970d\u5c14\u6728\u5179\u6d77\u5ce1 -&gt; \u5370\u5ea6\u6d0b -&gt; \u9a6c\u516d\u7532\u6d77\u5ce1 -&gt; \u592a\u5e73\u6d0b -&gt; \u4e1c\u4e9a(\u4e2d\u56fd\u3001\u65e5\u672c)</p> <p>B\u7ebf\uff1a\u6ce2\u65af\u6e7e -&gt; \u970d\u5c14\u6728\u5179\u6d77\u5ce1 -&gt; \u5370\u5ea6\u6d0b -&gt; \u597d\u671b\u89d2 -&gt; \u5927\u897f\u6d0b -&gt; \u897f\u6b27\u548c\u7f8e\u56fd</p> <p>C\u7ebf\uff1a\u6ce2\u65af\u6e7e -&gt; \u970d\u5c14\u6728\u5179\u6d77\u5ce1 -&gt; \u963f\u62c9\u4f2f\u6d77 -&gt; \u66fc\u5fb7\u6d77\u5ce1 -&gt; \u7ea2\u6d77 -&gt; \u82cf\u4f0a\u58eb\u8fd0\u6cb3 -&gt; \u5730\u4e2d\u6d77 -&gt; \u76f4\u5e03\u7f57\u9640\u6d77\u5ce1 -&gt; \u5927\u897f\u6d0b -&gt; \u897f\u6b27\u548c\u7f8e\u56fd</p> <p>D\u7ebf\uff1a\u6ce2\u65af\u6e7e -&gt; \u970d\u5c14\u6728\u5179\u6d77\u5ce1 -&gt; \u5370\u5ea6\u6d0b -&gt; \u6fb3\u5927\u5229\u4e9a</p> \u6ce2\u65af\u6e7e\u5730\u533a\u4f4d\u7f6e\u7279\u5f81 <p>\u4f4d\u4e8e\u4e9a\u6d32\u897f\u90e8\uff0c\u5730\u5904\u4e9a\u3001\u6b27\u3001\u975e\u4e09\u5927\u6d32\u7684\u8fde\u63a5\u5730\u5e26\uff0c\u6fd2\u4e34\u963f\u62c9\u4f2f\u6d77\u3001\u7ea2\u6d77\u3001\u91cc\u6d77\uff0c\u9760\u8fd1\u9ed1\u6d77\u548c\u5730\u4e2d\u6d77\uff0c\u5730\u7406\u4f4d\u7f6e\u6781\u4e3a\u91cd\u8981\uff0c\u7d20\u6709\u201c\u4e09\u6d32\u4e94\u6d77\u201d\u4e4b\u79f0\u3002</p> <ol> <li> <p>\u5408\u4e3a\u4e00\u4e2a\u8981\u7d20\u00a0\u21a9</p> </li> <li> <p>\u6781\u70b9\u9664\u5916\u00a0\u21a9</p> </li> </ol>"},{"location":"school-notes/senior-high-school/projectile-motion/","title":"Projectile motion","text":""},{"location":"school-notes/senior-high-school/projectile-motion/#_1","title":"\u5e73\u629b\u8fd0\u52a8\u6027\u8d28","text":""},{"location":"school-notes/senior-high-school/projectile-motion/#_2","title":"\u57fa\u7840","text":"<p>$\\Delta v = gt$</p> <p>\u6c34\u5e73:</p> <p>$ \\left{ \\begin{aligned} v_x &amp;= v_0 \\ x &amp;= v_0 t \\end{aligned} \\right. $</p> <p>\u7ad6\u76f4:</p> <p>$ \\left{ \\begin{aligned} v_y &amp;= gt \\ y &amp;= \\frac{1}{2} g t^2 \\ \\end{aligned} \\right. $</p> <p>\u5408\u6210:</p> <p>$ \\left{ \\begin{aligned} v &amp;= \\sqrt{v_x^2 + v_y^2} \\ x &amp;= \\sqrt{x^2 + y^2} \\ \\end{aligned} \\right. $</p>"},{"location":"school-notes/senior-high-school/projectile-motion/#_3","title":"\u63a8\u8bba","text":"<p>$ \\because \\left{ \\begin{aligned} \\tan{\\theta} &amp;= \\dfrac{v_y}{v_x} = \\dfrac{2y_A}{x_A} \\ \\tan{\\alpha} &amp;= \\dfrac{y_A}{x_A} \\end{aligned} \\right. $</p> <p>$ \\therefore \\tan{\\theta} = 2 \\tan{\\alpha}</p> <p>$\\tan{\\theta} = 2 \\tan{\\alpha}$</p> <p>$x_b = \\frac{1}{2} x_a$</p>"},{"location":"school-notes/senior-high-school/sets/","title":"\u96c6\u5408","text":""},{"location":"school-notes/senior-high-school/sets/#_1","title":"\u96c6\u5408\u53ca\u57fa\u672c\u5173\u7cfb","text":""},{"location":"school-notes/senior-high-school/sets/#_2","title":"\u5b9a\u4e49","text":"<p>\u5bf9\u8c61(\u5373\u5143\u7d20)\u6240\u6784\u6210\u7684\u6574\u4f53</p>"},{"location":"school-notes/senior-high-school/sets/#_3","title":"\u8868\u793a","text":"<p>\u96c6\u5408: \u5927\u5199\u5b57\u6bcd $\\mathbb{A}, \\mathbb{B}, \\mathbb{C},...$</p> <p>\u5143\u7d20: \u5c0f\u5199\u5b57\u6bcd $a, b, c,...$</p>"},{"location":"school-notes/senior-high-school/sets/#_4","title":"\u5199\u6cd5","text":""},{"location":"school-notes/senior-high-school/sets/#_5","title":"\u6570\u96c6\u7684\u5199\u6cd5","text":"\u5217\u4e3e\u6cd5\u63cf\u8ff0\u6cd5 <p>$\\mathbb{A} = \\{a, b, c, ..., z\\}$</p> <p>$\\mathbb{A} = \\{x \\mid p(x)\\}$ <sup>1</sup></p> \u7279\u6b8a\u7684\u96c6\u5408 <p>$\\mathbb{N}$: \u81ea\u7136\u6570\u96c6</p> <p>$\\mathbb{N}_+\\ or\\ \\mathbb{N}^*$: \u6b63\u6574\u6570\u96c6</p> <p>$\\mathbb{Z}$: \u6574\u6570\u96c6</p> <p>$\\mathbb{Q}$: \u6709\u7406\u6570\u96c6</p> <p>$\\mathbb{R}$: \u5b9e\u6570\u96c6</p> <p>$\\mathbb{C}$: \u590d\u6570\u96c6</p>"},{"location":"school-notes/senior-high-school/sets/#_6","title":"\u5173\u7cfb\u7684\u5199\u6cd5","text":"\u5143\u7d20\u4e0e\u96c6\u5408\u7684\u5173\u7cfb\u96c6\u5408\u4e0e\u96c6\u5408\u7684\u5173\u7cfb <p>\u5c5e\u4e8e: $a \\in \\mathbb{A}$</p> <p>\u4e0d\u5c5e\u4e8e: $b \\notin \\mathbb{A}$</p> <p>\u5b50\u96c6: $\\mathbb{B} \\subseteq \\mathbb{A} \\iff \\mathbb{A} \\supseteq \\mathbb{B} $</p> <p>\u76f8\u7b49: $\\mathbb{A} = \\mathbb{B}$</p> <p>\u771f\u5b50\u96c6: $\\mathbb{B} \\subsetneqq \\mathbb{A} \\iff \\mathbb{A} \\supsetneqq \\mathbb{B} $</p>"},{"location":"school-notes/senior-high-school/sets/#_7","title":"\u6570\u96c6 &amp; \u70b9\u96c6","text":"<p>\u6570\u96c6\uff0c\u8868\u8303\u56f4: $\\mathbb{A} = \\{x \\mid p(x)\\}$</p> <p>\u70b9\u96c6\uff0c\u8868\u56fe\u50cf: $\\mathbb{B} = \\{(x, y) \\mid y = x^2 \\}$</p>"},{"location":"school-notes/senior-high-school/sets/#_8","title":"\u96c6\u5408\u95f4\u7684\u8fd0\u7b97\u53ca\u8fd0\u7528","text":"\u4ea4\u96c6\u5e76\u96c6\u8865\u96c6 <p>$\\mathbb{A} \\cap \\mathbb{B} = \\{x \\mid x \\in \\mathbb{A}\\ and\\ x \\in \\mathbb{B}\\}$</p> <p>\u5373\u5bf9\u96c6\u5408\u8fdb\u884c \"\u4e0e $\\land$\" \u64cd\u4f5c</p> <p>$\\mathbb{A} \\cup \\mathbb{B} = \\{x \\mid x \\in \\mathbb{A}\\ or\\ x \\in \\mathbb{B}\\}$</p> <p>\u5373\u5bf9\u96c6\u5408\u8fdb\u884c \"\u6216 $\\lor$\" \u64cd\u4f5c</p> <p>$\\complement_\\mathbb{U} \\mathbb{A} = \\{x \\mid x \\in \\mathbb{U}\\ and\\ x \\notin \\mathbb{A}\\}$</p> \u8fd0\u7b97\u53d8\u5f62 &amp; \u6027\u8d28 \u5e76\u96c6\u7684\u6027\u8d28\u8865\u96c6\u7684\u6027\u8d28 <p>$\\mathbb{A} \\cup \\emptyset = \\mathbb{A}$</p> <p>$(\\mathbb{A} \\cup \\mathbb{B}) \\supseteq \\mathbb{A}$</p> <p>$\\mathbb{A} \\cup \\mathbb{B} \\iff \\mathbb{B} \\subseteq \\mathbb{A}$</p> <p>$\\complement_\\mathbb{U} \\mathbb{U} = \\emptyset$</p> <p>$\\complement_\\mathbb{U} \\emptyset = \\mathbb{U}$</p> <p>$\\complement_\\mathbb{U} (\\complement_\\mathbb{U} \\mathbb{A}) = \\mathbb{A}$</p> <p>$\\mathbb{A} \\cup (\\complement_\\mathbb{U} \\mathbb{A}) = \\mathbb{U}$</p> <p>$\\mathbb{A} \\cap (\\complement_\\mathbb{U} \\mathbb{A}) = \\emptyset$</p> <ol> <li> <p>\u9ed8\u8ba4 $x \\in \\mathbb{R}$\u00a0\u21a9</p> </li> </ol>"},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2024/#2024","title":"2024","text":""},{"location":"blog/category/ai/","title":"Ai","text":""},{"location":"blog/category/ai/#ai","title":"AI","text":""}]}